# NBA Prediction Pipeline

A Python pipeline for fetching NBA game data, generating features, training a logistic regression model, and producing daily win probability predictions. Outputs are saved locally in organized folders and can be connected directly to Power BI for dashboards.

---

## ğŸš€ How to Run

1. Install dependencies:
   ```bash
   pip install -r requirements.txt
   ```
2. Run the pipeline
```bash
  python run_pipeline.py
```
3. Outputs will be saved automatically into the data/ folder structure.
```bash
project_root/
â”‚
â”œâ”€â”€ run_pipeline.py        # main pipeline script
â”œâ”€â”€ config.yaml            # configuration file
â”œâ”€â”€ models/                # trained model files
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/               # raw API pulls (optional)
â”‚   â”œâ”€â”€ cache/             # cached training features
â”‚   â”œâ”€â”€ history/           # historical predictions
â”‚   â”œâ”€â”€ csv/               # daily CSV outputs
â”‚   â”œâ”€â”€ parquet/           # daily Parquet outputs
â”‚   â””â”€â”€ logs/              # pipeline + error logs
â””â”€â”€ tests/                 # unit tests
```
4. Power Bi Integration
- Connect to Historical Prediction
  - Open Power BI Desktop
  - Go to Home -> Get Data -> Parquet
  - Select data/history/predictions_history.parquet
  - Load the table into Power BI.

5. Connect to Multiple Daily Files
  - Use the Folder connector:
    - for CSVs -> data/csv/
    - for Parquet -> data/parquet/
    - Load the table into Power BI

## Example Dashboards
  - Accuracy trend â†’ Line chart with prediction_date vs. accuracy.
  - Team analytics â†’ Bar chart with TEAM_ID vs. average pred_proba.
  - Game drill_downs â†’ Table with stats + predictions.

6. ğŸ›  Features- Data Quality Checks â†’ Validates critical columns, drops nulls, logs anomalies.
   - Error Handling â†’ Retries API calls with exponential backoff, logs errors separately.
   - Configurable â†’ Paths, seasons, and model path defined in config.yaml.
   - Environment Separation â†’ Raw, cache, history, CSV, Parquet, logs all in distinct folders.
   - Deduplication â†’ Unique IDs prevent duplicate rows.
   - Performance â†’ Batch feature generation speeds up initial fetch.
   - Unit Tests â†’ Core functions tested with pytest.

## ğŸ‘¥ Contributors- Developed in Python with â¤ï¸ for NBA analytics.
  - Designed for easy integration with Power BI.

## ğŸ“ˆ Roadmap- v1.0 â†’ Local Parquet/CSV storage, Power BI dashboards.
  - v2.0 â†’ Optional migration to SQLite/PostgreSQL for larger datasets.
  - Future â†’ Cloud integration (Azure Synapse, BigQuery, etc.).

## - Version 1.0 â†’ stick with logistic regression + clean pipeline (done).
    - Version 2.0 â†’ migrate storage to SQLite/Postgres.
    - Version 3.0 â†’ add AI models (XGBoost or neural nets) and integrate explainability.

![Coverage](https://img.shields.io/codecov/c/github/your-org/your-repo?style=flat-square)
