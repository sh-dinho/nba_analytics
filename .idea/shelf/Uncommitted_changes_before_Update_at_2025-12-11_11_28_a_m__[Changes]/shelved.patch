Index: tests/test_game_features.py
===================================================================
diff --git a/tests/test_game_features.py b/tests/test_game_features.py
deleted file mode 100644
--- a/tests/test_game_features.py	(revision 00055356329a845d3b2ca81b77c3ffb2e2b15969)
+++ /dev/null	(revision 00055356329a845d3b2ca81b77c3ffb2e2b15969)
@@ -1,45 +0,0 @@
-# ============================================================
-# Path: tests/test_game_features.py
-# Filename: test_game_features.py
-# Author: Your Team
-# Date: December 2025
-# Purpose: Tests for game_features functions using real NBA data
-# ============================================================
-
-import pytest
-import pandas as pd
-from src.prediction_engine.game_features import (
-    fetch_season_games,
-    fetch_game_features,
-    generate_features_for_games,
-    _cache_path,
-)
-
-EXPECTED_COLUMNS = ["PTS", "REB", "AST", "FG_PCT", "FT_PCT", "PLUS_MINUS", "TOV", "win"]
-
-def test_fetch_season_games_returns_ids():
-    game_ids = fetch_season_games(2023, limit=3)
-    assert isinstance(game_ids, list)
-    assert len(game_ids) <= 3
-    assert all(isinstance(gid, str) for gid in game_ids)
-
-def test_fetch_game_features_structure():
-    game_ids = fetch_season_games(2023, limit=1)
-    df = fetch_game_features(game_ids[0])
-    assert isinstance(df, pd.DataFrame)
-    # Validate schema consistency
-    for col in EXPECTED_COLUMNS:
-        assert col in df.columns
-
-def test_generate_features_for_games_combines_multiple():
-    game_ids = fetch_season_games(2023, limit=2)
-    df = generate_features_for_games(game_ids)
-    assert isinstance(df, pd.DataFrame)
-    assert len(df) >= 2  # At least one row per team per game
-    # Validate schema consistency
-    for col in EXPECTED_COLUMNS:
-        assert col in df.columns
-
-def test_cache_path_exists():
-    assert _cache_path.exists()
-    assert _cache_path.is_dir()
Index: tests/test_logging.py
===================================================================
diff --git a/tests/test_logging.py b/tests/test_logging.py
deleted file mode 100644
--- a/tests/test_logging.py	(revision 00055356329a845d3b2ca81b77c3ffb2e2b15969)
+++ /dev/null	(revision 00055356329a845d3b2ca81b77c3ffb2e2b15969)
@@ -1,31 +0,0 @@
-# ============================================================
-# Path: tests/test_logging.py
-# Filename: test_logging.py
-# Author: Your Team
-# Date: December 2025
-# Purpose: Tests for logging configuration
-# ============================================================
-
-import os
-import logging
-from src.utils.logging_config import configure_logging
-
-def test_configure_logging_creates_file_and_logs(tmp_path):
-    # Use a temporary log file
-    log_file = tmp_path / "test_app.log"
-
-    # Configure logging
-    configure_logging(str(log_file))
-
-    # Write a test log entry
-    logging.info("This is a test log entry")
-
-    # Ensure file exists
-    assert log_file.exists()
-
-    # Ensure file is not empty
-    content = log_file.read_text().strip()
-    assert len(content) > 0
-
-    # Ensure our test log entry is present
-    assert "This is a test log entry" in content
Index: tests/test_database_settings.py
===================================================================
diff --git a/tests/test_database_settings.py b/tests/test_database_settings.py
deleted file mode 100644
--- a/tests/test_database_settings.py	(revision 00055356329a845d3b2ca81b77c3ffb2e2b15969)
+++ /dev/null	(revision 00055356329a845d3b2ca81b77c3ffb2e2b15969)
@@ -1,36 +0,0 @@
-# ============================================================
-# Path: tests/test_database_settings.py
-# Filename: test_database_settings.py
-# Author: Your Team
-# Date: December 2025
-# Purpose: Tests for DatabaseSettings with and without env vars
-# ============================================================
-
-import pytest
-from src.config_loader import load_settings, Settings
-
-def test_database_settings_without_env(set_env):
-    """
-    When only NBA_API_KEY is set, database fields should be None.
-    """
-    settings = load_settings()
-    assert isinstance(settings, Settings)
-    assert settings.NBA_API_KEY == "dummy_key"
-    assert settings.database.host is None
-    assert settings.database.port is None
-    assert settings.database.user is None
-    assert settings.database.password is None
-
-@pytest.mark.with_db
-def test_database_settings_with_env(set_env):
-    """
-    When NBA_API_KEY and dummy database env vars are set,
-    database fields should be populated.
-    """
-    settings = load_settings()
-    assert isinstance(settings, Settings)
-    assert settings.NBA_API_KEY == "dummy_key"
-    assert settings.database.host == "localhost"
-    assert settings.database.port == 5432
-    assert settings.database.user == "test_user"
-    assert settings.database.password == "test_pass"
Index: tests/test_integration_pipeline_mlflow.py
===================================================================
diff --git a/tests/test_integration_pipeline_mlflow.py b/tests/test_integration_pipeline_mlflow.py
deleted file mode 100644
--- a/tests/test_integration_pipeline_mlflow.py	(revision 00055356329a845d3b2ca81b77c3ffb2e2b15969)
+++ /dev/null	(revision 00055356329a845d3b2ca81b77c3ffb2e2b15969)
@@ -1,74 +0,0 @@
-# ============================================================
-# Path: tests/test_integration_pipeline_mlflow.py
-# Filename: test_integration_pipeline_mlflow.py
-# Author: Your Team
-# Date: December 2025
-# Purpose: End-to-end integration test for MLflow pipeline
-# ============================================================
-
-import pytest
-import mlflow
-import pandas as pd
-from src.model_training.train_logreg import train_logreg
-from src.prediction_engine.game_features import (
-    fetch_season_games,
-    generate_features_for_games,
-)
-from src.prediction_engine.predictor import NBAPredictor
-
-@pytest.mark.integration
-def test_full_pipeline(tmp_path):
-    # -----------------------------
-    # Step 1: Fetch real NBA data
-    # -----------------------------
-    game_ids = fetch_season_games(2023, limit=3)
-    features = generate_features_for_games(game_ids)
-
-    # Validate schema consistency
-    expected_columns = ["PTS", "REB", "AST", "FG_PCT", "FT_PCT", "PLUS_MINUS", "TOV", "win"]
-    for col in expected_columns:
-        assert col in features.columns
-
-    features_path = tmp_path / "features.parquet"
-    features.to_parquet(features_path, index=False)
-
-    # -----------------------------
-    # Step 2: Train and log model
-    # -----------------------------
-    model_dir = tmp_path / "models"
-    model_dir.mkdir()
-    result = train_logreg(str(features_path), out_dir=str(model_dir))
-
-    # Ensure result dict contains metrics
-    assert "metrics" in result
-    assert "accuracy" in result["metrics"]
-    assert "f1_score" in result["metrics"]
-    assert (model_dir / "nba_logreg.pkl").exists()
-
-    # -----------------------------
-    # Step 3: Load model via NBAPredictor
-    # -----------------------------
-    predictor = NBAPredictor(model_path=str(model_dir / "nba_logreg.pkl"))
-
-    # -----------------------------
-    # Step 4: Make predictions
-    # -----------------------------
-    sample = features.drop(columns=["win"]).iloc[:2]
-    preds = predictor.predict(sample)
-    probas = predictor.predict_proba(sample)
-
-    # Validate predictions
-    assert isinstance(preds, pd.Series)
-    assert all(label in [0, 1] for label in preds)
-
-    # Validate probabilities are flat list of floats
-    assert isinstance(probas, list)
-    assert all(isinstance(p, float) for p in probas)
-    assert all(0 <= p <= 1 for p in probas)
-
-    # -----------------------------
-    # Step 5: Verify MLflow logging
-    # -----------------------------
-    client = mlflow.tracking.MlflowClient()
-    runs = client.search_runs(experiment_ids=["0"])
-    assert len(runs) > 0
Index: scripts/tasks.py
===================================================================
diff --git a/scripts/tasks.py b/scripts/tasks.py
deleted file mode 100644
--- a/scripts/tasks.py	(revision 00055356329a845d3b2ca81b77c3ffb2e2b15969)
+++ /dev/null	(revision 00055356329a845d3b2ca81b77c3ffb2e2b15969)
@@ -1,101 +0,0 @@
-# ============================================================
-# File: tasks.py
-# Purpose: Python-native task runner for nba_analysis workflows.
-# ============================================================
-
-from invoke import task, Collection
-
-@task
-def test(c, cov=False):
-    cmd = "pytest"
-    if cov:
-        cmd += " --cov=src --cov-report=term-missing"
-    c.run(cmd)
-
-@task
-def lint(c):
-    c.run("flake8 src tests")
-
-@task
-def format(c):
-    c.run("black src tests")
-    c.run("isort src tests")
-
-@task
-def typecheck(c):
-    c.run("mypy src")
-
-@task
-def security(c):
-    c.run("bandit -r src")
-
-@task
-def check(c):
-    format(c)
-    lint(c)
-    typecheck(c)
-    security(c)
-    test(c)
-
-@task
-def integration(c):
-    c.run("pytest tests/test_integration_pipeline_mlflow.py -v")
-
-@task
-def ci(c):
-    format(c)
-    lint(c)
-    typecheck(c)
-    security(c)
-    test(c, cov=True)
-    integration(c)
-
-@task
-def train(c):
-    c.run("python -m src.model_training.training --model_type all")
-
-@task
-def features(c, game_ids="", season=""):
-    if game_ids:
-        c.run(
-            f"python -m src.prediction_engine.game_features --save --game_ids={game_ids} --log_level=DEBUG"
-        )
-    elif season:
-        c.run(
-            f"python -m src.prediction_engine.game_features --save --season={season} --log_level=DEBUG"
-        )
-    else:
-        c.run(
-            "python -m src.prediction_engine.game_features --save --game_ids=0042300401,0022300649 --log_level=DEBUG"
-        )
-
-@task
-def mlflow(c):
-    c.run("mlflow ui --backend-store-uri file:./mlruns")
-
-@task
-def clean(c):
-    """Remove temporary files and caches."""
-    c.run("rm -rf .pytest_cache .mypy_cache .coverage htmlcov")
-    c.run("find . -type d -name '__pycache__' -exec rm -rf {} +")
-
-@task
-def docs(c):
-    """Build project documentation with Sphinx."""
-    c.run("sphinx-build -b html docs build/docs")
-
-# Default namespace
-ns = Collection()
-ns.add_task(test)
-ns.add_task(lint)
-ns.add_task(format)
-ns.add_task(typecheck)
-ns.add_task(security)
-ns.add_task(check)
-ns.add_task(integration)
-ns.add_task(ci, default=True)
-ns.add_task(train)
-ns.add_task(features)
-ns.add_task(mlflow)
-ns.add_task(clean)
-ns.add_task(docs)
Index: tests/test_config_loader.py
===================================================================
diff --git a/tests/test_config_loader.py b/tests/test_config_loader.py
deleted file mode 100644
--- a/tests/test_config_loader.py	(revision 00055356329a845d3b2ca81b77c3ffb2e2b15969)
+++ /dev/null	(revision 00055356329a845d3b2ca81b77c3ffb2e2b15969)
@@ -1,19 +0,0 @@
-# ============================================================
-# Path: tests/test_config_loader.py
-# Purpose: Unit tests for config_loader.py with .env and JSON fallback
-# Project: nba_analysis
-# ============================================================
-
-import pytest
-from pydantic import ValidationError
-from src.config_loader import load_settings, Settings
-
-def test_load_settings_valid(set_env):
-    settings = load_settings()
-    assert isinstance(settings, Settings)
-    assert settings.NBA_API_KEY == "dummy_key"
-
-def test_load_settings_invalid(monkeypatch):
-    monkeypatch.delenv("NBA_API_KEY", raising=False)
-    with pytest.raises(ValidationError):
-        load_settings()
Index: src/model_inference/predictor.py
===================================================================
diff --git a/src/model_inference/predictor.py b/src/model_inference/predictor.py
deleted file mode 100644
--- a/src/model_inference/predictor.py	(revision 00055356329a845d3b2ca81b77c3ffb2e2b15969)
+++ /dev/null	(revision 00055356329a845d3b2ca81b77c3ffb2e2b15969)
@@ -1,57 +0,0 @@
-# ============================================================
-# Path: src/model_inference/predictor.py
-# File: predictor.py
-# Purpose: Robust predictor wrapper for probability and label outputs
-# Project: nba_analysis
-# ============================================================
-
-import numpy as np
-import pandas as pd
-from sklearn.base import BaseEstimator
-from scipy.special import softmax
-
-
-class Predictor:
-    """
-    Wraps a scikit-learn model to provide probability and label predictions.
-    Supports predict_proba, decision_function fallback, and thresholded labels.
-    """
-
-    def __init__(self, model: BaseEstimator):
-        self.model = model
-
-    def _validate_input(self, X):
-        if not isinstance(X, (pd.DataFrame, np.ndarray)):
-            raise TypeError("Input must be a pandas DataFrame or NumPy array.")
-        if len(X) == 0:
-            raise ValueError("Input data is empty.")
-
-    def predict_proba(self, X: pd.DataFrame) -> np.ndarray:
-        """
-        Predict class probabilities. Falls back to decision_function if predict_proba is unavailable.
-        """
-        self._validate_input(X)
-
-        if hasattr(self.model, "predict_proba"):
-            return self.model.predict_proba(X)
-
-        if hasattr(self.model, "decision_function"):
-            scores = self.model.decision_function(X)
-            if scores.ndim == 1:  # binary
-                probs = (scores - scores.min()) / (scores.max() - scores.min() + 1e-9)
-                return np.vstack([1 - probs, probs]).T
-            else:  # multi-class
-                return softmax(scores, axis=1)
-
-        raise AttributeError("Model does not support predict_proba or decision_function")
-
-    def predict_label(self, X: pd.DataFrame, threshold: float = 0.5) -> np.ndarray:
-        """
-        Predict class labels. For binary classification, allows thresholding on probabilities.
-        """
-        self._validate_input(X)
-
-        proba = self.predict_proba(X)
-        if proba.shape[1] == 2:  # binary
-            return (proba[:, 1] >= threshold).astype(int)
-        return self.model.predict(X)
Index: src/main.py
===================================================================
diff --git a/src/main.py b/src/main.py
deleted file mode 100644
--- a/src/main.py	(revision 00055356329a845d3b2ca81b77c3ffb2e2b15969)
+++ /dev/null	(revision 00055356329a845d3b2ca81b77c3ffb2e2b15969)
@@ -1,82 +0,0 @@
-# ============================================================
-# File: src/main.py
-# Purpose: CLI entrypoint for NBA analysis pipeline
-# ============================================================
-
-import argparse
-from pathlib import Path
-
-import pandas as pd
-
-from src.model_training.training import train_logreg
-from src.prediction_engine.game_features import generate_features_for_games
-from src.prediction_engine.predictor import NBAPredictor
-from src.utils.io import load_dataframe, save_dataframe
-from src.utils.logging import configure_logging
-
-logger = configure_logging(level="INFO", log_dir="logs", name="main")
-
-
-def run_pipeline(game_ids: list[str], season: int, out_dir: str = "results"):
-    """Run full pipeline: feature generation → training → prediction."""
-    logger.info("Starting NBA analysis pipeline...")
-
-    # --- Step 1: Generate features ---
-    features = generate_features_for_games(game_ids, season=season)
-    features_path = Path(out_dir) / "features.parquet"
-    save_dataframe(features, features_path)
-    logger.info(f"Features saved to {features_path}")
-
-    # --- Step 2: Train model ---
-    result = train_logreg(str(features_path), out_dir="models")
-    model_path = result["model_path"]
-    logger.info(f"Model trained and saved to {model_path}")
-
-    # --- Step 3: Predict outcomes ---
-    df_loaded = load_dataframe(features_path)
-    X = df_loaded.drop(columns=["game_id"], errors="ignore")
-    predictor = NBAPredictor(model_path)
-    proba = predictor.predict_proba(X)
-    labels = predictor.predict_label(X)
-
-    predictions = pd.DataFrame(
-        {
-            "game_id": df_loaded.get("game_id", range(len(X))),
-            "win_proba": proba,
-            "win_pred": labels,
-        }
-    )
-
-    pred_path = Path(out_dir) / "predictions.csv"
-    predictions.to_csv(pred_path, index=False)
-    logger.info(f"Predictions saved to {pred_path}")
-
-    logger.info("Pipeline complete.")
-    return pred_path
-
-
-def main():
-    parser = argparse.ArgumentParser(description="NBA Analysis Pipeline")
-    parser.add_argument(
-        "--game_ids",
-        type=str,
-        default="0042300401,0022300649",
-        help="Comma-separated list of NBA game IDs",
-    )
-    parser.add_argument(
-        "--season", type=int, default=2023, help="NBA season year (e.g., 2023)"
-    )
-    parser.add_argument(
-        "--out_dir",
-        type=str,
-        default="results",
-        help="Output directory for features and predictions",
-    )
-    args = parser.parse_args()
-
-    game_ids = args.game_ids.split(",")
-    run_pipeline(game_ids, args.season, args.out_dir)
-
-
-if __name__ == "__main__":
-    main()
Index: src/config_loader.py
===================================================================
diff --git a/src/config_loader.py b/src/config_loader.py
deleted file mode 100644
--- a/src/config_loader.py	(revision 00055356329a845d3b2ca81b77c3ffb2e2b15969)
+++ /dev/null	(revision 00055356329a845d3b2ca81b77c3ffb2e2b15969)
@@ -1,41 +0,0 @@
-# ============================================================
-# Path: src/config_loader.py
-# Filename: config_loader.py
-# Author: Your Team
-# Date: December 2025
-# Purpose: Load and validate application configuration using
-#          Pydantic Settings. Database fields are optional by
-#          default and mapped to environment variables.
-# ============================================================
-
-from typing import Optional
-try:
-    # Python 3.8+ has Literal in typing
-    from typing import Literal
-except ImportError:
-    # Fallback for Python <3.8
-    from typing_extensions import Literal
-
-from pydantic_settings import BaseSettings
-from pydantic import Field, ValidationError
-
-class DatabaseSettings(BaseSettings):
-    host: Optional[str] = Field(default=None, env="DATABASE_HOST")
-    port: Optional[int] = Field(default=None, env="DATABASE_PORT")
-    user: Optional[str] = Field(default=None, env="DATABASE_USER")
-    password: Optional[str] = Field(default=None, env="DATABASE_PASSWORD")
-
-class LoggingSettings(BaseSettings):
-    level: Literal["DEBUG", "INFO", "WARNING", "ERROR", "CRITICAL"] = "INFO"
-
-class Settings(BaseSettings):
-    NBA_API_KEY: str
-    database: DatabaseSettings = DatabaseSettings()
-    logging: LoggingSettings = LoggingSettings()
-
-def load_settings() -> Settings:
-    """
-    Load settings from environment variables.
-    Raises ValidationError if required values are missing.
-    """
-    return Settings()
Index: src/model_training/training.py
===================================================================
diff --git a/src/model_training/training.py b/src/model_training/training.py
deleted file mode 100644
--- a/src/model_training/training.py	(revision 00055356329a845d3b2ca81b77c3ffb2e2b15969)
+++ /dev/null	(revision 00055356329a845d3b2ca81b77c3ffb2e2b15969)
@@ -1,88 +0,0 @@
-# ============================================================
-# Path: src/model_training/training.py
-# File: training.py
-# Purpose: Train logistic regression model and log results with MLflow
-# Project: nba_analysis
-# ============================================================
-
-import mlflow
-import mlflow.sklearn
-import pandas as pd
-from sklearn.linear_model import LogisticRegression
-from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
-from sklearn.model_selection import train_test_split
-
-from src.utils.io import load_dataframe
-
-
-def train_logreg(features_path: str, out_dir: str, test_size: float = 0.2, random_state: int = 42):
-    """
-    Train a logistic regression model using features stored in a parquet file.
-    Logs metrics and model artifacts to MLflow.
-
-    Parameters
-    ----------
-    features_path : str
-        Path to the parquet file containing features and target column 'win'.
-    out_dir : str
-        Directory to save model artifacts.
-    test_size : float
-        Fraction of data to use for validation.
-    random_state : int
-        Random seed for reproducibility.
-
-    Returns
-    -------
-    model : LogisticRegression
-        Trained logistic regression model.
-    metrics : dict
-        Dictionary of evaluation metrics.
-    """
-    # Load features
-    df = load_dataframe(features_path)
-    if "win" not in df.columns:
-        raise ValueError("Training data must include a 'win' column as target label")
-
-    # Split into features and target
-    X = df.drop(columns=["win"])
-    y = df["win"]
-
-    if X.empty or y.empty:
-        raise ValueError("Training data is empty")
-
-    # Train/test split
-    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=test_size, random_state=random_state)
-
-    # Train model
-    model = LogisticRegression(max_iter=1000)
-    try:
-        model.fit(X_train, y_train)
-    except Exception as e:
-        raise RuntimeError(f"Model training failed: {e}")
-
-    # Evaluate
-    preds = model.predict(X_val)
-    acc = accuracy_score(y_val, preds)
-    precision = precision_score(y_val, preds, zero_division=0)
-    recall = recall_score(y_val, preds, zero_division=0)
-    f1 = f1_score(y_val, preds, zero_division=0)
-
-    metrics = {
-        "accuracy": acc,
-        "precision": precision,
-        "recall": recall,
-        "f1_score": f1,
-    }
-
-    # Log to MLflow
-    with mlflow.start_run():
-        mlflow.log_param("max_iter", 1000)
-        mlflow.log_param("solver", model.solver)
-        mlflow.log_param("C", model.C)
-
-        for k, v in metrics.items():
-            mlflow.log_metric(k, v)
-
-        mlflow.sklearn.log_model(model, "logreg_model")
-
-    return model, metrics
Index: .github/workflows/tests.yml
===================================================================
diff --git a/.github/workflows/tests.yml b/.github/workflows/tests.yml
deleted file mode 100644
--- a/.github/workflows/tests.yml	(revision 00055356329a845d3b2ca81b77c3ffb2e2b15969)
+++ /dev/null	(revision 00055356329a845d3b2ca81b77c3ffb2e2b15969)
@@ -1,14 +0,0 @@
-name: CI
-on: [push, pull_request]
-jobs:
-  ci:
-    runs-on: ubuntu-latest
-    steps:
-      - uses: actions/checkout@v4
-      - uses: actions/setup-python@v5
-        with:
-          python-version: "3.12"
-      - run: pip install --upgrade pip setuptools wheel
-      - run: pip install -r requirements.txt
-      - run: pip install "protobuf>=5.0,<7.0" "packaging>=24.2" mlflow==2.9.2
-      - run: make ci
Index: tests/test_train_logreg.py
===================================================================
diff --git a/tests/test_train_logreg.py b/tests/test_train_logreg.py
deleted file mode 100644
--- a/tests/test_train_logreg.py	(revision 00055356329a845d3b2ca81b77c3ffb2e2b15969)
+++ /dev/null	(revision 00055356329a845d3b2ca81b77c3ffb2e2b15969)
@@ -1,55 +0,0 @@
-# ============================================================
-# Path: tests/test_train_logreg.py
-# Filename: test_train_logreg.py
-# Author: Your Team
-# Date: December 9, 2025
-# Purpose: Unit tests for train_logreg function
-# ============================================================
-
-import os
-import pandas as pd
-import joblib
-import pytest
-
-from src.model_training.train_logreg import train_logreg
-
-@pytest.fixture
-def sample_features(tmp_path):
-    """Create a small sample dataset for testing."""
-    df = pd.DataFrame({
-        "PTS": [100, 110, 95, 120],
-        "REB": [40, 42, 38, 45],
-        "AST": [20, 18, 22, 25],
-        "FG_PCT": [0.45, 0.50, 0.42, 0.55],
-        "FT_PCT": [0.70, 0.75, 0.68, 0.80],
-        "PLUS_MINUS": [5, -3, 10, -8],
-        "TOV": [12, 10, 14, 9],
-        "win": [1, 0, 1, 0]
-    })
-    path = tmp_path / "features.parquet"
-    df.to_parquet(path, index=False)
-    return path
-
-def test_train_logreg_creates_model_and_metrics(sample_features, tmp_path):
-    """Ensure train_logreg trains, saves model, and returns metrics."""
-    out_dir = tmp_path / "models"
-    result = train_logreg(str(sample_features), out_dir=str(out_dir))
-
-    # Check result keys
-    assert "metrics" in result
-    assert "model_path" in result
-
-    # Check metrics structure
-    metrics = result["metrics"]
-    assert "accuracy" in metrics
-    assert "f1_score" in metrics
-    assert isinstance(metrics["accuracy"], float)
-    assert isinstance(metrics["f1_score"], float)
-
-    # Check model file exists
-    model_path = result["model_path"]
-    assert os.path.exists(model_path)
-
-    # Check model can be loaded
-    model = joblib.load(model_path)
-    assert hasattr(model, "predict")
Index: tests/test_utils.py
===================================================================
diff --git a/tests/test_utils.py b/tests/test_utils.py
deleted file mode 100644
--- a/tests/test_utils.py	(revision 00055356329a845d3b2ca81b77c3ffb2e2b15969)
+++ /dev/null	(revision 00055356329a845d3b2ca81b77c3ffb2e2b15969)
@@ -1,17 +0,0 @@
-import pandas as pd
-from src.utils import add_unique_id
-
-def test_add_unique_id():
-    df = pd.DataFrame({"GAME_ID":[1], "TEAM_ID":[100], "prediction_date":["2025-12-09"]})
-    df = add_unique_id(df)
-    assert "unique_id" in df.columns
-    assert df["unique_id"].nunique() == len(df)
-
-def test_deduplication():
-    df = pd.DataFrame({
-        "GAME_ID":[1,1],
-        "TEAM_ID":[100,100],
-        "prediction_date":["2025-12-09","2025-12-09"]
-    })
-    df = add_unique_id(df).drop_duplicates(subset=["unique_id"])
-    assert len(df) == 1
Index: src/prediction_engine/game_features.py
===================================================================
diff --git a/src/prediction_engine/game_features.py b/src/prediction_engine/game_features.py
deleted file mode 100644
--- a/src/prediction_engine/game_features.py	(revision 00055356329a845d3b2ca81b77c3ffb2e2b15969)
+++ /dev/null	(revision 00055356329a845d3b2ca81b77c3ffb2e2b15969)
@@ -1,94 +0,0 @@
-# ============================================================
-# Path: src/prediction_engine/game_features.py
-# Filename: game_features.py
-# Author: Your Team
-# Date: December 2025
-# Purpose: Functions to fetch NBA game IDs and transform
-#          raw game stats into model-ready features.
-# ============================================================
-
-import pathlib
-import pandas as pd
-from nba_api.stats.endpoints import boxscoretraditionalv2, leaguegamefinder
-
-# -----------------------------
-# Local cache path (used in tests)
-# -----------------------------
-_cache_path = pathlib.Path("results/cache")
-_cache_path.mkdir(parents=True, exist_ok=True)
-
-# -----------------------------
-# Fetch game IDs for a season
-# -----------------------------
-def fetch_season_games(season_year: int, limit: int = 10):
-    """
-    Fetch a list of NBA game IDs for a given season.
-
-    Args:
-        season_year (int): The starting year of the season (e.g., 2023 for 2023-24).
-        limit (int): Number of game IDs to return.
-
-    Returns:
-        list[str]: A list of game IDs.
-    """
-    season_str = f"{season_year}-{str(season_year+1)[-2:]}"
-    gamefinder = leaguegamefinder.LeagueGameFinder(season_nullable=season_str)
-    games = gamefinder.get_data_frames()[0]
-    game_ids = games["GAME_ID"].unique().tolist()
-    return game_ids[:limit]
-
-# -----------------------------
-# Fetch features for a single game
-# -----------------------------
-def fetch_game_features(game_id: str) -> pd.DataFrame:
-    """
-    Fetch box score stats for a given game and return model-ready features.
-
-    Args:
-        game_id (str): NBA game ID.
-
-    Returns:
-        pd.DataFrame: DataFrame with selected features and win/loss label.
-    """
-    boxscore = boxscoretraditionalv2.BoxScoreTraditionalV2(game_id=game_id)
-    stats = boxscore.get_data_frames()[0]
-
-    # Define the columns we want to aggregate
-    required_columns = ["PTS", "REB", "AST", "FG_PCT", "FT_PCT", "PLUS_MINUS", "TOV"]
-
-    # Build aggregation dict only for columns that exist
-    agg_dict = {}
-    for col in required_columns:
-        if col in stats.columns:
-            if col in ["FG_PCT", "FT_PCT"]:
-                agg_dict[col] = "mean"
-            else:
-                agg_dict[col] = "sum"
-
-    team_stats = stats.groupby("TEAM_ID").agg(agg_dict).reset_index()
-
-    # Ensure all required columns exist, fill with default if missing
-    for col in required_columns:
-        if col not in team_stats.columns:
-            team_stats[col] = 0
-
-    # Add win/loss label: positive PLUS_MINUS → win
-    team_stats["win"] = (team_stats["PLUS_MINUS"] > 0).astype(int)
-
-    return team_stats
-
-# -----------------------------
-# Generate features for multiple games
-# -----------------------------
-def generate_features_for_games(game_ids: list[str]) -> pd.DataFrame:
-    """
-    Generate features for a list of game IDs.
-
-    Args:
-        game_ids (list[str]): List of NBA game IDs.
-
-    Returns:
-        pd.DataFrame: Concatenated features for all games.
-    """
-    features = pd.concat([fetch_game_features(gid) for gid in game_ids], ignore_index=True)
-    return features
Index: src/utils/logging.py
===================================================================
diff --git a/src/utils/logging.py b/src/utils/logging.py
deleted file mode 100644
--- a/src/utils/logging.py	(revision 00055356329a845d3b2ca81b77c3ffb2e2b15969)
+++ /dev/null	(revision 00055356329a845d3b2ca81b77c3ffb2e2b15969)
@@ -1,66 +0,0 @@
-# ============================================================
-# File: src/utils/logging.py
-# Purpose: Central logging configuration for NBA analysis project
-# Project: nba_analysis
-# ============================================================
-
-import os
-import sys
-from loguru import logger
-
-# Track whether logging has already been configured
-_LOGGING_CONFIGURED = False
-
-
-def configure_logging(
-    level: str = "INFO",
-    log_dir: str = "logs",
-    name: str = "nba_analysis",
-    rotation: str = "10 MB",
-    retention: str = "7 days",
-):
-    """Configure Loguru logging with file + stdout sinks and contextual binding."""
-    global _LOGGING_CONFIGURED
-
-    if not _LOGGING_CONFIGURED:
-        os.makedirs(log_dir, exist_ok=True)
-        log_path = os.path.join(log_dir, f"{name}.log")
-
-        fmt = "{time:YYYY-MM-DD HH:mm:ss} | {level} | {name}:{function}:{line} - {message}"
-
-        # Reset previous handlers
-        logger.remove()
-
-        # File sink
-        logger.add(
-            log_path,
-            rotation=rotation,
-            retention=retention,
-            level=level,
-            format=fmt,
-            enqueue=True,  # safer for multi-process logging
-        )
-
-        # Console sink
-        logger.add(sys.stdout, level=level, format=fmt)
-
-        # Catch uncaught exceptions
-        def exception_handler(exc_type, exc_value, exc_traceback):
-            logger.opt(exception=True).error(
-                "Uncaught exception:", exc_info=(exc_type, exc_value, exc_traceback)
-            )
-
-        sys.excepthook = exception_handler
-
-        _LOGGING_CONFIGURED = True
-
-    # Return a contextual logger bound with module name
-    return logger.bind(name=name)
-
-
-def get_logger(name: str) -> logger:
-    """
-    Retrieve a contextual logger for a given module name.
-    Ensures logging is configured once globally.
-    """
-    return configure_logging(name=name)
Index: tests/test_validation.py
===================================================================
diff --git a/tests/test_validation.py b/tests/test_validation.py
deleted file mode 100644
--- a/tests/test_validation.py	(revision 00055356329a845d3b2ca81b77c3ffb2e2b15969)
+++ /dev/null	(revision 00055356329a845d3b2ca81b77c3ffb2e2b15969)
@@ -1,27 +0,0 @@
-# ============================================================
-# Path: tests/test_validation.py
-# Purpose: Unit tests for src/utils/validation.py
-# Project: nba_analysis
-# ============================================================
-
-import pytest
-from src.utils.validation import validate_config
-
-
-def test_validate_config_valid(tmp_path):
-    cfg = {
-        "season": "2025",
-        "output_dir": tmp_path,
-        "stat": "points",
-        "lineup": ["1234567","2345678","3456789","4567890","5678901"]
-    }
-    validated = validate_config(cfg, ["season","output_dir","stat","lineup"])
-    assert validated["season"] == "2024-25"
-    assert validated["stat"] == "points"
-    assert len(validated["lineup"]) == 5
-
-
-def test_validate_config_missing_key(tmp_path):
-    cfg = {"season": "2025"}
-    with pytest.raises(ValueError):
-        validate_config(cfg, ["season","output_dir"])
Index: tests/test_training.py
===================================================================
diff --git a/tests/test_training.py b/tests/test_training.py
deleted file mode 100644
--- a/tests/test_training.py	(revision 00055356329a845d3b2ca81b77c3ffb2e2b15969)
+++ /dev/null	(revision 00055356329a845d3b2ca81b77c3ffb2e2b15969)
@@ -1,49 +0,0 @@
-# ============================================================
-# Path: tests/test_training.py
-# Purpose: Unit tests for src/model_training/training.py
-# Project: nba_analysis
-# ============================================================
-
-import pytest
-import pandas as pd
-from src.model_training.training import train_logreg
-
-def test_train_logreg_valid(monkeypatch, tmp_path):
-    # Create synthetic dataset
-    df = pd.DataFrame({
-        "feat1": [0, 1, 0, 1, 0, 1],
-        "feat2": [1, 0, 1, 0, 1, 0],
-        "win":   [0, 1, 0, 1, 0, 1],
-    })
-    features_path = tmp_path / "features.parquet"
-    df.to_parquet(features_path)
-
-    # Mock mlflow functions
-    monkeypatch.setattr("mlflow.start_run", lambda *a, **kw: __import__("contextlib").nullcontext())
-    monkeypatch.setattr("mlflow.log_param", lambda *a, **kw: None)
-    monkeypatch.setattr("mlflow.log_metric", lambda *a, **kw: None)
-    monkeypatch.setattr("mlflow.sklearn.log_model", lambda *a, **kw: None)
-
-    model, metrics = train_logreg(str(features_path), out_dir=str(tmp_path))
-    assert "accuracy" in metrics
-    assert "precision" in metrics
-    assert "recall" in metrics
-    assert "f1_score" in metrics
-
-
-def test_train_logreg_missing_target(tmp_path):
-    df = pd.DataFrame({"feat1": [0, 1], "feat2": [1, 0]})
-    features_path = tmp_path / "features.parquet"
-    df.to_parquet(features_path)
-
-    with pytest.raises(ValueError):
-        train_logreg(str(features_path), out_dir=str(tmp_path))
-
-
-def test_train_logreg_empty_data(tmp_path):
-    df = pd.DataFrame(columns=["feat1", "feat2", "win"])
-    features_path = tmp_path / "features.parquet"
-    df.to_parquet(features_path)
-
-    with pytest.raises(ValueError):
-        train_logreg(str(features_path), out_dir=str(tmp_path))
Index: tests/test_mlflow_setup.py
===================================================================
diff --git a/tests/test_mlflow_setup.py b/tests/test_mlflow_setup.py
deleted file mode 100644
--- a/tests/test_mlflow_setup.py	(revision 00055356329a845d3b2ca81b77c3ffb2e2b15969)
+++ /dev/null	(revision 00055356329a845d3b2ca81b77c3ffb2e2b15969)
@@ -1,27 +0,0 @@
-# ============================================================
-# Path: tests/test_mlflow_setup.py
-# Purpose: Unit tests for mlflow_setup.py with metadata + system info + packages
-# Project: nba_analysis
-# ============================================================
-
-import mlflow
-from src.mlflow_setup import configure_mlflow, start_run_with_metadata
-
-
-def test_start_run_with_packages(monkeypatch, tmp_path):
-    monkeypatch.setenv("MLFLOW_TRACKING_URI", f"file:{tmp_path}/mlruns")
-    monkeypatch.setenv("MLFLOW_EXPERIMENT_NAME", "TestExperiment")
-
-    configure_mlflow()
-    run = start_run_with_metadata(run_name="test_run")
-
-    data = mlflow.get_run(run.info.run_id).data
-    tags = data.tags
-    params = data.params
-
-    assert tags["project"] == "nba_analysis"
-    assert "timestamp" in tags
-    assert "git_commit" in tags
-    assert "python_version" in tags
-    assert "os" in tags
-    assert any(k.startswith("pkg_") for k in params)
Index: src/model_training/train_logreg.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/src/model_training/train_logreg.py b/src/model_training/train_logreg.py
deleted file mode 100644
--- a/src/model_training/train_logreg.py	(revision 00055356329a845d3b2ca81b77c3ffb2e2b15969)
+++ /dev/null	(revision 00055356329a845d3b2ca81b77c3ffb2e2b15969)
@@ -1,83 +0,0 @@
-# ============================================================
-# Path: src/model_training/train_logreg.py
-# Filename: train_logreg.py
-# Author: Your Team
-# Date: December 9, 2025
-# Purpose: Train and log NBA logistic regression model
-# ============================================================
-
-import pandas as pd
-import joblib
-import os
-from sklearn.linear_model import LogisticRegression
-from sklearn.metrics import accuracy_score, f1_score
-from sklearn.preprocessing import OneHotEncoder
-from sklearn.compose import ColumnTransformer
-from sklearn.pipeline import Pipeline
-import mlflow
-import json
-
-def train_logreg(features_path: str, out_dir: str = "models") -> dict:
-    """
-    Train a logistic regression model on NBA features and log metrics.
-
-    Args:
-        features_path (str): Path to parquet file with features.
-        out_dir (str): Directory to save model.
-
-    Returns:
-        dict: Training metrics and model path.
-    """
-    # Load features
-    df = pd.read_parquet(features_path)
-    y = df["win"]
-
-    # Drop target + known non-numeric columns that shouldn't be used
-    drop_cols = ["win", "unique_id", "prediction_date"]
-    X = df.drop(columns=[c for c in drop_cols if c in df.columns])
-
-    # Identify categorical vs numeric
-    categorical_cols = X.select_dtypes(include=["object"]).columns.tolist()
-    numeric_cols = X.select_dtypes(include=["number"]).columns.tolist()
-
-    # Preprocessing: OneHotEncode categorical, passthrough numeric
-    preprocessor = ColumnTransformer(
-        transformers=[
-            ("categorical", OneHotEncoder(handle_unknown="ignore"), categorical_cols),
-            ("numeric", "passthrough", numeric_cols)
-        ]
-    )
-
-    # Build pipeline
-    model = Pipeline(steps=[
-        ("preprocessor", preprocessor),
-        ("classifier", LogisticRegression(max_iter=1000))
-    ])
-
-    # Train model
-    model.fit(X, y)
-
-    # Evaluate
-    preds = model.predict(X)
-    acc = accuracy_score(y, preds)
-    f1 = f1_score(y, preds)
-
-    # Save model
-    os.makedirs(out_dir, exist_ok=True)
-    model_path = os.path.join(out_dir, "nba_logreg.pkl")
-    joblib.dump(model, model_path)
-
-    # Log metrics with MLflow
-    mlflow.log_metric("accuracy", acc)
-    mlflow.log_metric("f1_score", f1)
-
-    # Save metrics to JSON for tracking
-    metrics_path = os.path.join(out_dir, "training_metrics.json")
-    with open(metrics_path, "w") as f:
-        json.dump({"accuracy": acc, "f1_score": f1}, f)
-
-    return {
-        "metrics": {"accuracy": acc, "f1_score": f1},
-        "model_path": model_path,
-        "metrics_path": metrics_path
-    }
Index: tasks.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/tasks.py b/tasks.py
new file mode 100644
--- /dev/null	(date 1765470426268)
+++ b/tasks.py	(date 1765470426268)
@@ -0,0 +1,79 @@
+# ============================================================
+# File: tasks.py
+# Purpose: Invoke task definitions for NBA AI project
+# Project: nba_analysis
+# Version: 1.1 (adds default pipeline task chaining)
+#
+# Dependencies:
+# - invoke
+# ============================================================
+
+from invoke import task
+
+
+@task
+def check(c):
+    """Run local validation pipeline (tests + lint)."""
+    c.run("pytest -q")
+    c.run("flake8 src")
+
+
+@task
+def ci(c):
+    """Run CI pipeline (tests + lint + type checks)."""
+    c.run("pytest --maxfail=1 --disable-warnings -q")
+    c.run("flake8 src")
+    c.run("mypy src")
+
+
+@task
+def clean(c):
+    """Clean caches and temp files."""
+    c.run("rm -rf .pytest_cache build dist *.egg-info")
+    c.run("find . -name '__pycache__' -exec rm -rf {} +")
+
+
+@task
+def docs(c):
+    """Build documentation."""
+    c.run("sphinx-build -b html docs build/docs")
+
+
+@task
+def serve_docs(c):
+    """Serve documentation locally."""
+    c.run("python -m http.server --directory build/docs 8000")
+
+
+@task
+def train(c):
+    """Train model."""
+    c.run("python src/model_training/train_combined.py")
+
+
+@task
+def features(c):
+    """Generate features."""
+    c.run("python src/features/feature_engineering.py")
+
+
+@task
+def mlflow(c):
+    """Launch MLflow UI."""
+    c.run("mlflow ui --backend-store-uri mlruns")
+
+
+@task
+def run(c, date="today"):
+    """Run pipeline script for predictions."""
+    c.run(f"python src/run_pipeline.py --date {date}")
+
+
+# --- Default pipeline task chaining ---
+@task(pre=[clean, check, features, train, run])
+def pipeline(c):
+    """
+    Run the full end-to-end pipeline:
+    clean -> check -> features -> train -> run
+    """
+    print("✅ Full pipeline completed successfully.")
Index: run_today.sh
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/run_today.sh b/run_today.sh
new file mode 100644
--- /dev/null	(date 1765470426238)
+++ b/run_today.sh	(date 1765470426238)
@@ -0,0 +1,46 @@
+#!/bin/bash
+set -euo pipefail
+
+PYTHON_ENV="nba_env"
+PROJECT_DIR="$HOME/nba_project"
+TODAY=$(date +%Y-%m-%d)
+OUTPUT_DIR="$PROJECT_DIR/results/$TODAY"
+PBI_DIR="$HOME/pbi_folder"
+
+mkdir -p "$PROJECT_DIR/data/cache" "$OUTPUT_DIR" "$PBI_DIR"
+
+LOG_FILE="$OUTPUT_DIR/run_today.log"
+exec > >(tee -a "$LOG_FILE") 2>&1
+
+echo "[INFO] Activating Python environment..."
+source ~/miniconda3/etc/profile.d/conda.sh
+conda activate "$PYTHON_ENV"
+
+cd "$PROJECT_DIR" || { echo "[ERROR] Project dir not found"; exit 1; }
+
+echo "[INFO] Generating today's schedule/features..."
+python -m src.scripts.generate_today_schedule || { echo "[ERROR] Schedule generation failed"; exit 1; }
+
+MODEL_PATH="$PROJECT_DIR/models/logreg.pkl"
+if [ ! -f "$MODEL_PATH" ]; then
+    echo "[ERROR] Model file not found at $MODEL_PATH"
+    exit 1
+fi
+
+echo "[INFO] Running today's pipeline..."
+python -m src.main_today \
+    --schedule_file "$PROJECT_DIR/data/cache/schedule.parquet" \
+    --model_path "$MODEL_PATH" \
+    --out_dir "$OUTPUT_DIR"
+
+echo "[INFO] Copying outputs to Power BI folder..."
+for f in todays_picks.csv bet_on.csv avoid.csv; do
+    if [ -f "$OUTPUT_DIR/$f" ]; then
+        cp "$OUTPUT_DIR/$f" "$PBI_DIR/"
+        echo "[INFO] Copied $f to Power BI folder"
+    else
+        echo "[WARN] $f not found in $OUTPUT_DIR"
+    fi
+done
+
+echo "[INFO] Daily NBA pipeline completed. Outputs ready for Power BI."
Index: src/config.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/src/config.py b/src/config.py
new file mode 100644
--- /dev/null	(date 1765470426243)
+++ b/src/config.py	(date 1765470426243)
@@ -0,0 +1,140 @@
+# ============================================================
+# File: src/config.py
+# Purpose: Unified configuration loader for NBA analysis pipeline
+# Project: nba_analysis
+# Version: 2.0 (merges schema + loader)
+# ============================================================
+
+from typing import List, Optional
+from pydantic import BaseModel, Field, ValidationError
+from pydantic_settings import BaseSettings
+import yaml
+
+
+# -----------------------------
+# PATH CONFIGURATION
+# -----------------------------
+class Paths(BaseModel):
+    raw: str = "data/raw"
+    cache: str = "data/cache"
+    history: str = "data/history"
+    csv: str = "data/csv"
+    parquet: str = "data/parquet"
+    logs: str = "data/logs"
+    models: str = "models"
+    mlflow_artifacts: Optional[str] = "mlruns"
+    analytics: Optional[str] = "data/analytics"
+
+
+# -----------------------------
+# NBA SETTINGS
+# -----------------------------
+class NBASettings(BaseModel):
+    seasons: List[str]
+    default_year: int
+    players_min_points: int = Field(..., gt=0)
+    fetch_retries: int = Field(default=3, ge=1)
+    retry_delay_ms: int = Field(default=1500, ge=0)
+
+
+# -----------------------------
+# MODEL SETTINGS
+# -----------------------------
+class ModelSettings(BaseModel):
+    path: str = Field(default="models/nba_logreg.pkl", env="MODEL_PATH")
+    type: str = Field(default="xgb", env="MODEL_TYPE")
+    threshold: float = Field(default=0.5, ge=0.0, le=1.0)
+    device: str = Field(default="cpu", env="DEVICE")
+    feature_version: str = "v1"
+
+
+# -----------------------------
+# OUTPUT SETTINGS
+# -----------------------------
+class OutputSettings(BaseModel):
+    save_csv: bool = True
+    save_parquet: bool = False
+    pretty_json: bool = True
+    include_player_stats: bool = True
+    include_betting_fields: bool = True
+
+
+# -----------------------------
+# LOGGING SETTINGS
+# -----------------------------
+class LoggingSettings(BaseModel):
+    level: str = Field(default="INFO", env="LOG_LEVEL")
+    file: str = Field(default="data/logs/pipeline.log", env="LOG_FILE")
+
+
+# -----------------------------
+# MLFLOW SETTINGS
+# -----------------------------
+class MLflowSettings(BaseModel):
+    enabled: bool = True
+    experiment: str = "nba_predictions"
+    run_prefix: str = "daily_prediction_"
+    log_avg_probability: bool = True
+    log_model_path: bool = True
+    tracking_uri: Optional[str] = None
+    artifact_location: Optional[str] = None
+
+
+# -----------------------------
+# BETTING SETTINGS
+# -----------------------------
+class BettingSettings(BaseModel):
+    threshold: float = Field(default=0.55, ge=0.0, le=1.0)
+
+
+# -----------------------------
+# RUNNER SETTINGS
+# -----------------------------
+class RunnerSettings(BaseModel):
+    shap_enabled: bool = False
+    commit_outputs: bool = True
+
+
+# -----------------------------
+# DATABASE SETTINGS
+# -----------------------------
+class DatabaseSettings(BaseModel):
+    host: Optional[str] = Field(default=None, env="DATABASE_HOST")
+    port: Optional[int] = Field(default=None, env="DATABASE_PORT")
+    user: Optional[str] = Field(default=None, env="DATABASE_USER")
+    password: Optional[str] = Field(default=None, env="DATABASE_PASSWORD")
+
+
+# -----------------------------
+# ROOT CONFIG
+# -----------------------------
+class Config(BaseSettings):
+    paths: Paths
+    nba: NBASettings
+    model: ModelSettings
+    output: OutputSettings
+    logging: LoggingSettings
+    mlflow: MLflowSettings
+    betting: BettingSettings = BettingSettings()
+    runner: RunnerSettings = RunnerSettings()
+    database: DatabaseSettings = DatabaseSettings()
+    NBA_API_KEY: Optional[str] = Field(default=None, env="NBA_API_KEY")
+
+    class Config:
+        env_nested_delimiter = "__"  # e.g. MODEL__PATH overrides model.path
+
+
+# -----------------------------
+# Loader function
+# -----------------------------
+def load_config(path: str = "config.yaml") -> Config:
+    """
+    Load and validate configuration from YAML + environment overrides.
+    Raises ValidationError if required values are missing or invalid.
+    """
+    with open(path, "r") as f:
+        data = yaml.safe_load(f)
+    try:
+        return Config(**data)
+    except ValidationError as e:
+        raise RuntimeError(f"Configuration error: {e}")
Index: cleanup_pycharm.sh
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/cleanup_pycharm.sh b/cleanup_pycharm.sh
new file mode 100644
--- /dev/null	(date 1765470426212)
+++ b/cleanup_pycharm.sh	(date 1765470426212)
@@ -0,0 +1,26 @@
+#!/usr/bin/env bash
+# ============================================================
+# Script: cleanup_repo.sh
+# Purpose: Remove redundant files from nba_analysis project
+# ============================================================
+
+set -e
+
+echo "Cleaning up redundant files..."
+
+# Prediction engine
+rm -f src/prediction_engine/daily_runner.py
+
+# Model training
+rm -f src/model_training/train_logreg.py
+rm -f src/model_training/train_xgb.py
+rm -f src/model_training/train_combined.py
+
+# Old CLIs (if present)
+rm -f src/model_training/train_cli.py
+rm -f src/prediction_engine/predictor_cli_old.py
+
+# Pipeline skeleton
+rm -f src/pipeline/run_pipeline.py
+
+echo "Cleanup complete. Canonical files remain intact."
Index: src/predictor/predictor.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/src/predictor/predictor.py b/src/predictor/predictor.py
new file mode 100644
--- /dev/null	(date 1765470426257)
+++ b/src/predictor/predictor.py	(date 1765470426257)
@@ -0,0 +1,122 @@
+# ============================================================
+# File: src/predictor/predictor.py
+# Purpose: Wrapper class for scikit-learn models with proba, labels, batch prediction
+# Project: nba_analysis
+# Version: 1.4 (named logger, multi-class logging, sparse batch handling)
+# ============================================================
+
+import numpy as np
+import pandas as pd
+import logging
+from scipy.special import expit, softmax
+from sklearn.base import BaseEstimator
+from sklearn.linear_model import LogisticRegression
+from sklearn.svm import SVC
+from scipy import sparse
+
+logger = logging.getLogger("predictor.Predictor")
+if not logger.handlers:
+    handler = logging.StreamHandler()
+    handler.setFormatter(logging.Formatter("%(asctime)s [%(levelname)s] %(message)s"))
+    logger.addHandler(handler)
+    logger.setLevel(logging.INFO)
+
+
+class Predictor:
+    def __init__(self, model: BaseEstimator):
+        self.model = model
+
+    def _validate_input(self, X):
+        if not isinstance(X, (pd.DataFrame, np.ndarray, sparse.spmatrix)):
+            raise TypeError(
+                "Input must be a pandas DataFrame, NumPy array, or scipy sparse matrix."
+            )
+        if X.shape[0] == 0:
+            raise ValueError("Input data has zero samples.")
+
+    def predict_proba(self, X) -> np.ndarray:
+        self._validate_input(X)
+        if hasattr(self.model, "predict_proba"):
+            proba = np.asarray(self.model.predict_proba(X))
+        elif hasattr(self.model, "decision_function"):
+            scores = self.model.decision_function(X)
+            if scores.ndim == 1:  # binary
+                p1 = expit(scores)
+                proba = np.column_stack([1 - p1, p1])
+            else:  # multi-class
+                proba = softmax(scores, axis=1)
+        else:
+            raise AttributeError(
+                f"Model {type(self.model).__name__} does not support predict_proba or decision_function"
+            )
+
+        # Log stats
+        if proba.shape[1] == 2:
+            p1 = proba[:, 1]
+            logger.info("Average win probability: %.3f", p1.mean())
+            logger.info("Min: %.3f, Max: %.3f, Std: %.3f", p1.min(), p1.max(), p1.std())
+        else:
+            logger.info("Multi-class probabilities; logging per-class means.")
+            for i in range(proba.shape[1]):
+                logger.info("Class %d mean probability: %.3f", i, proba[:, i].mean())
+        return proba
+
+    def predict_label(self, X, threshold: float = 0.5) -> np.ndarray:
+        self._validate_input(X)
+        proba = self.predict_proba(X)
+        if proba.shape[1] == 2:
+            labels = (proba[:, 1] >= threshold).astype(int)
+            logger.info("Predicted win rate: %.3f", labels.mean())
+            return labels
+        return proba.argmax(axis=1)
+
+    def fit(self, X, y) -> "Predictor":
+        if hasattr(self.model, "fit"):
+            self.model.fit(X, y)
+            return self
+        raise AttributeError("The model does not have a fit method.")
+
+    def batch_predict_proba(self, X, batch_size: int = 1000) -> np.ndarray:
+        self._validate_input(X)
+        n_samples = X.shape[0]
+        all_probs = []
+        for i in range(0, n_samples, batch_size):
+            if isinstance(X, pd.DataFrame):
+                batch = X.iloc[i : i + batch_size]
+            elif sparse.issparse(X):
+                batch = X[i : i + batch_size, :]
+            else:
+                batch = X[i : i + batch_size]
+            all_probs.append(self.predict_proba(batch))
+        return np.vstack(all_probs)
+
+    def get_model_name(self) -> str:
+        return self.model.__class__.__name__
+
+    def get_params(self) -> dict:
+        return self.model.get_params()
+
+
+class LogisticRegressionPredictor(Predictor):
+    def __init__(self, model: LogisticRegression):
+        if not isinstance(model, LogisticRegression):
+            raise TypeError("Expected a LogisticRegression model.")
+        super().__init__(model)
+
+    def predict_proba(self, X) -> np.ndarray:
+        self._validate_input(X)
+        if hasattr(self.model, "predict_log_proba"):
+            log_proba = self.model.predict_log_proba(X)
+            return np.exp(log_proba)
+        return super().predict_proba(X)
+
+
+class SVCWithProbabilities(Predictor):
+    def __init__(self, model: SVC):
+        if not isinstance(model, SVC):
+            raise TypeError("Expected an SVC model.")
+        if not model.probability:
+            raise ValueError(
+                "SVC must be initialized with probability=True to support predict_proba."
+            )
+        super().__init__(model)
Index: src/daily_runner/daily_runner_mflow.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/src/daily_runner/daily_runner_mflow.py b/src/daily_runner/daily_runner_mflow.py
new file mode 100644
--- /dev/null	(date 1765470426244)
+++ b/src/daily_runner/daily_runner_mflow.py	(date 1765470426244)
@@ -0,0 +1,205 @@
+# ============================================================
+# File: src/daily_runner/daily_runner_mflow.py
+# Purpose: Daily NBA prediction runner with MLflow logging + optional SHAP
+# Project: nba_analysis
+# Version: 3.4 (aligned schemas, dual team mapping, centralized logging)
+# ============================================================
+
+import logging
+import os
+from datetime import datetime
+from typing import Optional
+
+import mlflow
+import numpy as np
+import pandas as pd
+
+from mlflow_setup import mlflow_run_context, log_system_metrics
+from src.api.nba_api_client import (
+    fetch_games,
+    update_historical_games,
+)  # use JSON client for IDs
+from src.features.feature_engineering import generate_features_for_games
+from src.prediction_engine.predictor import Predictor
+from src.utils.mapping import map_team_ids
+
+# -----------------------------
+# CONFIG
+# -----------------------------
+DATA_FILE = "data/cache/games_history.csv"
+MODEL_PATH = "models/nba_logreg.pkl"
+LOG_FILE = "data/logs/daily_runner.log"
+
+os.makedirs(os.path.dirname(LOG_FILE), exist_ok=True)
+logger = logging.getLogger("daily_runner")
+if not logger.handlers:
+    fh = logging.FileHandler(LOG_FILE)
+    fh.setFormatter(logging.Formatter("%(asctime)s [%(levelname)s] %(message)s"))
+    logger.addHandler(fh)
+    logger.setLevel(logging.INFO)
+
+# -----------------------------
+# HELPER FUNCTIONS
+# -----------------------------
+
+
+def _map_both_team_names(df: pd.DataFrame) -> pd.DataFrame:
+    """
+    Add TEAM_NAME and OPPONENT_TEAM_NAME by applying team ID mapping to both columns.
+    Assumes df has TEAM_ID and OPPONENT_TEAM_ID.
+    """
+    df = map_team_ids(df, team_col="TEAM_ID")
+    df = df.rename(columns={"TEAM_NAME": "TEAM_NAME"})  # explicit for clarity
+
+    # Map opponent separately; avoid overwriting TEAM_NAME
+    df_opponent = df.rename(columns={"OPPONENT_TEAM_ID": "TEAM_ID"}).copy()
+    df_opponent = map_team_ids(df_opponent, team_col="TEAM_ID")
+    df["OPPONENT_TEAM_NAME"] = df_opponent["TEAM_NAME"]
+
+    return df
+
+
+def generate_predictions(today_games: pd.DataFrame, model_path: str) -> pd.DataFrame:
+    if today_games is None or today_games.empty:
+        return pd.DataFrame()
+
+    # Ensure required columns exist
+    required_cols = {"GAME_ID", "TEAM_ID", "OPPONENT_TEAM_ID", "GAME_DATE"}
+    missing = required_cols - set(today_games.columns)
+    if missing:
+        logger.error("today_games missing required columns: %s", missing)
+        return pd.DataFrame()
+
+    # Feature generation expects a DataFrame
+    features = generate_features_for_games(today_games)
+
+    # Map team IDs to names for readability (both team and opponent)
+    features = _map_both_team_names(features)
+
+    predictor = Predictor(model_path=model_path)
+    proba = predictor.predict_proba(
+        features
+    )  # array-like probabilities for TEAM_ID winning
+    label = predictor.predict_label(features)  # boolean/int labels
+
+    # Vectorized confidence: max(p, 1 - p)
+    confidence = np.maximum(proba, 1.0 - proba)
+
+    features["win_proba"] = proba
+    features["win_pred"] = label
+    features["prediction_confidence"] = confidence
+
+    return features
+
+
+def log_to_mlflow(predictions: pd.DataFrame, model_path: Optional[str] = None) -> None:
+    if predictions is None or predictions.empty:
+        return
+
+    with mlflow_run_context("daily_predictions", strict=False):
+        mlflow.log_param("model_path", model_path or MODEL_PATH)
+
+        # Save CSV artifact
+        csv_path = f"data/csv/daily_predictions_{datetime.now().strftime('%Y%m%d')}.csv"
+        os.makedirs(os.path.dirname(csv_path), exist_ok=True)
+        predictions.to_csv(csv_path, index=False)
+        mlflow.log_artifact(csv_path, artifact_path="daily_predictions")
+
+        # Metrics
+        avg_proba = float(np.nanmean(predictions["win_proba"]))
+        mlflow.log_metric("avg_win_proba", avg_proba)
+        mlflow.log_metric(
+            "pred_confidence_mean",
+            float(np.nanmean(predictions["prediction_confidence"])),
+        )
+        mlflow.log_metric(
+            "pred_confidence_min",
+            float(np.nanmin(predictions["prediction_confidence"])),
+        )
+        mlflow.log_metric(
+            "pred_confidence_max",
+            float(np.nanmax(predictions["prediction_confidence"])),
+        )
+
+        log_system_metrics()
+        logger.info("Logged predictions to MLflow. Avg win prob: %.2f", avg_proba)
+
+
+def print_summary(predictions: pd.DataFrame) -> None:
+    if predictions is None or predictions.empty:
+        logger.info("No NBA games today.")
+        return
+
+    logger.info("Today's NBA Predictions:")
+    for _, row in predictions.iterrows():
+        opponent_name = row.get("OPPONENT_TEAM_NAME", "Unknown")
+        winner = row["TEAM_NAME"] if bool(row["win_pred"]) else opponent_name
+        logger.info(
+            "%s vs %s | Win probability: %.2f | Predicted winner: %s | Confidence: %.2f",
+            row.get("TEAM_NAME", "Unknown"),
+            opponent_name,
+            float(row["win_proba"]),
+            winner,
+            float(row["prediction_confidence"]),
+        )
+
+
+# -----------------------------
+# MAIN
+# -----------------------------
+if __name__ == "__main__":
+    import argparse
+
+    parser = argparse.ArgumentParser(description="Daily NBA Predictor Runner")
+    parser.add_argument("--model", default=MODEL_PATH, help="Path to trained model")
+    parser.add_argument("--run_shap", action="store_true", help="Run SHAP analysis")
+    args = parser.parse_args()
+
+    logger.info("Starting MLflow NBA daily runner...")
+
+    # Load existing historical data (used by update_historical_games)
+    try:
+        existing_df = pd.read_csv(DATA_FILE)
+    except FileNotFoundError:
+        logger.info("No historical data found, starting fresh.")
+        existing_df = pd.DataFrame(
+            columns=[
+                "GAME_ID",
+                "GAME_DATE",
+                "TEAM_NAME",
+                "MATCHUP",
+                "TEAM_ID",
+                "OPPONENT_TEAM_ID",
+                "PTS",
+            ]
+        )
+
+    # Update historical games
+    updated_games_df = update_historical_games(existing_df)
+
+    # Fetch today's games using JSON client (ensures ID schema for features)
+    today_str = datetime.now().strftime("%Y-%m-%d")
+    today_games = fetch_games(today_str)
+
+    # Predict
+    predictions = generate_predictions(today_games, args.model)
+
+    # Log and summarize
+    log_system_metrics()  # snapshot after predictions
+    print_summary(predictions)
+    log_to_mlflow(predictions, model_path=args.model)
+
+    # Optional SHAP analysis
+    if args.run_shap:
+        try:
+            from src.interpretability.shap_analysis import run_shap
+
+            shap_dir = "data/shap"
+            os.makedirs(shap_dir, exist_ok=True)
+            logger.info("Running SHAP analysis...")
+            run_shap(args.model, cache_file=DATA_FILE, out_dir=shap_dir)
+            log_system_metrics()  # snapshot after SHAP
+        except ImportError:
+            logger.warning("SHAP analysis requested, but shap module is not available.")
+
+    logger.info("MLflow NBA daily runner finished.")
Index: src/mlflow_setup.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+># ============================================================\n# File: mlflow_setup.py\n# Purpose: Configure MLflow tracking for nba_analysis experiments.\n# Project: nba_analysis\n# ============================================================\n\nimport os\nimport sys\nimport mlflow\nimport datetime\nimport subprocess\nimport platform\nimport pkg_resources\n\n\ndef configure_mlflow():\n    \"\"\"Configure MLflow tracking URI and experiment, auto-creating if needed.\"\"\"\n    tracking_uri = os.getenv(\"MLFLOW_TRACKING_URI\", \"file:./mlruns\")\n    experiment_name = os.getenv(\"MLFLOW_EXPERIMENT_NAME\", \"NBA_Analysis_Experiments\")\n\n    try:\n        mlflow.set_tracking_uri(tracking_uri)\n\n        # Ensure experiment exists\n        experiment = mlflow.get_experiment_by_name(experiment_name)\n        if experiment is None:\n            experiment_id = mlflow.create_experiment(experiment_name)\n            mlflow.set_experiment(experiment_name)\n            print(f\"✅ Created new MLflow experiment '{experiment_name}' (ID={experiment_id})\")\n        else:\n            mlflow.set_experiment(experiment_name)\n            print(f\"✅ Using existing MLflow experiment '{experiment_name}' (ID={experiment.experiment_id})\")\n\n        print(f\"✅ MLflow configured with URI={tracking_uri}\")\n    except Exception as e:\n        print(f\"❌ Failed to configure MLflow: {e}\", file=sys.stderr)\n        sys.exit(1)\n\n\ndef start_run_with_metadata(run_name: str = \"nba_analysis_run\"):\n    \"\"\"\n    Start an MLflow run with standard metadata:\n    - Project name\n    - Timestamp\n    - Git commit hash (if available)\n    - System environment info (Python version, OS, machine)\n    - Installed package versions\n    \"\"\"\n    configure_mlflow()\n\n    # Collect metadata\n    timestamp = datetime.datetime.utcnow().isoformat()\n    try:\n        git_commit = subprocess.check_output(\n            [\"git\", \"rev-parse\", \"HEAD\"], stderr=subprocess.DEVNULL\n        ).decode(\"utf-8\").strip()\n    except Exception:\n        git_commit = \"unknown\"\n\n    run = mlflow.start_run(run_name=run_name)\n\n    # Tags for reproducibility\n    mlflow.set_tags({\n        \"project\": \"nba_analysis\",\n        \"timestamp\": timestamp,\n        \"git_commit\": git_commit,\n        \"python_version\": platform.python_version(),\n        \"os\": platform.system(),\n        \"os_version\": platform.version(),\n        \"machine\": platform.machine(),\n        \"processor\": platform.processor(),\n    })\n\n    # Log installed package versions\n    try:\n        installed_packages = {dist.project_name: dist.version for dist in pkg_resources.working_set}\n        for pkg, version in installed_packages.items():\n            mlflow.log_param(f\"pkg_{pkg}\", version)\n    except Exception as e:\n        print(f\"⚠\uFE0F Failed to log package versions: {e}\", file=sys.stderr)\n\n    print(f\"✅ MLflow run started: {run.info.run_id}\")\n    return run\n\n\nif __name__ == \"__main__\":\n    configure_mlflow()\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/src/mlflow_setup.py b/src/mlflow_setup.py
--- a/src/mlflow_setup.py	(revision 00055356329a845d3b2ca81b77c3ffb2e2b15969)
+++ b/src/mlflow_setup.py	(date 1765470426248)
@@ -1,81 +1,112 @@
 # ============================================================
 # File: mlflow_setup.py
-# Purpose: Configure MLflow tracking for nba_analysis experiments.
+# Purpose: Configure MLflow tracking for nba_analysis experiments
 # Project: nba_analysis
+# Version: 1.5 (adds system resource logging)
 # ============================================================
 
+import datetime
+import json
 import os
+import platform
+import subprocess
 import sys
+from tempfile import NamedTemporaryFile
+from contextlib import contextmanager
+
 import mlflow
-import datetime
-import subprocess
-import platform
 import pkg_resources
+import psutil  # ✅ new dependency for system metrics
 
 
-def configure_mlflow():
-    """Configure MLflow tracking URI and experiment, auto-creating if needed."""
-    tracking_uri = os.getenv("MLFLOW_TRACKING_URI", "file:./mlruns")
-    experiment_name = os.getenv("MLFLOW_EXPERIMENT_NAME", "NBA_Analysis_Experiments")
+def configure_mlflow(
+    tracking_uri: str = None, experiment_name: str = None, strict: bool = True
+):
+    """
+    Configure MLflow tracking URI and experiment.
+    Auto-creates experiment if it doesn't exist.
+    """
+    tracking_uri = tracking_uri or os.getenv("MLFLOW_TRACKING_URI", "file:./mlruns")
+    experiment_name = experiment_name or os.getenv(
+        "MLFLOW_EXPERIMENT_NAME", "NBA_Analysis_Experiments"
+    )
 
     try:
         mlflow.set_tracking_uri(tracking_uri)
-
-        # Ensure experiment exists
         experiment = mlflow.get_experiment_by_name(experiment_name)
+
         if experiment is None:
             experiment_id = mlflow.create_experiment(experiment_name)
             mlflow.set_experiment(experiment_name)
-            print(f"✅ Created new MLflow experiment '{experiment_name}' (ID={experiment_id})")
+            print(
+                f"✅ Created new MLflow experiment '{experiment_name}' (ID={experiment_id})"
+            )
         else:
             mlflow.set_experiment(experiment_name)
-            print(f"✅ Using existing MLflow experiment '{experiment_name}' (ID={experiment.experiment_id})")
+            print(
+                f"✅ Using existing MLflow experiment '{experiment_name}' (ID={experiment.experiment_id})"
+            )
 
         print(f"✅ MLflow configured with URI={tracking_uri}")
     except Exception as e:
-        print(f"❌ Failed to configure MLflow: {e}", file=sys.stderr)
-        sys.exit(1)
+        msg = f"❌ Failed to configure MLflow: {e}"
+        if strict:
+            print(msg, file=sys.stderr)
+            sys.exit(1)
+        else:
+            print(msg + " — continuing without MLflow logging", file=sys.stderr)
 
 
-def start_run_with_metadata(run_name: str = "nba_analysis_run"):
+def start_run_with_metadata(run_name: str = None, strict: bool = True):
     """
-    Start an MLflow run with standard metadata:
-    - Project name
-    - Timestamp
-    - Git commit hash (if available)
-    - System environment info (Python version, OS, machine)
-    - Installed package versions
+    Start an MLflow run with reproducibility metadata.
     """
-    configure_mlflow()
+    run_name = (
+        run_name
+        or f"nba_analysis_run_{datetime.datetime.utcnow().strftime('%Y%m%d_%H%M%S')}"
+    )
+    configure_mlflow(strict=strict)
 
-    # Collect metadata
     timestamp = datetime.datetime.utcnow().isoformat()
+
+    # Attempt to get git commit
     try:
-        git_commit = subprocess.check_output(
-            ["git", "rev-parse", "HEAD"], stderr=subprocess.DEVNULL
-        ).decode("utf-8").strip()
+        git_commit = (
+            subprocess.check_output(
+                ["git", "rev-parse", "HEAD"], stderr=subprocess.DEVNULL
+            )
+            .decode("utf-8")
+            .strip()
+        )
     except Exception:
         git_commit = "unknown"
 
     run = mlflow.start_run(run_name=run_name)
 
-    # Tags for reproducibility
-    mlflow.set_tags({
-        "project": "nba_analysis",
-        "timestamp": timestamp,
-        "git_commit": git_commit,
-        "python_version": platform.python_version(),
-        "os": platform.system(),
-        "os_version": platform.version(),
-        "machine": platform.machine(),
-        "processor": platform.processor(),
-    })
+    # Set tags for reproducibility
+    mlflow.set_tags(
+        {
+            "project": "nba_analysis",
+            "timestamp": timestamp,
+            "git_commit": git_commit,
+            "python_version": platform.python_version(),
+            "os": platform.system(),
+            "os_version": platform.version(),
+            "machine": platform.machine(),
+            "processor": platform.processor(),
+            "run_origin": os.getenv("RUN_ORIGIN", "manual"),
+            "schema_version": "2.1",
+        }
+    )
 
-    # Log installed package versions
+    # Log installed packages as JSON artifact
     try:
-        installed_packages = {dist.project_name: dist.version for dist in pkg_resources.working_set}
-        for pkg, version in installed_packages.items():
-            mlflow.log_param(f"pkg_{pkg}", version)
+        installed_packages = {
+            dist.project_name: dist.version for dist in pkg_resources.working_set
+        }
+        with NamedTemporaryFile("w", delete=False, suffix=".json") as f:
+            json.dump(installed_packages, f, indent=2)
+            mlflow.log_artifact(f.name, artifact_path="environment")
     except Exception as e:
         print(f"⚠️ Failed to log package versions: {e}", file=sys.stderr)
 
@@ -83,5 +114,51 @@
     return run
 
 
+def log_system_metrics():
+    """
+    Log current system resource usage (CPU %, memory) to MLflow.
+    """
+    try:
+        cpu_percent = psutil.cpu_percent(interval=1)
+        mem = psutil.virtual_memory()
+        mlflow.log_metric("cpu_percent", cpu_percent)
+        mlflow.log_metric("memory_percent", mem.percent)
+        mlflow.log_metric("memory_used_mb", mem.used / (1024 * 1024))
+        print(f"📊 Logged system metrics: CPU={cpu_percent}%, MEM={mem.percent}%")
+    except Exception as e:
+        print(f"⚠️ Failed to log system metrics: {e}", file=sys.stderr)
+
+
+def end_run_with_cleanup(status: str = "FINISHED"):
+    """
+    End the current MLflow run safely.
+    """
+    try:
+        mlflow.end_run(status=status)
+        print(f"✅ MLflow run ended with status={status}")
+    except Exception as e:
+        print(f"⚠️ Failed to end MLflow run: {e}", file=sys.stderr)
+
+
+@contextmanager
+def mlflow_run_context(run_name: str = None, strict: bool = True):
+    """
+    Context manager to ensure MLflow runs are always closed.
+    Also logs system metrics at start and end.
+    """
+    run = None
+    try:
+        run = start_run_with_metadata(run_name=run_name, strict=strict)
+        log_system_metrics()  # log at start
+        yield run
+        log_system_metrics()  # log at end
+        end_run_with_cleanup("FINISHED")
+    except Exception as e:
+        print(f"❌ Exception during MLflow run: {e}", file=sys.stderr)
+        log_system_metrics()  # log crash state
+        end_run_with_cleanup("FAILED")
+        raise
+
+
 if __name__ == "__main__":
     configure_mlflow()
Index: tests/test_nba_api_client.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/tests/test_nba_api_client.py b/tests/test_nba_api_client.py
new file mode 100644
--- /dev/null	(date 1765470426274)
+++ b/tests/test_nba_api_client.py	(date 1765470426274)
@@ -0,0 +1,111 @@
+# tests/test_nba_api_client.py
+import pandas as pd
+import pytest
+from types import SimpleNamespace
+import src.api.nba_api_client as client
+
+
+def test_fetch_season_games_schema(monkeypatch):
+    class DummyLGF:
+        def __init__(self, *args, **kwargs):
+            pass
+
+        def get_data_frames(self):
+            return [
+                pd.DataFrame(
+                    {
+                        "GAME_ID": ["001"],
+                        "GAME_DATE": ["2025-12-11"],
+                        "TEAM_NAME": ["LAL"],
+                        "MATCHUP": ["LAL vs BOS"],
+                    }
+                )
+            ]
+
+    monkeypatch.setattr(client.leaguegamefinder, "LeagueGameFinder", DummyLGF)
+    df = client.fetch_season_games("2025-26")
+    assert list(df.columns) == client.EXPECTED_SEASON_COLS
+    assert pd.api.types.is_datetime64_any_dtype(df["GAME_DATE"])
+
+
+def test_fetch_today_games_parsing(monkeypatch):
+    today = pd.Timestamp.now().date().strftime("%Y-%m-%d")
+
+    class DummyLGF:
+        def __init__(self, *args, **kwargs):
+            pass
+
+        def get_data_frames(self):
+            return [
+                pd.DataFrame(
+                    {
+                        "GAME_ID": ["001"],
+                        "GAME_DATE": [today],
+                        "TEAM_NAME": ["LAL"],
+                        "MATCHUP": ["LAL @ BOS"],
+                    }
+                )
+            ]
+
+    monkeypatch.setattr(client.leaguegamefinder, "LeagueGameFinder", DummyLGF)
+    df = client.fetch_today_games()
+    assert "home_team" in df.columns and "away_team" in df.columns
+    assert df.iloc[0]["home_team"] == "BOS"
+    assert df.iloc[0]["away_team"] == "LAL"
+
+
+def test_fetch_games_json_cache_and_parse(monkeypatch, tmp_path):
+    # Mock responses for full schedule
+    data = {
+        "lscd": [
+            {
+                "mscd": {
+                    "g": [
+                        {
+                            "gdte": "2025-12-11",
+                            "gid": "002250001",
+                            "h": {"tid": 1610612747},
+                            "v": {"tid": 1610612738},
+                        }
+                    ]
+                }
+            }
+        ]
+    }
+    # Redirect RAW_DIR
+    client.RAW_DIR = str(tmp_path)
+
+    def fake_get(url, timeout=10):
+        class Resp:
+            def raise_for_status(self):
+                pass
+
+            def json(self):
+                return data
+
+        return Resp()
+
+    monkeypatch.setattr(client.requests, "get", fake_get)
+    df = client.fetch_games("2025-12-11", use_cache=False)
+    assert set(df.columns) == set(client.EXPECTED_GAME_COLS)
+    assert len(df) == 2  # home + away perspectives
+
+
+def test_fetch_boxscores_missing_pd(monkeypatch, tmp_path):
+    client.RAW_DIR = str(tmp_path)
+
+    # Return JSON without 'pd'
+    def fake_get(url, timeout=10):
+        class Resp:
+            def raise_for_status(self):
+                pass
+
+            def json(self):
+                return {"g": {}}
+
+        return Resp()
+
+    monkeypatch.setattr(client.requests, "get", fake_get)
+    df = client.fetch_boxscores(["002250001"], use_cache=False)
+    assert df.empty
+    assert list(df.columns) == client.EXPECTED_BOX_COLS
Index: tests/test_add_unique_id.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/tests/test_add_unique_id.py b/tests/test_add_unique_id.py
new file mode 100644
--- /dev/null	(date 1765470426270)
+++ b/tests/test_add_unique_id.py	(date 1765470426270)
@@ -0,0 +1,54 @@
+# ============================================================
+# File: tests/test_add_unique_id.py
+# Purpose: Validate unique_id generation and deduplication
+# Project: nba_analysis
+# ============================================================
+
+import datetime
+import pandas as pd
+import pytest
+
+from src.utils.add_unique_id import add_unique_id
+
+
+def test_add_unique_id_basic():
+    df = pd.DataFrame(
+        {"GAME_ID": ["g1"], "TEAM_ID": [1], "prediction_date": ["2025-12-11"]}
+    )
+    out = add_unique_id(df)
+    assert "unique_id" in out.columns
+    assert out.loc[0, "unique_id"] == "g1_1_2025-12-11"
+
+
+def test_add_unique_id_missing_columns(monkeypatch):
+    df = pd.DataFrame({"TEAM_ID": [2]})
+    out = add_unique_id(df)
+    # GAME_ID should be placeholder
+    assert out.loc[0, "GAME_ID"].startswith("unknown_game")
+    # prediction_date should default to today
+    assert out.loc[0, "prediction_date"] == str(datetime.date.today())
+
+
+def test_add_unique_id_type_enforcement():
+    df = pd.DataFrame(
+        {"GAME_ID": [123], "TEAM_ID": ["5"], "prediction_date": ["2025-12-11"]}
+    )
+    out = add_unique_id(df)
+    # TEAM_ID coerced to int
+    assert out.loc[0, "TEAM_ID"] == 5
+    # GAME_ID coerced to str
+    assert isinstance(out.loc[0, "GAME_ID"], str)
+
+
+def test_add_unique_id_deduplication():
+    df = pd.DataFrame(
+        {
+            "GAME_ID": ["g1", "g1"],
+            "TEAM_ID": [1, 1],
+            "prediction_date": ["2025-12-11", "2025-12-11"],
+        }
+    )
+    out = add_unique_id(df)
+    # Deduplication should drop duplicate unique_id rows
+    assert len(out) == 1
+    assert out["unique_id"].is_unique
Index: tests/test_pipeline.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/tests/test_pipeline.py b/tests/test_pipeline.py
new file mode 100644
--- /dev/null	(date 1765470426275)
+++ b/tests/test_pipeline.py	(date 1765470426275)
@@ -0,0 +1,29 @@
+import pytest
+from src.prediction_engine.predictor import Predictor
+
+
+@pytest.fixture
+def mock_predictor():
+    # Create an instance of the predictor class with a mock model path
+    return Predictor(model_path="path/to/mock/model.pkl")
+
+
+def test_predictor_initialization(mock_predictor):
+    # Test that the predictor is initialized correctly
+    assert mock_predictor is not None
+    assert callable(
+        mock_predictor.model
+    )  # Ensure the model is callable (like a function)
+
+
+def test_predictor_prediction(mock_predictor):
+    # Test prediction functionality
+    mock_data = {
+        "TEAM_ID": 1,
+        "OPPONENT_TEAM_ID": 2,
+        "RollingPTS_5": 105,
+        "RollingWinPct_10": 0.8,
+    }
+
+    prediction = mock_predictor.predict(mock_data)
+    assert prediction in [0, 1]  # Assume binary outcome (win/loss)
Index: tests/test_api.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/tests/test_api.py b/tests/test_api.py
new file mode 100644
--- /dev/null	(date 1765470426270)
+++ b/tests/test_api.py	(date 1765470426270)
@@ -0,0 +1,39 @@
+import pytest
+from unittest.mock import patch
+from datetime import datetime
+import pandas as pd
+from src.api.nba_api_client import fetch_games
+
+
+# Test for the fetch_games function
+@patch("src.api.nba_api_client.requests.get")
+def test_fetch_games(mock_get):
+    # Mock the API response
+    mock_get.return_value.status_code = 200
+    mock_get.return_value.json.return_value = {
+        "lscd": [
+            {
+                "mscd": {
+                    "g": [
+                        {
+                            "gid": "001",
+                            "h": {"tid": 1},
+                            "v": {"tid": 2},
+                            "gdte": "2023-12-10",
+                        }
+                    ]
+                }
+            }
+        ]
+    }
+
+    # Call the function
+    date = "2023-12-10"
+    games_df = fetch_games(date, use_cache=False)
+
+    # Validate the output
+    assert isinstance(games_df, pd.DataFrame)  # Check if it returns a DataFrame
+    assert games_df.shape[0] > 0  # Ensure there are some rows
+    assert "GAME_ID" in games_df.columns  # Ensure that GAME_ID column is present
+    assert games_df["GAME_ID"].iloc[0] == "001"  # Ensure the GAME_ID is as expected
+    assert games_df["date"].iloc[0] == date  # Ensure the date is as expected
Index: tests/test_mapping.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/tests/test_mapping.py b/tests/test_mapping.py
new file mode 100644
--- /dev/null	(date 1765470426273)
+++ b/tests/test_mapping.py	(date 1765470426273)
@@ -0,0 +1,58 @@
+# ============================================================
+# File: tests/test_mapping.py
+# Purpose: Validate team and player ID mapping utilities
+# Project: nba_analysis
+# ============================================================
+
+import pandas as pd
+import pytest
+
+from src.utils.mapping import map_team_ids, map_player_ids, map_ids
+
+
+def test_map_team_ids_basic():
+    df = pd.DataFrame({"TEAM_ID": [0, 29, 99]})
+    out = map_team_ids(df)
+    assert out.loc[0, "TEAM_NAME"] == "Team_0"
+    assert out.loc[1, "TEAM_NAME"] == "Team_29"
+    assert out.loc[2, "TEAM_NAME"] == "UnknownTeam"
+
+
+def test_map_player_ids_basic():
+    df = pd.DataFrame({"PLAYER_ID": [0, 999, 1001]})
+    out = map_player_ids(df)
+    assert out.loc[0, "PLAYER_NAME"] == "Player_0"
+    assert out.loc[1, "PLAYER_NAME"] == "Player_999"
+    assert out.loc[2, "PLAYER_NAME"] == "UnknownPlayer"
+
+
+def test_map_ids_combined():
+    df = pd.DataFrame({"TEAM_ID": [0, 29, 99], "PLAYER_ID": [0, 999, 1001]})
+    out = map_ids(df)
+    # TEAM_NAME and PLAYER_NAME should both exist
+    assert "TEAM_NAME" in out.columns
+    assert "PLAYER_NAME" in out.columns
+    # Check known mappings
+    assert out.loc[0, "TEAM_NAME"] == "Team_0"
+    assert out.loc[0, "PLAYER_NAME"] == "Player_0"
+    # Check unknowns
+    assert out.loc[2, "TEAM_NAME"] == "UnknownTeam"
+    assert out.loc[2, "PLAYER_NAME"] == "UnknownPlayer"
+
+
+def test_map_ids_custom_maps():
+    custom_team_map = {100: "CustomTeam"}
+    custom_player_map = {2000: "CustomPlayer"}
+    df = pd.DataFrame({"TEAM_ID": [100], "PLAYER_ID": [2000]})
+    out = map_ids(df, team_map=custom_team_map, player_map=custom_player_map)
+    assert out.loc[0, "TEAM_NAME"] == "CustomTeam"
+    assert out.loc[0, "PLAYER_NAME"] == "CustomPlayer"
+
+
+def test_invalid_input_raises():
+    with pytest.raises(TypeError):
+        map_team_ids([1, 2, 3])
+    with pytest.raises(TypeError):
+        map_player_ids([1, 2, 3])
+    with pytest.raises(TypeError):
+        map_ids([1, 2, 3])
Index: .gitignore
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+># --- Python build artifacts ---\n__pycache__/\n*.pyc\n*.pyo\n*.pyd\n# Environmental variable\n.env\n\n# --- Virtual environments ---\nenv/\n.venv/\n.ENV/\nvenv/\n\n# --- Logs & databases ---\n*.log\nlogs/\n*.sqlite3\n*.db\n\n# --- Project outputs ---\nresults/\nmodels/\ndata/\narchives/\n\n# --- Jupyter notebooks ---\n.ipynb_checkpoints/\n*.ipynb\n\n# --- Testing & coverage ---\n.pytest_cache/\n.mypy_cache/\n.coverage\ncoverage.xml\nhtmlcov/\n\n# --- OS-specific junk ---\n.DS_Store\nThumbs.db\n\n# --- IDE/project files ---\n.vscode/\n.idea/\n\n# --- Build/distribution ---\nbuild/\ndist/\n*.egg-info/\n\n# --- Temporary files ---\n*.tmp\n*.bak\n*.swp\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/.gitignore b/.gitignore
--- a/.gitignore	(revision 00055356329a845d3b2ca81b77c3ffb2e2b15969)
+++ b/.gitignore	(date 1765470426209)
@@ -3,8 +3,11 @@
 *.pyc
 *.pyo
 *.pyd
-# Environmental variable
+
+# --- Environment variables ---
 .env
+.env.*
+*.lock
 
 # --- Virtual environments ---
 env/
@@ -23,6 +26,7 @@
 models/
 data/
 archives/
+mlruns/
 
 # --- Jupyter notebooks ---
 .ipynb_checkpoints/
@@ -52,3 +56,4 @@
 *.tmp
 *.bak
 *.swp
+*~
Index: tests/test_nba_api_wrapper.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/tests/test_nba_api_wrapper.py b/tests/test_nba_api_wrapper.py
new file mode 100644
--- /dev/null	(date 1765470426274)
+++ b/tests/test_nba_api_wrapper.py	(date 1765470426274)
@@ -0,0 +1,99 @@
+# ============================================================
+# File: tests/test_nba_api_wrapper.py
+# Purpose: Validate NBA API wrapper utilities
+# Project: nba_analysis
+# ============================================================
+
+import pandas as pd
+import pytest
+from src.utils import nba_api_wrapper
+
+
+class DummyGameFinder:
+    def __init__(self, *args, **kwargs):
+        pass
+
+    def get_data_frames(self):
+        return [
+            pd.DataFrame(
+                {
+                    "GAME_DATE": ["2025-01-01"],
+                    "TEAM_NAME": ["TestTeam"],
+                    "MATCHUP": ["TestTeam vs Opponent"],
+                    "GAME_ID": ["001"],
+                    "TEAM_ID": [123],
+                    "OPPONENT_TEAM_ID": [456],
+                    "PTS": [100],
+                    "WL": ["W"],
+                }
+            )
+        ]
+
+
+class DummyScoreboard:
+    def __init__(self, *args, **kwargs):
+        pass
+
+    def get_data_frames(self):
+        return [
+            pd.DataFrame(
+                {"GAME_ID": ["001"], "HOME_TEAM_ID": [123], "VISITOR_TEAM_ID": [456]}
+            )
+        ]
+
+
+def test_fetch_season_games_success(monkeypatch):
+    monkeypatch.setattr(
+        nba_api_wrapper.leaguegamefinder, "LeagueGameFinder", DummyGameFinder
+    )
+    df = nba_api_wrapper.fetch_season_games("2025-26")
+    assert not df.empty
+    assert "POINTS" in df.columns
+    assert "TARGET" in df.columns
+    assert df.loc[0, "TARGET"] == 1  # W mapped to 1
+
+
+def test_fetch_season_games_invalid_type():
+    with pytest.raises(TypeError):
+        nba_api_wrapper.fetch_season_games(2025)
+
+
+def test_fetch_season_games_error(monkeypatch):
+    def bad_finder(*args, **kwargs):
+        raise Exception("API error")
+
+    monkeypatch.setattr(
+        nba_api_wrapper.leaguegamefinder, "LeagueGameFinder", bad_finder
+    )
+    df = nba_api_wrapper.fetch_season_games("2025-26")
+    assert list(df.columns) == nba_api_wrapper.EXPECTED_COLS
+
+
+def test_fetch_today_games_success(monkeypatch):
+    monkeypatch.setattr(nba_api_wrapper.scoreboard, "Scoreboard", DummyScoreboard)
+    df = nba_api_wrapper.fetch_today_games()
+    assert not df.empty
+    assert "AWAY_TEAM_ID" in df.columns
+
+
+def test_fetch_today_games_error(monkeypatch):
+    def bad_scoreboard(*args, **kwargs):
+        raise Exception("API error")
+
+    monkeypatch.setattr(nba_api_wrapper.scoreboard, "Scoreboard", bad_scoreboard)
+    df = nba_api_wrapper.fetch_today_games()
+    assert list(df.columns) == nba_api_wrapper.EXPECTED_TODAY_COLS
+
+
+def test_fetch_games_today(monkeypatch):
+    monkeypatch.setattr(nba_api_wrapper.scoreboard, "Scoreboard", DummyScoreboard)
+    df = nba_api_wrapper.fetch_games()
+    assert "AWAY_TEAM_ID" in df.columns
+
+
+def test_fetch_games_season(monkeypatch):
+    monkeypatch.setattr(
+        nba_api_wrapper.leaguegamefinder, "LeagueGameFinder", DummyGameFinder
+    )
+    df = nba_api_wrapper.fetch_games("2025-26")
+    assert "POINTS" in df.columns
Index: tests/test_generate_historical_schedule.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/tests/test_generate_historical_schedule.py b/tests/test_generate_historical_schedule.py
new file mode 100644
--- /dev/null	(date 1765470426272)
+++ b/tests/test_generate_historical_schedule.py	(date 1765470426272)
@@ -0,0 +1,100 @@
+# ============================================================
+# File: tests/test_generate_historical_schedule.py
+# Purpose: Validate historical schedule generation script
+# Project: nba_analysis
+# ============================================================
+
+import pandas as pd
+import pytest
+import os
+
+import src.scripts.generate_historical_schedule as script
+
+
+class DummyGameFinder:
+    """Mock LeagueGameFinder returning a simple DataFrame."""
+
+    def __init__(self, *args, **kwargs):
+        pass
+
+    def get_data_frames(self):
+        return [
+            pd.DataFrame(
+                {
+                    "GAME_DATE": ["2025-01-01"],
+                    "TEAM_NAME": ["TestTeam"],
+                    "MATCHUP": ["TestTeam vs Opponent"],
+                    "GAME_ID": ["001"],
+                    "TEAM_ID": [123],
+                    "PTS": [100],
+                    "WL": ["W"],
+                }
+            )
+        ]
+
+
+def test_successful_fetch(monkeypatch, tmp_path):
+    # Patch LeagueGameFinder to return dummy data
+    monkeypatch.setattr(
+        script,
+        "leaguegamefinder",
+        type("LGF", (), {"LeagueGameFinder": DummyGameFinder}),
+    )
+    # Patch OUTPUT_FILE to temporary location
+    script.OUTPUT_FILE = tmp_path / "historical_schedule.parquet"
+
+    script.main()
+
+    # Verify file created
+    assert script.OUTPUT_FILE.exists()
+    df = pd.read_parquet(script.OUTPUT_FILE)
+    assert "GAME_ID" in df.columns
+    assert len(df) == 1
+
+
+def test_error_fetch(monkeypatch, tmp_path):
+    # Patch LeagueGameFinder to raise error
+    def bad_finder(*args, **kwargs):
+        raise Exception("API error")
+
+    monkeypatch.setattr(
+        script, "leaguegamefinder", type("LGF", (), {"LeagueGameFinder": bad_finder})
+    )
+    script.OUTPUT_FILE = tmp_path / "historical_schedule.parquet"
+
+    # Run main, should not create file
+    script.main()
+    assert not script.OUTPUT_FILE.exists()
+
+
+def test_no_games(monkeypatch, tmp_path):
+    # Patch LeagueGameFinder to return empty DataFrame
+    class EmptyGameFinder:
+        def __init__(self, *args, **kwargs):
+            pass
+
+        def get_data_frames(self):
+            return [
+                pd.DataFrame(
+                    columns=[
+                        "GAME_DATE",
+                        "TEAM_NAME",
+                        "MATCHUP",
+                        "GAME_ID",
+                        "TEAM_ID",
+                        "PTS",
+                        "WL",
+                    ]
+                )
+            ]
+
+    monkeypatch.setattr(
+        script,
+        "leaguegamefinder",
+        type("LGF", (), {"LeagueGameFinder": EmptyGameFinder}),
+    )
+    script.OUTPUT_FILE = tmp_path / "historical_schedule.parquet"
+
+    script.main()
+    # File should not exist because no games were fetched
+    assert not script.OUTPUT_FILE.exists()
Index: tests/test_shap_analysis.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/tests/test_shap_analysis.py b/tests/test_shap_analysis.py
new file mode 100644
--- /dev/null	(date 1765470426277)
+++ b/tests/test_shap_analysis.py	(date 1765470426277)
@@ -0,0 +1,122 @@
+# ============================================================
+# File: tests/test_shap_analysis.py
+# Purpose: Unit tests for interpretability.shap_analysis.run_shap
+# Project: nba_analysis
+# ============================================================
+
+import os
+import tempfile
+import pandas as pd
+import numpy as np
+import joblib
+import pytest
+
+from sklearn.pipeline import Pipeline
+from sklearn.preprocessing import StandardScaler
+from xgboost import XGBClassifier
+
+import src.interpretability.shap_analysis as shap_analysis
+
+
+@pytest.fixture
+def dummy_pipeline(tmp_path):
+    """Create a simple sklearn pipeline with XGBClassifier and save it."""
+    X = pd.DataFrame(
+        {
+            "f1": np.random.randn(20),
+            "f2": np.random.randn(20),
+            "target": np.random.randint(0, 2, 20),
+        }
+    )
+    y = X["target"]
+    X_train = X.drop(columns=["target"])
+
+    clf = XGBClassifier(
+        n_estimators=5, max_depth=2, use_label_encoder=False, eval_metric="logloss"
+    )
+    pipeline = Pipeline(
+        [
+            ("scaler", StandardScaler()),
+            ("clf", clf),
+        ]
+    )
+    pipeline.fit(X_train, y)
+
+    model_path = tmp_path / "pipeline.pkl"
+    joblib.dump(pipeline, model_path)
+
+    # Save dataset as CSV
+    cache_file = tmp_path / "cache.csv"
+    X.to_csv(cache_file, index=False)
+
+    return str(model_path), str(cache_file)
+
+
+@pytest.fixture
+def mock_mlflow(monkeypatch):
+    """Mock MLflow functions to capture calls."""
+    calls = {"artifacts": [], "metrics": {}, "params": {}}
+
+    class DummyRun:
+        def __enter__(self):
+            return self
+
+        def __exit__(self, *args):
+            return False
+
+    monkeypatch.setattr(shap_analysis.mlflow, "start_run", lambda **kwargs: DummyRun())
+    monkeypatch.setattr(
+        shap_analysis.mlflow,
+        "log_artifact",
+        lambda path, artifact_path=None: calls["artifacts"].append(
+            (path, artifact_path)
+        ),
+    )
+    monkeypatch.setattr(
+        shap_analysis.mlflow,
+        "log_metric",
+        lambda k, v: calls["metrics"].__setitem__(k, v),
+    )
+    monkeypatch.setattr(
+        shap_analysis.mlflow,
+        "log_param",
+        lambda k, v: calls["params"].__setitem__(k, v),
+    )
+
+    return calls
+
+
+def test_run_shap_creates_plots_and_logs(dummy_pipeline, mock_mlflow, tmp_path):
+    model_path, cache_file = dummy_pipeline
+    out_dir = tmp_path / "out"
+
+    shap_analysis.run_shap(model_path, cache_file, out_dir=str(out_dir), top_n=2)
+
+    # Check summary plot exists
+    summary_path = out_dir / "shap_summary.png"
+    assert summary_path.exists()
+
+    # Check dependence plots exist
+    dep_files = list(out_dir.glob("shap_dependence_*.png"))
+    assert len(dep_files) == 2
+
+    # Check MLflow artifacts logged
+    assert any("shap_summary.png" in a[0] for a in mock_mlflow["artifacts"])
+    assert any("shap_dependence_" in a[0] for a in mock_mlflow["artifacts"])
+
+    # Check metrics logged for top features
+    assert any(k.startswith("shap_mean_abs_") for k in mock_mlflow["metrics"].keys())
+
+
+def test_run_shap_invalid_step_raises(dummy_pipeline):
+    model_path, cache_file = dummy_pipeline
+    with pytest.raises(KeyError):
+        shap_analysis.run_shap(model_path, cache_file, clf_step="not_a_step")
+
+
+def test_run_shap_invalid_cache_format(dummy_pipeline, tmp_path):
+    model_path, _ = dummy_pipeline
+    bad_file = tmp_path / "cache.txt"
+    bad_file.write_text("dummy")
+    with pytest.raises(ValueError):
+        shap_analysis.run_shap(model_path, str(bad_file))
Index: src/features/feature_engineering.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/src/features/feature_engineering.py b/src/features/feature_engineering.py
new file mode 100644
--- /dev/null	(date 1765470426246)
+++ b/src/features/feature_engineering.py	(date 1765470426246)
@@ -0,0 +1,107 @@
+# ============================================================
+# File: src/features/feature_engineering.py
+# Purpose: Generate features for NBA games with rolling stats + home/away flag + sanity checks + CSV append
+# ============================================================
+
+import pandas as pd
+import logging
+import os
+from datetime import datetime
+
+logger = logging.getLogger("features.feature_engineering")
+
+
+def generate_features_for_games(
+    df: pd.DataFrame, stats_out="data/results/feature_stats.csv"
+) -> pd.DataFrame:
+    """
+    Generate features for NBA games.
+    Handles TEAM_NAME/TEAM_ABBREVIATION gracefully, adds rolling stats, home/away flag,
+    logs sanity checks, and appends distributions to CSV for historical tracking.
+    """
+
+    # --- Target column ---
+    if "WL" in df.columns:
+        df["win"] = df["WL"].apply(lambda x: 1 if str(x).upper() == "W" else 0)
+        logger.info("Using WL column to generate win target.")
+    elif "win" in df.columns:
+        logger.info("Win column already present.")
+    else:
+        logger.error("No WL or win column found. Cannot generate target.")
+        return pd.DataFrame()
+
+    # --- Team identifier ---
+    team_col = None
+    if "TEAM_NAME" in df.columns:
+        team_col = "TEAM_NAME"
+    elif "TEAM_ABBREVIATION" in df.columns:
+        team_col = "TEAM_ABBREVIATION"
+
+    # --- Base features ---
+    features = pd.DataFrame()
+    features["GAME_ID"] = df["GAME_ID"]
+    if team_col:
+        features["TEAM"] = df[team_col]
+    if "MATCHUP" in df.columns:
+        features["MATCHUP"] = df["MATCHUP"]
+        # Home/away flag: '@' means away, otherwise home
+        features["home_game"] = df["MATCHUP"].apply(lambda x: 0 if "@" in str(x) else 1)
+        # Log distribution
+        home_counts = features["home_game"].value_counts().to_dict()
+        logger.info("Home/Away distribution: %s", home_counts)
+    if "GAME_DATE" in df.columns:
+        features["GAME_DATE"] = pd.to_datetime(df["GAME_DATE"], errors="coerce")
+    features["win"] = df["win"]
+
+    # --- Sanity check: wins vs losses ---
+    win_counts = features["win"].value_counts().to_dict()
+    logger.info("Win/Loss distribution: %s", win_counts)
+
+    # --- Export sanity checks (append mode) ---
+    stats = pd.DataFrame(
+        [
+            {
+                "timestamp": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
+                "metric": "home_game",
+                "home": home_counts.get(1, 0),
+                "away": home_counts.get(0, 0),
+            },
+            {
+                "timestamp": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
+                "metric": "win",
+                "wins": win_counts.get(1, 0),
+                "losses": win_counts.get(0, 0),
+            },
+        ]
+    )
+
+    os.makedirs(os.path.dirname(stats_out), exist_ok=True)
+    if os.path.exists(stats_out):
+        stats.to_csv(stats_out, mode="a", header=False, index=False)
+    else:
+        stats.to_csv(stats_out, index=False)
+    logger.info("Feature sanity stats appended to %s", stats_out)
+
+    # --- Rolling stats ---
+    if "PTS" in df.columns:
+        df = df.sort_values(["TEAM_ID", "GAME_DATE"])
+        df["rolling_pts"] = df.groupby("TEAM_ID")["PTS"].transform(
+            lambda x: x.shift().rolling(5, min_periods=1).mean()
+        )
+        features["rolling_pts"] = df["rolling_pts"]
+
+    if "PTS_OPP" in df.columns:
+        df = df.sort_values(["TEAM_ID", "GAME_DATE"])
+        df["rolling_pts_allowed"] = df.groupby("TEAM_ID")["PTS_OPP"].transform(
+            lambda x: x.shift().rolling(5, min_periods=1).mean()
+        )
+        features["rolling_pts_allowed"] = df["rolling_pts_allowed"]
+
+    # Win streak feature
+    df["win_streak"] = df.groupby("TEAM_ID")["win"].transform(
+        lambda x: x.shift().rolling(5, min_periods=1).sum()
+    )
+    features["win_streak"] = df["win_streak"]
+
+    logger.info("Generated features with shape %s", features.shape)
+    return features
Index: README.md
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+># NBA Prediction Pipeline\n\nA Python pipeline for fetching NBA game data, generating features, training a logistic regression model, and producing daily win probability predictions. Outputs are saved locally in organized folders and can be connected directly to Power BI for dashboards.\n\n---\n\n## \uD83D\uDE80 How to Run\n\n1. Install dependencies:\n   ```bash\n   pip install -r requirements.txt\n   ```\n2. Run the pipeline\n```bash\n  python run_pipeline.py\n```\n3. Outputs will be saved automatically into the data/ folder structure.\n```bash\nproject_root/\n│\n├── run_pipeline.py        # main pipeline script\n├── config.yaml            # configuration file\n├── models/                # trained model files\n├── data/\n│   ├── raw/               # raw API pulls (optional)\n│   ├── cache/             # cached training features\n│   ├── history/           # historical predictions\n│   ├── csv/               # daily CSV outputs\n│   ├── parquet/           # daily Parquet outputs\n│   └── logs/              # pipeline + error logs\n└── tests/                 # unit tests\n```\n4. Power Bi Integration\n- Connect to Historical Prediction\n  - Open Power BI Desktop\n  - Go to Home -> Get Data -> Parquet\n  - Select data/history/predictions_history.parquet\n  - Load the table into Power BI.\n\n5. Connect to Multiple Daily Files\n  - Use the Folder connector:\n    - for CSVs -> data/csv/\n    - for Parquet -> data/parquet/\n    - Load the table into Power BI\n\n## Example Dashboards\n  - Accuracy trend → Line chart with prediction_date vs. accuracy.\n  - Team analytics → Bar chart with TEAM_ID vs. average pred_proba.\n  - Game drill_downs → Table with stats + predictions.\n\n6. \uD83D\uDEE0 Features- Data Quality Checks → Validates critical columns, drops nulls, logs anomalies.\n   - Error Handling → Retries API calls with exponential backoff, logs errors separately.\n   - Configurable → Paths, seasons, and model path defined in config.yaml.\n   - Environment Separation → Raw, cache, history, CSV, Parquet, logs all in distinct folders.\n   - Deduplication → Unique IDs prevent duplicate rows.\n   - Performance → Batch feature generation speeds up initial fetch.\n   - Unit Tests → Core functions tested with pytest.\n\n## \uD83D\uDC65 Contributors- Developed in Python with ❤\uFE0F for NBA analytics.\n  - Designed for easy integration with Power BI.\n\n## \uD83D\uDCC8 Roadmap- v1.0 → Local Parquet/CSV storage, Power BI dashboards.\n  - v2.0 → Optional migration to SQLite/PostgreSQL for larger datasets.\n  - Future → Cloud integration (Azure Synapse, BigQuery, etc.).\n\n## - Version 1.0 → stick with logistic regression + clean pipeline (done).\n    - Version 2.0 → migrate storage to SQLite/Postgres.\n    - Version 3.0 → add AI models (XGBoost or neural nets) and integrate explainability.\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/README.md b/README.md
--- a/README.md	(revision 00055356329a845d3b2ca81b77c3ffb2e2b15969)
+++ b/README.md	(date 1765470426211)
@@ -1,68 +1,210 @@
-# NBA Prediction Pipeline
-
-A Python pipeline for fetching NBA game data, generating features, training a logistic regression model, and producing daily win probability predictions. Outputs are saved locally in organized folders and can be connected directly to Power BI for dashboards.
+# 📘 NBA Prediction Pipeline — Clean & Production-Ready README (v1.3)
+*A modular Python pipeline for fetching NBA game data, generating features, training ML models, and producing daily win-probability predictions. Fully compatible with Power BI.*
 
 ---
 
-## 🚀 How to Run
+# 🚀 Quick Start
 
-1. Install dependencies:
-   ```bash
-   pip install -r requirements.txt
-   ```
-2. Run the pipeline
+### **1. Install requirements**
+```bash
+pip install -r requirements.txt
+```
+### 2.Run the daily prediction runner
 ```bash
-  python run_pipeline.py
+python run_pipeline.py --model models/nba_logreg.pkl
 ```
-3. Outputs will be saved automatically into the data/ folder structure.
+### 3.  Optional) Run the MLflow-enabled runner
 ```bash
-project_root/
+python daily_runner_mflow.py --model models/nba_logreg.pkl
+```
+### 4. View outputs
+All outputs are saved automatically into the standardized folder structure:
+```bash
+data/
+  raw/           # raw NBA API dumps (optional)
+  cache/         # cached training features
+  history/       # historical predictions
+  csv/           # daily CSV predictions
+  parquet/       # daily Parquet predictions
+  logs/          # runner logs + API failure logs
+models/
+results/
+```
+Your predictions are now ready for Power BI dashboards.
+
+# 🏗 Project Structure
+
+```
+nba_analysis/
 │
-├── run_pipeline.py        # main pipeline script
-├── config.yaml            # configuration file
-├── models/                # trained model files
+├── src/
+│   ├── api/
+│   │   └── nba_api_wrapper.py
+│   ├── features/
+│   ├── model_training/
+│   ├── prediction_engine/
+│   ├── tracker/
+│   │   └── game_tracker.py
+│   ├── utils/
+│   │   ├── add_unique_id.py
+│   │   ├── io.py
+│   │   ├── logging.py
+│   │   ├── logging_config.py
+│   │   ├── mapping.py
+│   │   ├── nba_api_wrapper.py
+│   │   ├── validation.py
+│   └── scripts/
+│       ├── generate_historical_schedule.py
+│       └── generate_today_schedule.py
 ├── data/
-│   ├── raw/               # raw API pulls (optional)
-│   ├── cache/             # cached training features
-│   ├── history/           # historical predictions
-│   ├── csv/               # daily CSV outputs
-│   ├── parquet/           # daily Parquet outputs
-│   └── logs/              # pipeline + error logs
-└── tests/                 # unit tests
+│   ├── cache/
+│   └── results/
+├── logs/
+├── models/
+├── tests/
+├── docs/
+├── .editorconfig
+├── .gitignore
+├── requirements.txt
+├── setup_project.sh
+└── Makefile
+
 ```
-4. Power Bi Integration
-- Connect to Historical Prediction
-  - Open Power BI Desktop
-  - Go to Home -> Get Data -> Parquet
-  - Select data/history/predictions_history.parquet
-  - Load the table into Power BI.
+📊 Power BI Integration
+1. Load Historical Prediction Data
 
-5. Connect to Multiple Daily Files
-  - Use the Folder connector:
-    - for CSVs -> data/csv/
-    - for Parquet -> data/parquet/
-    - Load the table into Power BI
+Power BI → Get Data → Parquet
+
+Select:
+````
+data/history/predictions_history.parquet
+````
+2. Load Multiple Daily Prediction Files
+
+- Use the Folder connector:
+
+  -For CSVs: data/csv/
+
+- For Parquet: data/parquet/
+
+Power BI automatically appends all files.
+
+🛠 Key Pipeline Features
+1. Data Quality Checks
+
+validates required columns
+
+ensures correct data types
+
+detects anomalies
 
-## Example Dashboards
-  - Accuracy trend → Line chart with prediction_date vs. accuracy.
-  - Team analytics → Bar chart with TEAM_ID vs. average pred_proba.
-  - Game drill_downs → Table with stats + predictions.
+logs issues to data/logs/
 
-6. 🛠 Features- Data Quality Checks → Validates critical columns, drops nulls, logs anomalies.
-   - Error Handling → Retries API calls with exponential backoff, logs errors separately.
-   - Configurable → Paths, seasons, and model path defined in config.yaml.
-   - Environment Separation → Raw, cache, history, CSV, Parquet, logs all in distinct folders.
-   - Deduplication → Unique IDs prevent duplicate rows.
-   - Performance → Batch feature generation speeds up initial fetch.
-   - Unit Tests → Core functions tested with pytest.
+2. Error Handling
 
-## 👥 Contributors- Developed in Python with ❤️ for NBA analytics.
-  - Designed for easy integration with Power BI.
+automatic retry logic with backoff
 
-## 📈 Roadmap- v1.0 → Local Parquet/CSV storage, Power BI dashboards.
-  - v2.0 → Optional migration to SQLite/PostgreSQL for larger datasets.
-  - Future → Cloud integration (Azure Synapse, BigQuery, etc.).
+safe API wrappers
 
-## - Version 1.0 → stick with logistic regression + clean pipeline (done).
-    - Version 2.0 → migrate storage to SQLite/Postgres.
-    - Version 3.0 → add AI models (XGBoost or neural nets) and integrate explainability.
+separate error logs
+
+3. Config-Driven
+
+config.yaml controls:
+
+seasons
+
+model paths
+
+thresholds
+
+save locations
+
+retry settings
+
+MLflow parameters
+
+4. File Structure Organization
+
+Separate folders for:
+
+raw API data
+
+feature cache
+
+prediction history
+
+CSV & Parquet daily outputs
+
+logs
+
+5. Deduplication
+
+Unified ID prevents duplicate rows:
+
+GAME_ID
+
+TEAM_ID
+
+prediction_date
+
+6. Performance
+
+Vectorized feature engineering
+
+Batch operations
+
+Cached repeated lookups
+
+7. Tested with pytest
+
+Core components include tests:
+
+feature generation
+
+API wrapper
+
+predictor logic
+
+data cleaning
+
+👥 Contributors
+
+Developed in Python with ❤️ for NBA analytics, reproducible ML pipelines, and Power BI integration.
+
+🗺 Roadmap
+v1.0 — Complete
+
+Logistic regression baseline
+
+Clean pipeline
+
+CSV/Parquet outputs
+
+Power BI dashboards
+
+v2.0 — Coming Soon
+
+Migrate storage to SQLite/Postgres
+
+Historical rollups
+
+Scheduled ETL jobs
+
+v3.0 — ML Enhancements
+
+XGBoost / Random Forest / Neural Net models
+
+SHAP explainability
+
+MLflow model versioning
+
+v4.0 — Cloud Integration
+
+Azure Synapse
+
+BigQuery
+
+AWS Glue
+
+cloud-based MLflow
Index: tests/test_feature_engineering.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/tests/test_feature_engineering.py b/tests/test_feature_engineering.py
new file mode 100644
--- /dev/null	(date 1765470426271)
+++ b/tests/test_feature_engineering.py	(date 1765470426271)
@@ -0,0 +1,84 @@
+# ============================================================
+# File: tests/test_feature_engineering.py
+# Purpose: Unit tests for feature_engineering.generate_features_for_games
+# Project: nba_analysis
+# ============================================================
+
+import pandas as pd
+import numpy as np
+import pytest
+
+from src.features.feature_engineering import (
+    generate_features_for_games,
+    EXPECTED_COLUMNS,
+)
+
+
+def test_empty_input_returns_expected_schema():
+    df = generate_features_for_games([])
+    assert list(df.columns) == EXPECTED_COLUMNS
+    assert df.empty
+
+
+def test_dict_input_basic_fields():
+    games = [{"GAME_ID": "001", "TEAM_ID": 1610612747}]
+    df = generate_features_for_games(games)
+    assert "unique_id" in df.columns
+    assert df.iloc[0]["GAME_ID"] == "001"
+    assert df.iloc[0]["TEAM_ID"] == 1610612747
+
+
+def test_dataframe_input_with_game_date_and_points():
+    df_in = pd.DataFrame(
+        {
+            "GAME_ID": ["001", "002"],
+            "TEAM_ID": [1, 1],
+            "GAME_DATE": ["2025-12-01", "2025-12-05"],
+            "POINTS": [100, 110],
+            "TARGET": [1, 0],
+        }
+    )
+    df_out = generate_features_for_games(df_in)
+    assert "RollingPTS_5" in df_out.columns
+    assert not df_out["RollingPTS_5"].isna().all()
+    assert "RollingWinPct_10" in df_out.columns
+    assert not df_out["RollingWinPct_10"].isna().all()
+    assert "RestDays" in df_out.columns
+    assert df_out["RestDays"].iloc[1] == 4  # 5th - 1st December
+
+
+def test_player_points_dict_and_list():
+    games = [
+        {"GAME_ID": "003", "TEAM_ID": 2, "PLAYER_POINTS": {"p1": 25, "p2": 10}},
+        {"GAME_ID": "004", "TEAM_ID": 3, "PLAYER_POINTS": [12, 22, 30]},
+        {
+            "GAME_ID": "005",
+            "TEAM_ID": 4,
+            "PLAYER_POINTS": [{"pid": 1, "pts": 28}, {"pid": 2, "pts": 15}],
+        },
+    ]
+    df = generate_features_for_games(games)
+    assert all(df["Players20PlusPts"] >= 1)
+
+
+def test_opponent_win_pct_merge():
+    df_in = pd.DataFrame(
+        {
+            "GAME_ID": ["006", "007"],
+            "TEAM_ID": [1, 2],
+            "OPPONENT_TEAM_ID": [2, 1],
+            "GAME_DATE": ["2025-12-01", "2025-12-02"],
+            "TARGET": [1, 0],
+        }
+    )
+    df_out = generate_features_for_games(df_in)
+    assert "OppWinPctToDate" in df_out.columns
+    # Opponent win pct should be numeric or NaN
+    assert pd.api.types.is_numeric_dtype(df_out["OppWinPctToDate"])
+
+
+def test_expected_columns_always_present():
+    games = [{"GAME_ID": "008", "TEAM_ID": 5}]
+    df = generate_features_for_games(games)
+    for col in EXPECTED_COLUMNS:
+        assert col in df.columns
Index: src/model_training/trainer_cli.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/src/model_training/trainer_cli.py b/src/model_training/trainer_cli.py
new file mode 100644
--- /dev/null	(date 1765470426251)
+++ b/src/model_training/trainer_cli.py	(date 1765470426251)
@@ -0,0 +1,134 @@
+# ============================================================
+# File: src/model_training/trainer_cli.py
+# Purpose: CLI for training models and logging metrics + feature importance
+# ============================================================
+
+import logging
+import click
+import os
+import pandas as pd
+import joblib
+from sklearn.metrics import (
+    accuracy_score,
+    precision_score,
+    recall_score,
+    f1_score,
+    roc_auc_score,
+)
+from sklearn.model_selection import train_test_split
+from sklearn.linear_model import LogisticRegression
+from sklearn.ensemble import RandomForestClassifier
+from xgboost import XGBClassifier
+
+logger = logging.getLogger("model_training.trainer_cli")
+logging.basicConfig(level=logging.INFO)
+
+
+def evaluate_model(model, X_test, y_test, label):
+    """Evaluate model and return metrics dict."""
+    y_pred = model.predict(X_test)
+    y_prob = None
+    try:
+        y_prob = model.predict_proba(X_test)[:, 1]
+    except Exception:
+        pass
+
+    metrics = {
+        "model": label,
+        "accuracy": accuracy_score(y_test, y_pred),
+        "precision": precision_score(y_test, y_pred, zero_division=0),
+        "recall": recall_score(y_test, y_pred, zero_division=0),
+        "f1": f1_score(y_test, y_pred, zero_division=0),
+        "roc_auc": roc_auc_score(y_test, y_prob) if y_prob is not None else None,
+    }
+    return metrics
+
+
+@click.command()
+@click.option(
+    "--model",
+    default="xgb",
+    type=click.Choice(["logreg", "rf", "xgb"]),
+    help="Model type",
+)
+@click.option("--season", required=True, type=int, help="Season year (e.g., 2025)")
+@click.option("--features", required=True, help="Path to features parquet file")
+@click.option(
+    "--out", default="models/nba_model.pkl", help="Path to save trained model"
+)
+@click.option(
+    "--metrics_out",
+    default="data/results/model_metrics.csv",
+    help="Path to save metrics CSV",
+)
+@click.option(
+    "--importance_out",
+    default="data/results/feature_importance.csv",
+    help="Path to save feature importance CSV",
+)
+def cli(model, season, features, out, metrics_out, importance_out):
+    logger.info("Training model: %s", model)
+
+    # --- Load features ---
+    df = pd.read_parquet(features)
+    X = df.drop(columns=["WIN"], errors="ignore")
+    y = df["WIN"] if "WIN" in df.columns else None
+
+    if y is None:
+        logger.error("No WIN column found in features file.")
+        return
+
+    # Train/test split
+    X_train, X_test, y_train, y_test = train_test_split(
+        X, y, test_size=0.2, random_state=42
+    )
+
+    # --- Select model ---
+    if model == "logreg":
+        clf = LogisticRegression(max_iter=1000)
+    elif model == "rf":
+        clf = RandomForestClassifier(n_estimators=200, random_state=42)
+    else:  # xgb
+        clf = XGBClassifier(
+            n_estimators=300,
+            learning_rate=0.05,
+            max_depth=6,
+            subsample=0.8,
+            colsample_bytree=0.8,
+            random_state=42,
+            use_label_encoder=False,
+            eval_metric="logloss",
+        )
+
+    # --- Train ---
+    clf.fit(X_train, y_train)
+
+    # --- Evaluate ---
+    metrics = evaluate_model(clf, X_test, y_test, model)
+    os.makedirs(os.path.dirname(metrics_out), exist_ok=True)
+    pd.DataFrame([metrics]).to_csv(metrics_out, index=False)
+    logger.info("Metrics saved to %s", metrics_out)
+
+    # --- Save model ---
+    os.makedirs(os.path.dirname(out), exist_ok=True)
+    joblib.dump(clf, out)
+    logger.info("Model saved to %s", out)
+
+    # --- Feature importance ---
+    importance_df = pd.DataFrame()
+    if model == "rf":
+        importance_df = pd.DataFrame(
+            {"feature": X.columns, "importance": clf.feature_importances_}
+        ).sort_values("importance", ascending=False)
+    elif model == "xgb":
+        importance_df = pd.DataFrame(
+            {"feature": X.columns, "importance": clf.feature_importances_}
+        ).sort_values("importance", ascending=False)
+
+    if not importance_df.empty:
+        importance_df.to_csv(importance_out, index=False)
+        logger.info("Feature importance saved to %s", importance_out)
+
+
+if __name__ == "__main__":
+    cli()
Index: src/add_header.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+># ============================================================\n# Path: scripts/add_header.py\n# File: add_header.py\n# Purpose: Pre-commit hook script to enforce standardized headers in Python files\n# Project: nba_analysis\n# ============================================================\n\nimport pathlib\nimport sys\n\nHEADER_TEMPLATE = \"\"\"# ============================================================\n# Path: {path}\n# File: {file}\n# Purpose: <add short description here>\n# Project: nba_analysis\n# ============================================================\n\n\"\"\"\n\nEXPECTED_START = \"# ============================================================\"\nEXPECTED_PROJECT = \"# Project: nba_analysis\"\n\n\ndef has_valid_header(content: list[str]) -> bool:\n    \"\"\"Check if the file already has a valid standardized header.\"\"\"\n    return (\n        len(content) >= 5\n        and content[0].startswith(EXPECTED_START)\n        and any(EXPECTED_PROJECT in line for line in content[:6])\n    )\n\n\ndef ensure_header(file_path: pathlib.Path):\n    \"\"\"Ensure the file has a standardized header, fixing malformed ones if needed.\"\"\"\n    content = file_path.read_text(encoding=\"utf-8\").splitlines()\n\n    if has_valid_header(content):\n        # Already valid, do nothing\n        return\n\n    # Always replace with a fresh header\n    header = HEADER_TEMPLATE.format(path=file_path.as_posix(), file=file_path.name)\n    new_content = header + \"\\n\".join(content)\n    file_path.write_text(new_content, encoding=\"utf-8\")\n\n\ndef main():\n    for filename in sys.argv[1:]:\n        path = pathlib.Path(filename)\n        if path.suffix == \".py\":\n            ensure_header(path)\n\n\nif __name__ == \"__main__\":\n    main()\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/src/add_header.py b/src/add_header.py
--- a/src/add_header.py	(revision 00055356329a845d3b2ca81b77c3ffb2e2b15969)
+++ b/src/add_header.py	(date 1765470426239)
@@ -1,55 +1,48 @@
+import os
+from datetime import datetime
+
+# Template for header
+header_template = """
 # ============================================================
-# Path: scripts/add_header.py
-# File: add_header.py
-# Purpose: Pre-commit hook script to enforce standardized headers in Python files
-# Project: nba_analysis
+# File: {filename}
+# Purpose: {purpose}
+# Version: 1.2
+# Author: Your Team
+# Date: {date}
 # ============================================================
-
-import pathlib
-import sys
-
-HEADER_TEMPLATE = """# ============================================================
-# Path: {path}
-# File: {file}
-# Purpose: <add short description here>
-# Project: nba_analysis
-# ============================================================
-
 """
 
-EXPECTED_START = "# ============================================================"
-EXPECTED_PROJECT = "# Project: nba_analysis"
-
-
-def has_valid_header(content: list[str]) -> bool:
-    """Check if the file already has a valid standardized header."""
-    return (
-        len(content) >= 5
-        and content[0].startswith(EXPECTED_START)
-        and any(EXPECTED_PROJECT in line for line in content[:6])
-    )
 
+def add_header_to_script(script_path, purpose):
+    """
+    Adds the header to the script if it's not already present.
+    """
+    date = datetime.now().strftime("%B %Y")
+    filename = os.path.basename(script_path)
+    header = header_template.format(filename=filename, purpose=purpose, date=date)
 
-def ensure_header(file_path: pathlib.Path):
-    """Ensure the file has a standardized header, fixing malformed ones if needed."""
-    content = file_path.read_text(encoding="utf-8").splitlines()
+    # Read the content of the script
+    with open(script_path, 'r') as file:
+        content = file.read()
 
-    if has_valid_header(content):
-        # Already valid, do nothing
-        return
+    # If the header is already present, do nothing
+    if not content.startswith(
+        "# =========================================================="):
+        with open(script_path, 'w') as file:
+            # Prepend the header and then the rest of the content
+            file.write(header + content)
 
-    # Always replace with a fresh header
-    header = HEADER_TEMPLATE.format(path=file_path.as_posix(), file=file_path.name)
-    new_content = header + "\n".join(content)
-    file_path.write_text(new_content, encoding="utf-8")
 
-
-def main():
-    for filename in sys.argv[1:]:
-        path = pathlib.Path(filename)
-        if path.suffix == ".py":
-            ensure_header(path)
+def process_directory(directory, purpose="Automated header insertion"):
+    """
+    Walk through all files in the given directory and insert header into Python files.
+    """
+    for subdir, _, files in os.walk(directory):
+        for file in files:
+            if file.endswith('.py'):
+                script_path = os.path.join(subdir, file)
+                add_header_to_script(script_path, purpose)
 
 
-if __name__ == "__main__":
-    main()
+# Specify the root directory where your scripts are located
+process_directory("src")  # You can change this to any other directory if needed
Index: src/features/__init__.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/src/features/__init__.py b/src/features/__init__.py
new file mode 100644
--- /dev/null	(date 1765470426245)
+++ b/src/features/__init__.py	(date 1765470426245)
@@ -0,0 +1,12 @@
+# ============================================================
+# Path: src/features/__init__.py
+# Purpose: Initialize features package
+# Version: 1.1
+# ============================================================
+
+# Expose key functions at package level
+from .feature_engineering import generate_features_for_games
+
+__all__ = [
+    "generate_features_for_games",
+]
Index: src/model_training/compare_algorithms.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/src/model_training/compare_algorithms.py b/src/model_training/compare_algorithms.py
new file mode 100644
--- /dev/null	(date 1765470426249)
+++ b/src/model_training/compare_algorithms.py	(date 1765470426249)
@@ -0,0 +1,96 @@
+# ============================================================
+# File: src/model_training/compare_algorithms.py
+# Purpose: Compare multiple ML algorithms and log metrics
+# ============================================================
+
+import logging
+import os
+import pandas as pd
+from sklearn.model_selection import train_test_split
+from sklearn.metrics import (
+    accuracy_score,
+    precision_score,
+    recall_score,
+    f1_score,
+    roc_auc_score,
+)
+from sklearn.linear_model import LogisticRegression
+from sklearn.ensemble import RandomForestClassifier
+from xgboost import XGBClassifier
+
+logger = logging.getLogger("model_training.compare_algorithms")
+logging.basicConfig(level=logging.INFO)
+
+
+# --- Helper: Evaluate model ---
+def evaluate_model(model, X_test, y_test, label):
+    y_pred = model.predict(X_test)
+    y_prob = None
+    try:
+        y_prob = model.predict_proba(X_test)[:, 1]
+    except Exception:
+        pass
+
+    metrics = {
+        "model": label,
+        "accuracy": accuracy_score(y_test, y_pred),
+        "precision": precision_score(y_test, y_pred, zero_division=0),
+        "recall": recall_score(y_test, y_pred, zero_division=0),
+        "f1": f1_score(y_test, y_pred, zero_division=0),
+        "roc_auc": roc_auc_score(y_test, y_prob) if y_prob is not None else None,
+    }
+    return metrics
+
+
+def main():
+    # --- Load features ---
+    features_path = "data/cache/features_full.parquet"
+    df = pd.read_parquet(features_path)
+
+    # Assume target column is 'WIN' (1=win, 0=loss)
+    X = df.drop(columns=["WIN"])
+    y = df["WIN"]
+
+    # Train/test split
+    X_train, X_test, y_train, y_test = train_test_split(
+        X, y, test_size=0.2, random_state=42
+    )
+
+    results = []
+
+    # --- Logistic Regression ---
+    logger.info("Training Logistic Regression...")
+    lr = LogisticRegression(max_iter=1000)
+    lr.fit(X_train, y_train)
+    results.append(evaluate_model(lr, X_test, y_test, "LogisticRegression"))
+
+    # --- Random Forest ---
+    logger.info("Training Random Forest...")
+    rf = RandomForestClassifier(n_estimators=200, random_state=42)
+    rf.fit(X_train, y_train)
+    results.append(evaluate_model(rf, X_test, y_test, "RandomForest"))
+
+    # --- XGBoost ---
+    logger.info("Training XGBoost...")
+    xgb = XGBClassifier(
+        n_estimators=300,
+        learning_rate=0.05,
+        max_depth=6,
+        subsample=0.8,
+        colsample_bytree=0.8,
+        random_state=42,
+        use_label_encoder=False,
+        eval_metric="logloss",
+    )
+    xgb.fit(X_train, y_train)
+    results.append(evaluate_model(xgb, X_test, y_test, "XGBoost"))
+
+    # --- Save metrics ---
+    out_path = "data/results/model_comparison.csv"
+    os.makedirs(os.path.dirname(out_path), exist_ok=True)
+    pd.DataFrame(results).to_csv(out_path, index=False)
+    logger.info("Comparison metrics saved to %s", out_path)
+
+
+if __name__ == "__main__":
+    main()
Index: src/model_training/utils.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/src/model_training/utils.py b/src/model_training/utils.py
new file mode 100644
--- /dev/null	(date 1765470426251)
+++ b/src/model_training/utils.py	(date 1765470426251)
@@ -0,0 +1,68 @@
+# ============================================================
+# File: src/model_training/utils.py
+# Purpose: Utility functions for model loading and feature building
+# ============================================================
+
+import logging
+import joblib
+import pandas as pd
+
+logger = logging.getLogger("model_training.utils")
+
+
+# --- Load model ---
+def load_model(path: str):
+    """Load a trained model from disk using joblib."""
+    try:
+        model = joblib.load(path)
+        logger.info("Model loaded from %s", path)
+        return model
+    except FileNotFoundError:
+        logger.error("Model file not found: %s", path)
+        raise
+    except Exception as e:
+        logger.error("Failed to load model from %s: %s", path, e)
+        raise
+
+
+# --- Build features ---
+def build_features(games_df: pd.DataFrame, season: int) -> pd.DataFrame:
+    """
+    Build enriched features for prediction from games DataFrame.
+    Includes rolling averages, home/away flags, win streaks.
+    """
+    try:
+        df = games_df.copy()
+
+        # --- Home/Away flag ---
+        df["IS_HOME"] = df["HOME_TEAM_ABBREVIATION"].apply(
+            lambda x: 1 if pd.notna(x) else 0
+        )
+
+        # --- Rolling averages (points scored/allowed) ---
+        if "PTS" in df.columns and "PTS_OPP" in df.columns:
+            df["AVG_PTS_LAST3"] = df.groupby("HOME_TEAM_ABBREVIATION")["PTS"].transform(
+                lambda x: x.shift().rolling(3, min_periods=1).mean()
+            )
+            df["AVG_PTS_ALLOWED_LAST3"] = df.groupby("HOME_TEAM_ABBREVIATION")[
+                "PTS_OPP"
+            ].transform(lambda x: x.shift().rolling(3, min_periods=1).mean())
+
+        # --- Win streaks ---
+        if "WL" in df.columns:
+            df["WIN_STREAK"] = df.groupby("HOME_TEAM_ABBREVIATION")["WL"].transform(
+                lambda x: x.eq("W").astype(int).groupby(x.ne("W").cumsum()).cumsum()
+            )
+
+        # --- Season flag ---
+        df["SEASON"] = season
+
+        # --- Select numeric features only ---
+        features = df.select_dtypes(include=["number"]).fillna(0)
+
+        logger.info("Features built (shape: %s)", features.shape)
+        return features
+
+    except Exception as e:
+        logger.error("Failed to build features: %s", e)
+        raise
Index: src/model_training/train.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/src/model_training/train.py b/src/model_training/train.py
new file mode 100644
--- /dev/null	(date 1765470426250)
+++ b/src/model_training/train.py	(date 1765470426250)
@@ -0,0 +1,92 @@
+# ============================================================
+# File: src/model_training/train.py
+# Purpose: Train NBA prediction models on engineered features
+# Project: nba_analysis
+# Version: 2.0 (schedule-based features, safe preprocessing)
+# ============================================================
+
+import logging
+import pandas as pd
+import numpy as np
+
+from sklearn.model_selection import train_test_split
+from sklearn.linear_model import LogisticRegression
+from sklearn.metrics import accuracy_score
+import xgboost as xgb
+
+logger = logging.getLogger("model_training.train")
+if not logger.handlers:
+    handler = logging.StreamHandler()
+    handler.setFormatter(logging.Formatter("%(asctime)s [%(levelname)s] %(message)s"))
+    logger.addHandler(handler)
+    logger.setLevel(logging.INFO)
+
+
+def _prepare_data(features_df: pd.DataFrame):
+    """
+    Prepare X and y for training.
+    Expects schedule-based features with HOME_GAME, DAYS_SINCE_GAME, and optional win target.
+    """
+    df = features_df.copy()
+
+    # Target variable
+    if "win" not in df.columns:
+        logger.warning("No 'win' column found. Using HOME_GAME as proxy target.")
+        df["win"] = df["HOME_GAME"]
+
+    y = df["win"]
+
+    # Feature set: numeric/categorical
+    feature_cols = []
+    for col in ["HOME_GAME", "DAYS_SINCE_GAME"]:
+        if col in df.columns:
+            feature_cols.append(col)
+
+    # Encode TEAM_NAME as categorical if present
+    if "TEAM_NAME" in df.columns:
+        df["TEAM_NAME"] = df["TEAM_NAME"].astype("category").cat.codes
+        feature_cols.append("TEAM_NAME")
+
+    X = df[feature_cols]
+
+    logger.info("Prepared data with X shape %s, y shape %s", X.shape, y.shape)
+    return X, y
+
+
+def train_model(model_type: str, features_df: pd.DataFrame):
+    """
+    Train a model (logreg or xgb) on the provided features DataFrame.
+    """
+    X, y = _prepare_data(features_df)
+
+    X_train, X_test, y_train, y_test = train_test_split(
+        X, y, test_size=0.2, random_state=42, stratify=y
+    )
+
+    if model_type == "logreg":
+        model = LogisticRegression(max_iter=1000)
+        model.fit(X_train, y_train)
+        preds = model.predict(X_test)
+        acc = accuracy_score(y_test, preds)
+        logger.info("Logistic Regression accuracy: %.3f", acc)
+        return model
+
+    elif model_type == "xgb":
+        model = xgb.XGBClassifier(
+            n_estimators=200,
+            max_depth=4,
+            learning_rate=0.1,
+            subsample=0.8,
+            colsample_bytree=0.8,
+            random_state=42,
+            use_label_encoder=False,
+            eval_metric="logloss",
+        )
+        model.fit(X_train, y_train)
+        preds = model.predict(X_test)
+        acc = accuracy_score(y_test, preds)
+        logger.info("XGBoost accuracy: %.3f", acc)
+        return model
+
+    else:
+        raise ValueError(f"Unsupported model type: {model_type}")
Index: mlruns/0/760c2a5498f54b02bcc1292f75970c1c/meta.yaml
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/mlruns/0/760c2a5498f54b02bcc1292f75970c1c/meta.yaml b/mlruns/0/760c2a5498f54b02bcc1292f75970c1c/meta.yaml
new file mode 100644
--- /dev/null	(date 1765470426218)
+++ b/mlruns/0/760c2a5498f54b02bcc1292f75970c1c/meta.yaml	(date 1765470426218)
@@ -0,0 +1,15 @@
+artifact_uri: file:///C:/Users/Mohamadou/PycharmProjects/nba_analytics/mlruns/0/760c2a5498f54b02bcc1292f75970c1c/artifacts
+end_time: 1765312123227
+entry_point_name: ''
+experiment_id: '0'
+lifecycle_stage: active
+run_id: 760c2a5498f54b02bcc1292f75970c1c
+run_name: xgb_pipeline_v1.3
+run_uuid: 760c2a5498f54b02bcc1292f75970c1c
+source_name: ''
+source_type: 4
+source_version: ''
+start_time: 1765312123197
+status: 4
+tags: []
+user_id: Mohamadou
Index: src/prediction_engine/predictor.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+># ============================================================\n# Path: src/prediction_engine/predictor.py\n# Filename: predictor.py\n# Author: Your Team\n# Date: December 2025\n# Purpose: Wrapper around trained NBA logistic regression model\n# ============================================================\n\nimport joblib\nimport numpy as np\nimport pandas as pd\nfrom sklearn.linear_model import LogisticRegression\n\nclass NBAPredictor:\n    \"\"\"\n    Wrapper class for NBA logistic regression predictor.\n    \"\"\"\n\n    def __init__(self, model_path: str = None):\n        \"\"\"\n        Initialize predictor by loading trained model.\n\n        Args:\n            model_path (str): Path to saved model file (.pkl).\n        \"\"\"\n        if model_path:\n            self.model = joblib.load(model_path)\n        else:\n            # Default: create a dummy untrained model for CLI usage\n            self.model = LogisticRegression(max_iter=1000)\n\n    def predict(self, X: pd.DataFrame) -> pd.Series:\n        \"\"\"\n        Predict labels (win/loss).\n\n        Args:\n            X (pd.DataFrame): Feature matrix.\n\n        Returns:\n            pd.Series: Predicted labels (0 or 1).\n        \"\"\"\n        preds = self.model.predict(X)\n        return pd.Series(preds, index=X.index)\n\n    def predict_proba(self, X: pd.DataFrame) -> list[float]:\n        \"\"\"\n        Predict probabilities of winning.\n\n        Args:\n            X (pd.DataFrame): Feature matrix.\n\n        Returns:\n            list[float]: List of probabilities for the positive class (win).\n        \"\"\"\n        # predict_proba returns shape (n_samples, 2)\n        proba = self.model.predict_proba(X)\n        # Take probability of class \"1\" (win)\n        return proba[:, 1].tolist()\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/src/prediction_engine/predictor.py b/src/prediction_engine/predictor.py
--- a/src/prediction_engine/predictor.py	(revision 00055356329a845d3b2ca81b77c3ffb2e2b15969)
+++ b/src/prediction_engine/predictor.py	(date 1765470426255)
@@ -1,58 +1,67 @@
 # ============================================================
-# Path: src/prediction_engine/predictor.py
-# Filename: predictor.py
-# Author: Your Team
-# Date: December 2025
-# Purpose: Wrapper around trained NBA logistic regression model
+# File: src/prediction_engine/predictor.py
+# Purpose: Load trained model and run predictions
+# Project: nba_analysis
+# Version: 2.0 (schedule-based features, safe preprocessing)
 # ============================================================
 
+import logging
 import joblib
-import numpy as np
 import pandas as pd
-from sklearn.linear_model import LogisticRegression
 
-class NBAPredictor:
-    """
-    Wrapper class for NBA logistic regression predictor.
-    """
+logger = logging.getLogger("prediction_engine.predictor")
+if not logger.handlers:
+    handler = logging.StreamHandler()
+    handler.setFormatter(logging.Formatter("%(asctime)s [%(levelname)s] %(message)s"))
+    logger.addHandler(handler)
+    logger.setLevel(logging.INFO)
 
-    def __init__(self, model_path: str = None):
-        """
-        Initialize predictor by loading trained model.
 
-        Args:
-            model_path (str): Path to saved model file (.pkl).
-        """
-        if model_path:
+class Predictor:
+    def __init__(self, model_path: str):
+        """Load a trained model from disk."""
+        try:
             self.model = joblib.load(model_path)
+            logger.info("Loaded model from %s", model_path)
+        except Exception as e:
+            logger.error("Failed to load model: %s", e)
+            raise
+
+    def _prepare_features(self, X: pd.DataFrame) -> pd.DataFrame:
+        """
+        Ensure features are in the right format for prediction.
+        Handles categorical TEAM_NAME and numeric HOME_GAME/DAYS_SINCE_GAME.
+        """
+        df = X.copy()
+
+        # TEAM_NAME categorical encoding
+        if "TEAM_NAME" in df.columns:
+            df["TEAM_NAME"] = df["TEAM_NAME"].astype("category").cat.codes
+
+        # HOME_GAME numeric
+        if "HOME_GAME" in df.columns:
+            df["HOME_GAME"] = pd.to_numeric(df["HOME_GAME"], errors="coerce").fillna(0)
+
+        # DAYS_SINCE_GAME numeric
+        if "DAYS_SINCE_GAME" in df.columns:
+            df["DAYS_SINCE_GAME"] = pd.to_numeric(
+                df["DAYS_SINCE_GAME"], errors="coerce"
+            ).fillna(0)
+
+        return df
+
+    def predict_proba(self, X: pd.DataFrame):
+        """Return win probability predictions."""
+        df = self._prepare_features(X)
+        if hasattr(self.model, "predict_proba"):
+            return self.model.predict_proba(df)[:, 1]
         else:
-            # Default: create a dummy untrained model for CLI usage
-            self.model = LogisticRegression(max_iter=1000)
-
-    def predict(self, X: pd.DataFrame) -> pd.Series:
-        """
-        Predict labels (win/loss).
+            logger.warning(
+                "Model does not support predict_proba, using predict instead."
+            )
+            return self.model.predict(df)
 
-        Args:
-            X (pd.DataFrame): Feature matrix.
-
-        Returns:
-            pd.Series: Predicted labels (0 or 1).
-        """
-        preds = self.model.predict(X)
-        return pd.Series(preds, index=X.index)
-
-    def predict_proba(self, X: pd.DataFrame) -> list[float]:
-        """
-        Predict probabilities of winning.
-
-        Args:
-            X (pd.DataFrame): Feature matrix.
-
-        Returns:
-            list[float]: List of probabilities for the positive class (win).
-        """
-        # predict_proba returns shape (n_samples, 2)
-        proba = self.model.predict_proba(X)
-        # Take probability of class "1" (win)
-        return proba[:, 1].tolist()
+    def predict_label(self, X: pd.DataFrame):
+        """Return win/loss label predictions."""
+        df = self._prepare_features(X)
+        return self.model.predict(df)
Index: src/prediction_engine/predictor_cli.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+># ============================================================\n# Path: src/prediction_engine/predictor_cli.py\n# Filename: predictor_cli.py\n# Author: Your Team\n# Date: December 2025\n# Purpose: Command-line interface for NBAPredictor with date filtering\n# ============================================================\n\nimport click\nimport pandas as pd\nfrom src.prediction_engine.predictor import NBAPredictor\nfrom src.prediction_engine.game_features import generate_features_for_games, fetch_season_games\nfrom nba_api.stats.endpoints import leaguegamefinder\n\ndef fetch_games_by_date(year: int, game_date: str) -> list[str]:\n    \"\"\"Fetch NBA game IDs for a specific date (YYYY-MM-DD).\"\"\"\n    finder = leaguegamefinder.LeagueGameFinder(season_nullable=str(year))\n    df = finder.get_data_frames()[0]\n    df_today = df[df[\"GAME_DATE\"] == game_date]\n    return df_today[\"GAME_ID\"].tolist()\n\n@click.command()\n@click.option(\"--limit\", default=None, help=\"Limit number of games to fetch.\")\n@click.option(\"--date\", default=None, help=\"Filter games by date (YYYY-MM-DD).\")\n@click.option(\"--proba\", is_flag=True, help=\"Output predicted probabilities.\")\n@click.option(\"--label\", is_flag=True, help=\"Output predicted labels.\")\n@click.option(\"--tags\", is_flag=True, help=\"Output predictions with tags.\")\n@click.option(\"--all\", is_flag=True, help=\"Output both probabilities and labels together.\")\ndef cli(limit, date, proba, label, tags, all):\n    \"\"\"\n    CLI for running NBA predictions.\n    Fetches games by date or limit, generates features, loads model, and outputs predictions.\n    \"\"\"\n    # -----------------------------\n    # Step 1: Fetch games\n    # -----------------------------\n    if date:\n        game_ids = fetch_games_by_date(2025, date)\n    else:\n        game_ids = fetch_season_games(2025, limit=limit or 5)\n\n    features = generate_features_for_games(game_ids)\n\n    # -----------------------------\n    # Step 2: Load predictor\n    # -----------------------------\n    predictor = NBAPredictor()\n    X = features.drop(columns=[\"win\"])\n    y = features[\"win\"]\n\n    # Fit model if not already fitted\n    if not hasattr(predictor.model, \"classes_\"):\n        predictor.model.fit(X, y)\n\n    # -----------------------------\n    # Step 3: Make predictions\n    # -----------------------------\n    output = pd.DataFrame(index=features.index)\n\n    if all:\n        output[\"proba\"] = predictor.predict_proba(X)\n        output[\"label\"] = predictor.predict(X).tolist()\n    else:\n        if proba:\n            output[\"proba\"] = predictor.predict_proba(X)\n        if label:\n            output[\"label\"] = predictor.predict(X).tolist()\n        if tags:\n            output[\"tags\"] = [[\"NBA\", \"prediction\"] for _ in range(len(output))]\n\n    # -----------------------------\n    # Step 4: Print JSON output\n    # -----------------------------\n    click.echo(output.to_json(orient=\"records\"))\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/src/prediction_engine/predictor_cli.py b/src/prediction_engine/predictor_cli.py
--- a/src/prediction_engine/predictor_cli.py	(revision 00055356329a845d3b2ca81b77c3ffb2e2b15969)
+++ b/src/prediction_engine/predictor_cli.py	(date 1765470426255)
@@ -1,74 +1,69 @@
+#!/usr/bin/env python
 # ============================================================
-# Path: src/prediction_engine/predictor_cli.py
-# Filename: predictor_cli.py
-# Author: Your Team
-# Date: December 2025
-# Purpose: Command-line interface for NBAPredictor with date filtering
+# File: src/prediction_engine/predictor_cli.py
+# Purpose: Command-line interface for NBA Prediction
+# Project: nba_analysis
+# Version: 1.4 (named logger, safe error handling, clearer defaults)
 # ============================================================
 
 import click
 import pandas as pd
+import logging
+import traceback
+
 from src.prediction_engine.predictor import NBAPredictor
-from src.prediction_engine.game_features import generate_features_for_games, fetch_season_games
-from nba_api.stats.endpoints import leaguegamefinder
+from src.features.feature_engineering import generate_features_for_games
+from src.api.nba_api_client import fetch_season_games
 
-def fetch_games_by_date(year: int, game_date: str) -> list[str]:
-    """Fetch NBA game IDs for a specific date (YYYY-MM-DD)."""
-    finder = leaguegamefinder.LeagueGameFinder(season_nullable=str(year))
-    df = finder.get_data_frames()[0]
-    df_today = df[df["GAME_DATE"] == game_date]
-    return df_today["GAME_ID"].tolist()
+logger = logging.getLogger("prediction_engine.predictor_cli")
+if not logger.handlers:
+    handler = logging.StreamHandler()
+    handler.setFormatter(logging.Formatter("%(asctime)s [%(levelname)s] %(message)s"))
+    logger.addHandler(handler)
+    logger.setLevel(logging.INFO)
+
 
 @click.command()
-@click.option("--limit", default=None, help="Limit number of games to fetch.")
-@click.option("--date", default=None, help="Filter games by date (YYYY-MM-DD).")
-@click.option("--proba", is_flag=True, help="Output predicted probabilities.")
-@click.option("--label", is_flag=True, help="Output predicted labels.")
-@click.option("--tags", is_flag=True, help="Output predictions with tags.")
-@click.option("--all", is_flag=True, help="Output both probabilities and labels together.")
-def cli(limit, date, proba, label, tags, all):
-    """
-    CLI for running NBA predictions.
-    Fetches games by date or limit, generates features, loads model, and outputs predictions.
-    """
-    # -----------------------------
-    # Step 1: Fetch games
-    # -----------------------------
-    if date:
-        game_ids = fetch_games_by_date(2025, date)
-    else:
-        game_ids = fetch_season_games(2025, limit=limit or 5)
+@click.option("--model", required=True, help="Path to trained pipeline model (.pkl)")
+@click.option("--season", default=2025, help="NBA season year")
+@click.option("--limit", default=5, help="Limit number of games to fetch")
+@click.option(
+    "--proba",
+    is_flag=True,
+    help="Output predicted probabilities (default if no flag given)",
+)
+@click.option("--label", is_flag=True, help="Output predicted labels")
+def cli(model, season, limit, proba, label):
+    """CLI for running NBA predictions."""
+    try:
+        # Fetch games
+        games = fetch_season_games(season, limit=limit)
+        if games is None or games.empty:
+            click.echo(f"❌ No games found for season {season}")
+            return
 
-    features = generate_features_for_games(game_ids)
+        # Generate features
+        game_data_list = games.to_dict(orient="records")
+        features = generate_features_for_games(game_data_list)
 
-    # -----------------------------
-    # Step 2: Load predictor
-    # -----------------------------
-    predictor = NBAPredictor()
-    X = features.drop(columns=["win"])
-    y = features["win"]
+        predictor = NBAPredictor(model_path=model)
+        X = features.drop(columns=["win"], errors="ignore")
 
-    # Fit model if not already fitted
-    if not hasattr(predictor.model, "classes_"):
-        predictor.model.fit(X, y)
-
-    # -----------------------------
-    # Step 3: Make predictions
-    # -----------------------------
-    output = pd.DataFrame(index=features.index)
-
-    if all:
-        output["proba"] = predictor.predict_proba(X)
-        output["label"] = predictor.predict(X).tolist()
-    else:
-        if proba:
-            output["proba"] = predictor.predict_proba(X)
+        output = pd.DataFrame(index=features.index)
+        if proba or (not proba and not label):  # default to proba
+            output["win_proba"] = predictor.predict_proba(X)
         if label:
-            output["label"] = predictor.predict(X).tolist()
-        if tags:
-            output["tags"] = [["NBA", "prediction"] for _ in range(len(output))]
+            output["win_pred"] = predictor.predict_label(X)
+
+        logger.info("Generated predictions for %d games", len(output))
+        click.echo(output.head().to_string(index=False))
+        click.echo(output.to_json(orient="records", indent=2))
 
-    # -----------------------------
-    # Step 4: Print JSON output
-    # -----------------------------
-    click.echo(output.to_json(orient="records"))
+    except Exception as e:
+        logger.error("Prediction run failed: %s", e)
+        logger.debug(traceback.format_exc())
+        click.echo("❌ Prediction run failed. See logs for details.")
+
+
+if __name__ == "__main__":
+    cli()
Index: config.yaml
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+># ============================================================\n# NBA Prediction Pipeline Configuration (v1.0)\n# ============================================================\n\npaths:\n  raw: data/raw          # raw API pulls (optional if you want to store raw responses)\n  cache: data/cache      # cached training features\n  history: data/history  # historical predictions\n  csv: data/csv          # daily CSV outputs\n  parquet: data/parquet  # daily Parquet outputs\n  logs: data/logs        # pipeline + error logs\n  models: models         # trained model files\n\n# Seasons to fetch for training\nseasons: [2022, 2023, 2024, 2025]\n\n# Model path (used by NBAPredictor)\nmodel_path: models/nba_logreg.pkl\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/config.yaml b/config.yaml
--- a/config.yaml	(revision 00055356329a845d3b2ca81b77c3ffb2e2b15969)
+++ b/config.yaml	(date 1765470426213)
@@ -1,18 +1,72 @@
 # ============================================================
-# NBA Prediction Pipeline Configuration (v1.0)
+# NBA Prediction Pipeline Configuration (v1.2 → v2.1)
+# Centralized configuration for API pulls, caching, model loading,
+# training periods, output formats, and runner defaults.
 # ============================================================
 
+# -----------------------------
+# PATH CONFIGURATION
+# -----------------------------
 paths:
-  raw: data/raw          # raw API pulls (optional if you want to store raw responses)
-  cache: data/cache      # cached training features
-  history: data/history  # historical predictions
-  csv: data/csv          # daily CSV outputs
-  parquet: data/parquet  # daily Parquet outputs
-  logs: data/logs        # pipeline + error logs
-  models: models         # trained model files
+  raw: data/raw
+  cache: data/cache
+  history: data/history
+  csv: data/csv
+  parquet: data/parquet
+  logs: data/logs
+  models: models
+  mlflow_artifacts: mlruns
+
+# -----------------------------
+# NBA DATA SETTINGS
+# -----------------------------
+nba:
+  seasons: [2022-23, 2023-24, 2024-25, 2025-26]
+  default_year: 2025
+  players_min_points: 20
+  fetch_retries: 3
+  retry_delay_ms: 1500
+
+# -----------------------------
+# MODEL SETTINGS
+# -----------------------------
+model:
+  path: models/nba_logreg.pkl
+  type: xgb
+  threshold: 0.5
+  device: cpu
+  feature_version: v1
+
+# -----------------------------
+# OUTPUT SETTINGS
+# -----------------------------
+output:
+  save_csv: true
+  save_parquet: false
+  pretty_json: true
+  include_player_stats: true
+  include_betting_fields: true
 
-# Seasons to fetch for training
-seasons: [2022, 2023, 2024, 2025]
+# -----------------------------
+# LOGGING SETTINGS
+# -----------------------------
+logging:
+  level: INFO
+  file: data/logs/pipeline.log
 
-# Model path (used by NBAPredictor)
-model_path: models/nba_logreg.pkl
+# -----------------------------
+# MLFLOW SETTINGS
+# -----------------------------
+mlflow:
+  enabled: true
+  experiment: nba_predictions
+  run_prefix: daily_prediction_
+  log_avg_probability: true
+  log_model_path: true
+  tracking_uri: http://localhost:5000   # ✅ Added for validation
+  artifact_location: mlruns             # ✅ Optional, but good practice
+
+# -----------------------------
+# META
+# -----------------------------
+schema_version: "2.1"
Index: mlruns/0/760c2a5498f54b02bcc1292f75970c1c/tags/mlflow.source.git.commit
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/mlruns/0/760c2a5498f54b02bcc1292f75970c1c/tags/mlflow.source.git.commit b/mlruns/0/760c2a5498f54b02bcc1292f75970c1c/tags/mlflow.source.git.commit
new file mode 100644
--- /dev/null	(date 1765470426220)
+++ b/mlruns/0/760c2a5498f54b02bcc1292f75970c1c/tags/mlflow.source.git.commit	(date 1765470426220)
@@ -0,0 +1,1 @@
+ba58bcf9c077cd4a4662500f7d7cc2c58a4ec45c
\ No newline at end of file
Index: mlruns/0/760c2a5498f54b02bcc1292f75970c1c/tags/mlflow.runName
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/mlruns/0/760c2a5498f54b02bcc1292f75970c1c/tags/mlflow.runName b/mlruns/0/760c2a5498f54b02bcc1292f75970c1c/tags/mlflow.runName
new file mode 100644
--- /dev/null	(date 1765470426219)
+++ b/mlruns/0/760c2a5498f54b02bcc1292f75970c1c/tags/mlflow.runName	(date 1765470426219)
@@ -0,0 +1,1 @@
+xgb_pipeline_v1.3
\ No newline at end of file
Index: mlruns/0/760c2a5498f54b02bcc1292f75970c1c/tags/mlflow.source.type
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/mlruns/0/760c2a5498f54b02bcc1292f75970c1c/tags/mlflow.source.type b/mlruns/0/760c2a5498f54b02bcc1292f75970c1c/tags/mlflow.source.type
new file mode 100644
--- /dev/null	(date 1765470426221)
+++ b/mlruns/0/760c2a5498f54b02bcc1292f75970c1c/tags/mlflow.source.type	(date 1765470426221)
@@ -0,0 +1,1 @@
+LOCAL
\ No newline at end of file
Index: mlruns/0/760c2a5498f54b02bcc1292f75970c1c/tags/mlflow.source.name
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/mlruns/0/760c2a5498f54b02bcc1292f75970c1c/tags/mlflow.source.name b/mlruns/0/760c2a5498f54b02bcc1292f75970c1c/tags/mlflow.source.name
new file mode 100644
--- /dev/null	(date 1765470426221)
+++ b/mlruns/0/760c2a5498f54b02bcc1292f75970c1c/tags/mlflow.source.name	(date 1765470426221)
@@ -0,0 +1,1 @@
+run_pipeline.py
\ No newline at end of file
Index: src/scripts/generate_today_schedule.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/src/scripts/generate_today_schedule.py b/src/scripts/generate_today_schedule.py
new file mode 100644
--- /dev/null	(date 1765470426260)
+++ b/src/scripts/generate_today_schedule.py	(date 1765470426260)
@@ -0,0 +1,43 @@
+# ============================================================
+# File: src/scripts/generate_today_schedule.py
+# Purpose: Generate today's NBA schedule (or next game day) and save to CSV
+# ============================================================
+
+import logging
+import os
+import pandas as pd
+from src.api.nba_api_client import fetch_today_games
+
+logger = logging.getLogger("scripts.generate_today_schedule")
+logging.basicConfig(level=logging.INFO)
+
+
+def main():
+    logger.info("Fetching today's NBA schedule...")
+    df = fetch_today_games()
+
+    if df.empty:
+        logger.warning("No games found today or in the next 7 days.")
+        return
+
+    # Ensure results folder exists
+    out_path = "data/results/today_schedule.csv"
+    os.makedirs(os.path.dirname(out_path), exist_ok=True)
+
+    # Save to CSV
+    df.to_csv(out_path, index=False)
+    logger.info("Schedule saved to %s (shape: %s)", out_path, df.shape)
+
+    # Optional: also save to parquet for consistency
+    parquet_path = "data/results/today_schedule.parquet"
+    df.to_parquet(parquet_path, index=False)
+    logger.info("Schedule also saved to %s", parquet_path)
+
+    # Display next game day info
+    next_date = df["GAME_DATE_EST"].iloc[0]
+    game_type = df["GAME_TYPE"].iloc[0]
+    logger.info("Next NBA game day is %s (%s)", next_date, game_type)
+
+
+if __name__ == "__main__":
+    main()
Index: src/model_training/predictor.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+># ============================================================\n# File: src/prediction_engine/predictor.py\n# Purpose: Predict win probabilities using trained model with logging + CLI\n# ============================================================\n\nfrom pathlib import Path\nimport joblib\nimport pandas as pd\nimport argparse\nfrom src.utils.logging import configure_logging\n\n\nclass NBAPredictor:\n    def __init__(self, model_path: str, log_level=\"INFO\", log_dir=\"logs\"):\n        self.model_path = Path(model_path)\n        self.logger = configure_logging(level=log_level, log_dir=log_dir, name=\"predictor\")\n\n        if not self.model_path.exists():\n            self.logger.error(f\"Model not found at {self.model_path}\")\n            raise FileNotFoundError(f\"Model not found at {self.model_path}\")\n\n        self.logger.info(f\"Loading model from {self.model_path}\")\n        try:\n            self.model = joblib.load(self.model_path)\n        except Exception as e:\n            self.logger.error(f\"Failed to load model: {e}\")\n            raise\n        self.logger.info(\"Model successfully loaded.\")\n\n    def _validate_features(self, features: pd.DataFrame):\n        if not isinstance(features, pd.DataFrame):\n            raise TypeError(\"Features must be a pandas DataFrame.\")\n        if features.empty:\n            raise ValueError(\"Features DataFrame is empty.\")\n\n    def predict_proba(self, features: pd.DataFrame) -> pd.Series:\n        \"\"\"Predict win probabilities for given features.\"\"\"\n        self._validate_features(features)\n        self.logger.info(f\"Generating probability predictions for {len(features)} samples\")\n\n        try:\n            proba = self.model.predict_proba(features)[:, 1]\n        except Exception as e:\n            self.logger.error(f\"Prediction failed: {e}\")\n            raise\n\n        self.logger.info(f\"Average win probability: {proba.mean():.3f}\")\n        self.logger.info(f\"Min: {proba.min():.3f}, Max: {proba.max():.3f}, Std: {proba.std():.3f}\")\n        return pd.Series(proba, index=features.index, name=\"win_proba\")\n\n    def predict_label(self, features: pd.DataFrame, threshold: float = 0.5) -> pd.Series:\n        \"\"\"Predict win/loss labels based on threshold.\"\"\"\n        self._validate_features(features)\n        self.logger.info(f\"Generating label predictions with threshold={threshold}\")\n\n        proba = self.predict_proba(features)\n        labels = (proba >= threshold).astype(int).rename(\"win_pred\")\n        win_rate = labels.mean()\n        self.logger.info(f\"Predicted win rate: {win_rate:.3f}\")\n        return labels\n\n\n# -------------------------------\n# CLI Wrapper\n# -------------------------------\n\ndef main():\n    parser = argparse.ArgumentParser(description=\"NBA Predictor CLI\")\n    parser.add_argument(\"--model\", required=True, help=\"Path to trained model file (.pkl)\")\n    parser.add_argument(\"--features\", required=True, help=\"Path to CSV file with features\")\n    parser.add_argument(\"--mode\", choices=[\"proba\", \"label\"], default=\"proba\")\n    parser.add_argument(\"--threshold\", type=float, default=0.5)\n    parser.add_argument(\"--output\", help=\"Optional path to save predictions as CSV\")\n    args = parser.parse_args()\n\n    predictor = NBAPredictor(model_path=args.model)\n    features = pd.read_csv(args.features)\n\n    if args.mode == \"proba\":\n        preds = predictor.predict_proba(features)\n    else:\n        preds = predictor.predict_label(features, threshold=args.threshold)\n\n    print(preds)\n    if args.output:\n        preds.to_csv(args.output, index=True)\n        print(f\"Predictions saved to {args.output}\")\n\n\nif __name__ == \"__main__\":\n    main()\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/src/model_training/predictor.py b/src/model_training/predictor.py
--- a/src/model_training/predictor.py	(revision 00055356329a845d3b2ca81b77c3ffb2e2b15969)
+++ b/src/model_training/predictor.py	(date 1765470426249)
@@ -1,19 +1,28 @@
 # ============================================================
 # File: src/prediction_engine/predictor.py
 # Purpose: Predict win probabilities using trained model with logging + CLI
+# Project: nba_analysis
+# Version: 1.3 (fixes CLI flag, numeric coercion, feature alignment, alias)
 # ============================================================
 
+import argparse
 from pathlib import Path
+from typing import Optional
+
 import joblib
 import pandas as pd
-import argparse
-from src.utils.logging import configure_logging
+import logging
+import mlflow
+
+from src.utils.logging_config import configure_logging
 
 
 class NBAPredictor:
-    def __init__(self, model_path: str, log_level="INFO", log_dir="logs"):
+    def __init__(self, model_path: str, log_level: str = "INFO", log_dir: str = "logs"):
         self.model_path = Path(model_path)
-        self.logger = configure_logging(level=log_level, log_dir=log_dir, name="predictor")
+        self.logger = configure_logging(
+            level=log_level, log_dir=log_dir, name="predictor"
+        )
 
         if not self.model_path.exists():
             self.logger.error(f"Model not found at {self.model_path}")
@@ -27,16 +36,50 @@
             raise
         self.logger.info("Model successfully loaded.")
 
-    def _validate_features(self, features: pd.DataFrame):
+        # For sklearn models/pipelines, feature_names_in_ may exist
+        self.expected_features = getattr(self.model, "feature_names_in_", None)
+
+    def _validate_features(self, features: pd.DataFrame) -> pd.DataFrame:
         if not isinstance(features, pd.DataFrame):
             raise TypeError("Features must be a pandas DataFrame.")
         if features.empty:
             raise ValueError("Features DataFrame is empty.")
 
+        # Drop known non-feature identifier columns; extend as needed
+        drop_cols = [
+            "win",
+            "unique_id",
+            "prediction_date",
+            "TEAM_NAME",
+            "OPPONENT_TEAM_NAME",
+            "GAME_ID",
+        ]
+        features = features.drop(
+            columns=[c for c in drop_cols if c in features.columns], errors="ignore"
+        )
+
+        # Coerce everything to numeric to avoid dtype issues
+        features = features.apply(pd.to_numeric, errors="coerce").fillna(0)
+
+        # Align with model's expected features if available
+        if self.expected_features is not None:
+            missing = [f for f in self.expected_features if f not in features.columns]
+            if missing:
+                self.logger.warning(f"Missing expected features: {missing}")
+            features = features.reindex(
+                columns=list(self.expected_features), fill_value=0
+            )
+
+        return features
+
     def predict_proba(self, features: pd.DataFrame) -> pd.Series:
-        """Predict win probabilities for given features."""
-        self._validate_features(features)
-        self.logger.info(f"Generating probability predictions for {len(features)} samples")
+        features = self._validate_features(features)
+        self.logger.info(
+            f"Generating probability predictions for {len(features)} samples"
+        )
+
+        if not hasattr(self.model, "predict_proba"):
+            raise AttributeError("Loaded model does not support predict_proba.")
 
         try:
             proba = self.model.predict_proba(features)[:, 1]
@@ -44,47 +87,92 @@
             self.logger.error(f"Prediction failed: {e}")
             raise
 
-        self.logger.info(f"Average win probability: {proba.mean():.3f}")
-        self.logger.info(f"Min: {proba.min():.3f}, Max: {proba.max():.3f}, Std: {proba.std():.3f}")
-        return pd.Series(proba, index=features.index, name="win_proba")
+        proba_series = pd.Series(proba, index=features.index, name="win_proba")
+        self.logger.info(f"Average win probability: {proba_series.mean():.3f}")
+        self.logger.info(
+            f"Min: {proba_series.min():.3f}, Max: {proba_series.max():.3f}, Std: {proba_series.std():.3f}"
+        )
+        return proba_series
 
-    def predict_label(self, features: pd.DataFrame, threshold: float = 0.5) -> pd.Series:
-        """Predict win/loss labels based on threshold."""
-        self._validate_features(features)
+    def predict_label(
+        self, features: pd.DataFrame, threshold: float = 0.5
+    ) -> pd.Series:
+        features = self._validate_features(features)
         self.logger.info(f"Generating label predictions with threshold={threshold}")
 
         proba = self.predict_proba(features)
         labels = (proba >= threshold).astype(int).rename("win_pred")
-        win_rate = labels.mean()
+        win_rate = float(labels.mean())
         self.logger.info(f"Predicted win rate: {win_rate:.3f}")
         return labels
 
 
+# Backward-compatible alias to match existing imports in your codebase
+class Predictor(NBAPredictor):
+    pass
+
+
 # -------------------------------
 # CLI Wrapper
 # -------------------------------
 
+
 def main():
     parser = argparse.ArgumentParser(description="NBA Predictor CLI")
-    parser.add_argument("--model", required=True, help="Path to trained model file (.pkl)")
-    parser.add_argument("--features", required=True, help="Path to CSV file with features")
+    parser.add_argument(
+        "--model", required=True, help="Path to trained model file (.pkl)"
+    )
+    parser.add_argument(
+        "--features", required=True, help="Path to CSV or Parquet file with features"
+    )
     parser.add_argument("--mode", choices=["proba", "label"], default="proba")
     parser.add_argument("--threshold", type=float, default=0.5)
     parser.add_argument("--output", help="Optional path to save predictions as CSV")
+    parser.add_argument(
+        "--mlflow", action="store_true", help="Log predictions to MLflow"
+    )  # fixed flag
     args = parser.parse_args()
 
-    predictor = NBAPredictor(model_path=args.model)
-    features = pd.read_csv(args.features)
+    try:
+        predictor = NBAPredictor(model_path=args.model)
+
+        feat_path = Path(args.features)
+        if feat_path.suffix.lower() == ".parquet":
+            features = pd.read_parquet(feat_path)
+        else:
+            features = pd.read_csv(feat_path)
 
-    if args.mode == "proba":
-        preds = predictor.predict_proba(features)
-    else:
-        preds = predictor.predict_label(features, threshold=args.threshold)
+        if args.mode == "proba":
+            preds = predictor.predict_proba(features)
+            summary = f"Summary: mean={preds.mean():.3f}, min={preds.min():.3f}, max={preds.max():.3f}"
+        else:
+            preds = predictor.predict_label(features, threshold=args.threshold)
+            summary = f"Summary: win_rate={preds.mean():.3f}"
 
-    print(preds)
-    if args.output:
-        preds.to_csv(args.output, index=True)
-        print(f"Predictions saved to {args.output}")
+        print(preds.head())
+        print(f"... total {len(preds)} predictions")
+        print(summary)
+
+        if args.output:
+            out_path = Path(args.output).resolve()
+            preds.to_csv(out_path, index=True)
+            print(f"Predictions saved to {out_path}")
+
+        if args.mlflow:
+            model_abs = Path(args.model).resolve()
+            feats_abs = Path(args.features).resolve()
+            with mlflow.start_run(run_name="predictor_cli", nested=True):
+                mlflow.log_param("model_path", str(model_abs))
+                mlflow.log_param("features_file", str(feats_abs))
+                mlflow.log_metric("mean_prediction", float(preds.mean()))
+                mlflow.log_artifact(str(model_abs), artifact_path="model")
+                if args.output:
+                    mlflow.log_artifact(
+                        str(Path(args.output).resolve()), artifact_path="predictions"
+                    )
+
+    except Exception as e:
+        print(f"❌ Prediction failed: {e}")
 
 
 if __name__ == "__main__":
Index: mlruns/0/760c2a5498f54b02bcc1292f75970c1c/tags/mlflow.user
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/mlruns/0/760c2a5498f54b02bcc1292f75970c1c/tags/mlflow.user b/mlruns/0/760c2a5498f54b02bcc1292f75970c1c/tags/mlflow.user
new file mode 100644
--- /dev/null	(date 1765470426222)
+++ b/mlruns/0/760c2a5498f54b02bcc1292f75970c1c/tags/mlflow.user	(date 1765470426222)
@@ -0,0 +1,1 @@
+Mohamadou
\ No newline at end of file
Index: src/scripts/test.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/src/scripts/test.py b/src/scripts/test.py
new file mode 100644
--- /dev/null	(date 1765470426261)
+++ b/src/scripts/test.py	(date 1765470426261)
@@ -0,0 +1,4 @@
+import pandas as pd
+
+df = pd.read_parquet("data/cache/features_full.parquet")
+print(df["win"].value_counts())
Index: src/scripts/generate_features.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/src/scripts/generate_features.py b/src/scripts/generate_features.py
new file mode 100644
--- /dev/null	(date 1765470426258)
+++ b/src/scripts/generate_features.py	(date 1765470426258)
@@ -0,0 +1,50 @@
+#!/usr/bin/env python
+# ============================================================
+# File: src/scripts/generate_features.py
+# Purpose: Generate features from enriched schedule (prefers WL outcomes)
+# ============================================================
+
+import os
+import pandas as pd
+from src.features.feature_engineering import generate_features_for_games
+from src.utils.logging_config import configure_logging
+
+# Prefer enriched file if available
+ENRICHED_FILE = "data/cache/historical_schedule_with_results.parquet"
+DEFAULT_FILE = "data/cache/historical_schedule.parquet"
+
+FEATURES_FILE = "data/cache/features_full.parquet"
+FEATURES_CSV = "data/csv/features_full.csv"
+
+
+def main():
+    logger = configure_logging(name="scripts.generate_features")
+    logger.info("Starting feature generation from historical NBA games...")
+
+    # Choose enriched file if it exists
+    input_file = ENRICHED_FILE if os.path.exists(ENRICHED_FILE) else DEFAULT_FILE
+    logger.info("Loading schedule from %s", input_file)
+
+    df = pd.read_parquet(input_file)
+    logger.info("Loaded %d historical games.", len(df))
+
+    logger.info("Generating features for the games...")
+    features = generate_features_for_games(df)
+
+    if features is None or features.empty:
+        logger.error("No features generated.")
+        return
+
+    os.makedirs(os.path.dirname(FEATURES_FILE), exist_ok=True)
+    os.makedirs(os.path.dirname(FEATURES_CSV), exist_ok=True)
+
+    features.to_parquet(FEATURES_FILE, index=False)
+    features.to_csv(FEATURES_CSV, index=False)
+
+    logger.info("Features saved to %s (shape: %s)", FEATURES_FILE, features.shape)
+    logger.info("Features CSV saved to %s", FEATURES_CSV)
+    logger.info("Feature generation process completed successfully.")
+
+
+if __name__ == "__main__":
+    main()
Index: src/scripts/enrich_schedule.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/src/scripts/enrich_schedule.py b/src/scripts/enrich_schedule.py
new file mode 100644
--- /dev/null	(date 1765470426258)
+++ b/src/scripts/enrich_schedule.py	(date 1765470426258)
@@ -0,0 +1,87 @@
+# ============================================================
+# File: src/scripts/enrich_schedule.py
+# Purpose: Fetch and enrich NBA schedule with WL outcomes
+# ============================================================
+
+import logging
+import os
+import time
+import pandas as pd
+import requests
+from nba_api.stats.endpoints import leaguegamefinder
+
+logger = logging.getLogger("scripts.enrich_schedule")
+logging.basicConfig(level=logging.INFO)
+
+
+def fetch_game_results(season: int, retries: int = 3, delay: int = 10) -> pd.DataFrame:
+    """
+    Fetch game results for a given season with retry logic.
+    Returns a DataFrame or empty DataFrame if API fails.
+    """
+    for attempt in range(retries):
+        try:
+            logger.info(
+                "Fetching season games with WL outcomes for %s (attempt %d)",
+                season,
+                attempt + 1,
+            )
+            gamefinder = leaguegamefinder.LeagueGameFinder(season_nullable=season)
+            df = gamefinder.get_data_frames()[0]
+            return df
+        except requests.exceptions.ReadTimeout:
+            logger.warning(
+                "Timeout fetching season %s games. Retrying in %s seconds...",
+                season,
+                delay,
+            )
+            time.sleep(delay)
+        except Exception as e:
+            logger.error("Unexpected error fetching season %s games: %s", season, e)
+            return pd.DataFrame()
+
+    logger.error("Failed to fetch season %s games after %d retries", season, retries)
+    return pd.DataFrame()
+
+
+def main():
+    import argparse
+
+    parser = argparse.ArgumentParser()
+    parser.add_argument(
+        "--season", type=int, required=True, help="Season year (e.g., 2025)"
+    )
+    args = parser.parse_args()
+
+    season = args.season
+    out_path = "data/cache/schedule.csv"
+    os.makedirs(os.path.dirname(out_path), exist_ok=True)
+
+    # --- Try to fetch results ---
+    results = fetch_game_results(season)
+
+    if results.empty:
+        logger.warning("No schedule data retrieved for season %s", season)
+
+        # --- Fallback: use cached file if available ---
+        if os.path.exists(out_path):
+            logger.info("Using cached schedule file at %s", out_path)
+            return
+        else:
+            # --- Write empty file so downstream steps succeed ---
+            empty_cols = [
+                "GAME_ID",
+                "GAME_DATE_EST",
+                "HOME_TEAM_ID",
+                "VISITOR_TEAM_ID",
+                "WL",
+            ]
+            pd.DataFrame(columns=empty_cols).to_csv(out_path, index=False)
+            logger.warning("Empty schedule file written to %s", out_path)
+    else:
+        results.to_csv(out_path, index=False)
+        logger.info("Schedule saved to %s (rows: %d)", out_path, len(results))
+
+
+if __name__ == "__main__":
+    main()
Index: pyproject.toml
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+># ============================================================\n# pyproject.toml\n# Author: Your Team\n# Date: December 2025\n# Purpose: Build configuration for nba_analytics project\n# ============================================================\n\n[build-system]\nrequires = [\"setuptools>=61.0\", \"wheel\"]\nbuild-backend = \"setuptools.build_meta\"\n\n[project]\nname = \"nba-analytics\"\nversion = \"0.1.0\"\ndescription = \"NBA win prediction pipeline with MLflow and SHAP explainability\"\nauthors = [\n    { name = \"Your Team\", email = \"team@example.com\" }\n]\nreadme = \"README.md\"\nrequires-python = \">=3.9\"\ndependencies = [\n    \"pandas==2.2.3\",\n    \"numpy==1.26.4\",\n    \"scikit-learn==1.5.2\",\n    \"scipy==1.14.1\",\n    \"matplotlib==3.9.2\",\n    \"seaborn==0.13.2\",\n    \"nba_api==1.4.1\",\n    \"loguru==0.7.2\",\n    \"joblib==1.4.2\",\n    \"mlflow==2.16.2\",\n    \"shap==0.46.0\",\n    \"pydantic==2.12.0\",\n    \"pydantic-settings==2.2.1\",\n    \"pytest==9.0.2\"\n]\n\n[tool.setuptools.packages.find]\nwhere = [\"src\"]\n\n[tool.pytest.ini_options]\n# Tell pytest where to look for tests and source code\ntestpaths = [\"tests\"]\npythonpath = [\"src\"]\naddopts = \"-ra -q\"\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/pyproject.toml b/pyproject.toml
--- a/pyproject.toml	(revision 00055356329a845d3b2ca81b77c3ffb2e2b15969)
+++ b/pyproject.toml	(date 1765470426236)
@@ -1,45 +1,49 @@
-# ============================================================
-# pyproject.toml
-# Author: Your Team
-# Date: December 2025
-# Purpose: Build configuration for nba_analytics project
-# ============================================================
-
-[build-system]
-requires = ["setuptools>=61.0", "wheel"]
-build-backend = "setuptools.build_meta"
-
 [project]
 name = "nba-analytics"
-version = "0.1.0"
+dynamic = ["version"]
 description = "NBA win prediction pipeline with MLflow and SHAP explainability"
 authors = [
     { name = "Your Team", email = "team@example.com" }
 ]
 readme = "README.md"
 requires-python = ">=3.9"
+license = { text = "MIT" }
+classifiers = [
+    "Programming Language :: Python :: 3",
+    "License :: OSI Approved :: MIT License",
+    "Operating System :: OS Independent",
+]
 dependencies = [
-    "pandas==2.2.3",
-    "numpy==1.26.4",
-    "scikit-learn==1.5.2",
-    "scipy==1.14.1",
-    "matplotlib==3.9.2",
-    "seaborn==0.13.2",
+    "pandas~=2.2",
+    "numpy~=1.26",
+    "scikit-learn~=1.5",
+    "scipy~=1.14",
+    "matplotlib~=3.9",
+    "seaborn~=0.13",
     "nba_api==1.4.1",
-    "loguru==0.7.2",
-    "joblib==1.4.2",
-    "mlflow==2.16.2",
-    "shap==0.46.0",
-    "pydantic==2.12.0",
-    "pydantic-settings==2.2.1",
-    "pytest==9.0.2"
+    "loguru~=0.7",
+    "joblib~=1.4",
+    "mlflow~=2.16",
+    "shap~=0.46",
+    "pydantic~=2.12",
+    "pydantic-settings~=2.2"
 ]
 
 [tool.setuptools.packages.find]
 where = ["src"]
 
+[tool.setuptools.dynamic]
+version = { attr = "nba_analytics.__version__" }
+
+[project.optional-dependencies]
+dev = [
+    "pytest~=9.0",
+    "black~=24.10",
+    "isort~=5.13",
+    "mypy~=1.11"
+]
+
 [tool.pytest.ini_options]
-# Tell pytest where to look for tests and source code
 testpaths = ["tests"]
 pythonpath = ["src"]
 addopts = "-ra -q"
Index: scripts/setup_project.sh
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/scripts/setup_project.sh b/src/scripts/setup_project.sh
rename from scripts/setup_project.sh
rename to src/scripts/setup_project.sh
--- a/scripts/setup_project.sh	(revision 00055356329a845d3b2ca81b77c3ffb2e2b15969)
+++ b/src/scripts/setup_project.sh	(date 1765470426261)
@@ -4,19 +4,23 @@
 # Purpose: Bootstrap NBA AI project structure and hygiene tools.
 # ============================================================
 
-set -e
+set -euo pipefail
 
 echo "🚀 Setting up NBA AI project..."
 
 # --- Check prerequisites ---
-command -v python >/dev/null 2>&1 || { echo "❌ Python not found"; exit 1; }
-command -v pip >/dev/null 2>&1 || { echo "❌ pip not found"; exit 1; }
+command -v python3 >/dev/null 2>&1 || { echo "❌ Python3 not found, please install it"; exit 1; }
+command -v pip >/dev/null 2>&1 || { echo "❌ pip not found, please install it"; exit 1; }
 
 # --- Create directories ---
-mkdir -p src/prediction_engine src/model_training src/utils tests data results models logs docs
+mkdir -p src/prediction_engine src/model_training src/utils src/features tests data results models logs docs
 
 # --- Create placeholder files ---
-touch src/prediction_engine/__init__.py src/model_training/__init__.py src/utils/__init__.py tests/__init__.py
+touch src/prediction_engine/__init__.py \
+      src/model_training/__init__.py \
+      src/utils/__init__.py \
+      src/features/__init__.py \
+      tests/__init__.py
 
 # --- Initialize Git repo if not already ---
 if [ ! -d ".git" ]; then
@@ -50,7 +54,7 @@
 htmlcov/
 .DS_Store
 Thumbs.db
-.VS Code/
+.vscode/
 .idea/
 build/
 dist/
@@ -83,7 +87,6 @@
 [*.{yml,yaml,json}]
 indent_style = space
 indent_size = 2
-quote_type = double
 
 [*.toml]
 indent_style = space
@@ -98,13 +101,23 @@
 indent_style = tab
 EOF
 
+# --- Setup virtual environment ---
+if [ ! -d ".venv" ]; then
+  python3 -m venv .venv
+fi
+source .venv/bin/activate
+
 # --- Install dependencies ---
 pip install --upgrade pip setuptools wheel
-pip install -r requirements.txt || true
+if [ -f requirements.txt ]; then
+  pip install -r requirements.txt
+else
+  echo "❌ No requirements.txt found. Please ensure dependencies are listed."
+  exit 1
+fi
 
 # --- Fix common dependency conflicts ---
-pip install --upgrade "protobuf>=5.0,<7.0"
-pip install --upgrade "packaging>=24.2"
+pip install --upgrade "protobuf>=5.0,<7.0" "packaging>=24.2"
 
 # --- Ensure pinned mlflow version ---
 pip uninstall -y mlflow mlflow-skinny || true
@@ -112,8 +125,11 @@
 
 # --- Initialize pre-commit ---
 pip install pre-commit
-pre-commit install
-pre-commit install --hook-type pre-push
+if ! command -v pre-commit >/dev/null 2>&1; then
+  echo "❌ pre-commit installation failed."
+  exit 1
+fi
+pre-commit install --hook-type pre-push --install-hooks
 
 # --- Initialize GitHub Actions workflow ---
 mkdir -p .github/workflows
@@ -129,14 +145,14 @@
         with:
           python-version: "3.12"
       - run: pip install --upgrade pip setuptools wheel
-      - run: pip install -r requirements.txt
+      - run: pip install -r requirements.txt || true
       - run: pip install "protobuf>=5.0,<7.0" "packaging>=24.2" mlflow==2.9.2
       - run: make ci
 EOF
 
 # --- Compatibility check ---
 echo "🔍 Verifying installed versions..."
-python -c "import mlflow, packaging, google.protobuf as pb; \
+python3 -c "import mlflow, packaging, google.protobuf as pb; \
 print(f'MLflow version: {mlflow.__version__}'); \
 print(f'Packaging version: {packaging.__version__}'); \
 print(f'Protobuf version: {pb.__version__}')"
Index: src/scripts/generate_historical_schedule.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/src/scripts/generate_historical_schedule.py b/src/scripts/generate_historical_schedule.py
new file mode 100644
--- /dev/null	(date 1765470426259)
+++ b/src/scripts/generate_historical_schedule.py	(date 1765470426259)
@@ -0,0 +1,82 @@
+#!/usr/bin/env python
+# ============================================================
+# File: src/scripts/generate_historical_schedule.py
+# Purpose: Fetch historical NBA games across multiple seasons and save as Parquet
+# Project: nba_analysis
+# Version: 1.3 (named logger, safer deduplication, exit codes)
+# ============================================================
+
+import os
+import sys
+import pandas as pd
+from nba_api.stats.endpoints import leaguegamefinder
+from src.utils.logging_config import configure_logging
+
+# -----------------------------
+# CONFIGURATION
+# -----------------------------
+SEASONS = ["2022-23", "2023-24", "2024-25"]
+OUTPUT_FILE = "data/cache/historical_schedule.parquet"
+
+
+def ensure_dir(path: str):
+    """Ensure directory exists for a given file path."""
+    dir_name = os.path.dirname(path)
+    if dir_name:
+        os.makedirs(dir_name, exist_ok=True)
+
+
+# -----------------------------
+# MAIN
+# -----------------------------
+def main():
+    logger = configure_logging(name="scripts.generate_historical_schedule")
+    ensure_dir(OUTPUT_FILE)
+
+    all_games = []
+    logger.info("Fetching historical NBA games...")
+
+    for season in SEASONS:
+        try:
+            logger.info("Fetching data for season %s...", season)
+            gamefinder = leaguegamefinder.LeagueGameFinder(season_nullable=season)
+            df = gamefinder.get_data_frames()[0]
+
+            # Keep relevant columns
+            df = df[
+                ["GAME_DATE", "TEAM_NAME", "MATCHUP", "GAME_ID", "TEAM_ID", "PTS", "WL"]
+            ]
+            all_games.append(df)
+            logger.info("Season %s: %d games fetched", season, len(df))
+        except Exception as e:
+            logger.error("Error fetching season %s: %s", season, e)
+
+    if not all_games:
+        logger.warning("No games fetched. Historical schedule not created.")
+        sys.exit(0)
+
+    combined_df = pd.concat(all_games, ignore_index=True)
+
+    # Deduplicate by GAME_ID + TEAM_ID to keep both teams per game
+    initial_len = len(combined_df)
+    combined_df = combined_df.drop_duplicates(subset=["GAME_ID", "TEAM_ID"])
+    final_len = len(combined_df)
+
+    # Convert GAME_DATE to datetime
+    combined_df["GAME_DATE"] = pd.to_datetime(combined_df["GAME_DATE"], errors="coerce")
+
+    logger.info("Removed %d duplicate rows.", initial_len - final_len)
+    logger.info("Final dataset contains %d unique rows.", final_len)
+
+    try:
+        combined_df.to_parquet(OUTPUT_FILE, index=False)
+        logger.info(
+            "Historical schedule saved to %s (%d rows total)", OUTPUT_FILE, final_len
+        )
+    except Exception as e:
+        logger.error("Error saving historical schedule: %s", e)
+        sys.exit(1)
+
+
+if __name__ == "__main__":
+    main()
Index: requirements.txt
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+># Core data science stack\npandas==2.2.3\nnumpy==1.26.4\nscikit-learn==1.5.2\nscipy==1.14.1\n\n# Visualization\nmatplotlib==3.9.2\nseaborn==0.13.2\n\n# NBA data API\nnba_api==1.4.1\n\n# Logging & utilities\nloguru==0.7.2\njoblib==1.4.2\n\n# Experiment tracking\nmlflow==2.16.2\n\n# Model explainability\nshap==0.46.0\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/requirements.txt b/requirements.txt
--- a/requirements.txt	(revision 00055356329a845d3b2ca81b77c3ffb2e2b15969)
+++ b/requirements.txt	(date 1765470426237)
@@ -14,9 +14,26 @@
 # Logging & utilities
 loguru==0.7.2
 joblib==1.4.2
+tqdm==4.67.1
+pyyaml==6.0.2
+invoke==2.2.0
 
 # Experiment tracking
 mlflow==2.16.2
 
 # Model explainability
 shap==0.46.0
+
+# Documentation (optional)
+sphinx==7.1.2
+
+# Dev tools (optional)
+flake8==7.1.1
+mypy==1.11.2
+
+# Testing (optional)
+pytest==7.4.0
+
+click
+
+xgboost
Index: mlruns/0/36f6a6756d984d5f9adcc63774d26eed/meta.yaml
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/mlruns/0/36f6a6756d984d5f9adcc63774d26eed/meta.yaml b/mlruns/0/36f6a6756d984d5f9adcc63774d26eed/meta.yaml
new file mode 100644
--- /dev/null	(date 1765470426214)
+++ b/mlruns/0/36f6a6756d984d5f9adcc63774d26eed/meta.yaml	(date 1765470426214)
@@ -0,0 +1,15 @@
+artifact_uri: file:///C:/Users/Mohamadou/PycharmProjects/nba_analytics/mlruns/0/36f6a6756d984d5f9adcc63774d26eed/artifacts
+end_time: 1765312876962
+entry_point_name: ''
+experiment_id: '0'
+lifecycle_stage: active
+run_id: 36f6a6756d984d5f9adcc63774d26eed
+run_name: xgb_pipeline_v2.0
+run_uuid: 36f6a6756d984d5f9adcc63774d26eed
+source_name: ''
+source_type: 4
+source_version: ''
+start_time: 1765312876897
+status: 4
+tags: []
+user_id: Mohamadou
Index: src/utils/io.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+># ============================================================\n# File: src/utils/io.py\n# Purpose: File I/O helpers for NBA analysis project with logging\n# Project: nba_analysis\n# ============================================================\n\nimport json\nimport shutil\nfrom pathlib import Path\nfrom typing import Any, Union\n\nimport pandas as pd\n\nfrom src.utils.logging import configure_logging\n\n# Configure a module-level logger\nlogger = configure_logging(level=\"INFO\", log_dir=\"logs\", name=\"io\")\n\n\ndef ensure_dir(path: Union[str, Path]) -> Path:\n    \"\"\"Ensure directory exists, return Path object.\"\"\"\n    p = Path(path)\n    p.mkdir(parents=True, exist_ok=True)\n    logger.debug(f\"Ensured directory exists: {p}\")\n    return p\n\n\ndef save_dataframe(df: pd.DataFrame, path: Union[str, Path]) -> Path:\n    \"\"\"Save DataFrame to Parquet, CSV, or Excel based on extension. Returns path.\"\"\"\n    p = Path(path)\n    ensure_dir(p.parent)\n    try:\n        if p.suffix == \".parquet\":\n            try:\n                df.to_parquet(p, index=False)\n            except ImportError as e:\n                raise RuntimeError(\"Parquet support requires pyarrow or fastparquet\") from e\n        elif p.suffix == \".csv\":\n            df.to_csv(p, index=False)\n        elif p.suffix in [\".xlsx\", \".xls\"]:\n            try:\n                df.to_excel(p, index=False)\n            except ImportError as e:\n                raise RuntimeError(\"Excel support requires openpyxl or xlwt\") from e\n        else:\n            raise ValueError(f\"Unsupported file type for saving: {p.suffix}\")\n        logger.info(f\"Saved DataFrame to {p}\")\n        return p\n    except Exception as e:\n        logger.error(f\"Failed to save DataFrame to {p}: {e}\")\n        raise RuntimeError(f\"Failed to save DataFrame to {p}: {e}\") from e\n\n\ndef load_dataframe(path: Union[str, Path]) -> pd.DataFrame:\n    \"\"\"Load DataFrame from Parquet, CSV, or Excel.\"\"\"\n    p = Path(path)\n    try:\n        if p.suffix == \".parquet\":\n            df = pd.read_parquet(p)\n        elif p.suffix == \".csv\":\n            df = pd.read_csv(p)\n        elif p.suffix in [\".xlsx\", \".xls\"]:\n            df = pd.read_excel(p)\n        else:\n            raise ValueError(f\"Unsupported file type for loading: {p.suffix}\")\n        logger.info(f\"Loaded DataFrame from {p} (rows={len(df)})\")\n        return df\n    except Exception as e:\n        logger.error(f\"Failed to load DataFrame from {p}: {e}\")\n        raise RuntimeError(f\"Failed to load DataFrame from {p}: {e}\") from e\n\n\ndef save_json(obj: Any, path: Union[str, Path]) -> Path:\n    \"\"\"Save JSON-serializable object to file. Returns path.\"\"\"\n    p = Path(path)\n    ensure_dir(p.parent)\n    try:\n        with p.open(\"w\", encoding=\"utf-8\") as f:\n            json.dump(obj, f, indent=2)\n        logger.info(f\"Saved JSON to {p}\")\n        return p\n    except Exception as e:\n        logger.error(f\"Failed to save JSON to {p}: {e}\")\n        raise RuntimeError(f\"Failed to save JSON to {p}: {e}\") from e\n\n\ndef load_json(path: Union[str, Path]) -> Any:\n    \"\"\"Load JSON object (dict, list, etc.) from file.\"\"\"\n    p = Path(path)\n    try:\n        with p.open(\"r\", encoding=\"utf-8\") as f:\n            data = json.load(f)\n        logger.info(f\"Loaded JSON from {p}\")\n        return data\n    except Exception as e:\n        logger.error(f\"Failed to load JSON from {p}: {e}\")\n        raise RuntimeError(f\"Failed to load JSON from {p}: {e}\") from e\n\n\ndef safe_load(path: Union[str, Path]) -> Any:\n    \"\"\"Convenience loader: detects file type and loads appropriately.\"\"\"\n    p = Path(path)\n    if p.suffix in [\".parquet\", \".csv\", \".xlsx\", \".xls\"]:\n        return load_dataframe(p)\n    elif p.suffix == \".json\":\n        return load_json(p)\n    else:\n        raise ValueError(f\"Unsupported file type for safe_load: {p.suffix}\")\n\n\ndef safe_save(obj: Any, path: Union[str, Path]) -> Path:\n    \"\"\"Convenience saver: detects object type and saves appropriately.\"\"\"\n    p = Path(path)\n    if isinstance(obj, pd.DataFrame):\n        return save_dataframe(obj, p)\n    else:\n        return save_json(obj, p)\n\n\ndef safe_copy(src: Union[str, Path], dest: Union[str, Path]) -> Path:\n    \"\"\"Copy a file from src to dest with logging.\"\"\"\n    src_path = Path(src)\n    dest_path = Path(dest)\n    ensure_dir(dest_path.parent)\n    try:\n        shutil.copy2(src_path, dest_path)\n        logger.info(f\"Copied file from {src_path} to {dest_path}\")\n        return dest_path\n    except Exception as e:\n        logger.error(f\"Failed to copy file from {src_path} to {dest_path}: {e}\")\n        raise RuntimeError(f\"Failed to copy file from {src_path} to {dest_path}: {e}\") from e\n\n\ndef safe_move(src: Union[str, Path], dest: Union[str, Path]) -> Path:\n    \"\"\"Move a file from src to dest with logging.\"\"\"\n    src_path = Path(src)\n    dest_path = Path(dest)\n    ensure_dir(dest_path.parent)\n    try:\n        shutil.move(str(src_path), str(dest_path))\n        logger.info(f\"Moved file from {src_path} to {dest_path}\")\n        return dest_path\n    except Exception as e:\n        logger.error(f\"Failed to move file from {src_path} to {dest_path}: {e}\")\n        raise RuntimeError(f\"Failed to move file from {src_path} to {dest_path}: {e}\") from e\n\n\ndef safe_delete(path: Union[str, Path]) -> None:\n    \"\"\"Delete a file safely with logging.\"\"\"\n    p = Path(path)\n    try:\n        if p.exists():\n            p.unlink()\n            logger.info(f\"Deleted file: {p}\")\n        else:\n            logger.warning(f\"File not found for deletion: {p}\")\n    except Exception as e:\n        logger.error(f\"Failed to delete file {p}: {e}\")\n        raise RuntimeError(f\"Failed to delete file {p}: {e}\") from e\n\n\ndef safe_exists(path: Union[str, Path]) -> bool:\n    \"\"\"Check if a file exists and log the result.\"\"\"\n    p = Path(path)\n    exists = p.exists()\n    if exists:\n        logger.info(f\"File exists: {p}\")\n    else:\n        logger.warning(f\"File does not exist: {p}\")\n    return exists\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/src/utils/io.py b/src/utils/io.py
--- a/src/utils/io.py	(revision 00055356329a845d3b2ca81b77c3ffb2e2b15969)
+++ b/src/utils/io.py	(date 1765470426264)
@@ -1,170 +1,89 @@
 # ============================================================
 # File: src/utils/io.py
-# Purpose: File I/O helpers for NBA analysis project with logging
+# Purpose: Utility functions for loading and saving DataFrames
 # Project: nba_analysis
+# Version: 1.7 (adds read_or_create helper)
 # ============================================================
 
-import json
-import shutil
-from pathlib import Path
-from typing import Any, Union
-
+import os
 import pandas as pd
+import logging
+from logging.handlers import TimedRotatingFileHandler
 
-from src.utils.logging import configure_logging
+# -----------------------------
+# LOGGING CONFIGURATION
+# -----------------------------
+LOG_DIR = "logs"
+LOG_FILE = os.path.join(LOG_DIR, "io_operations.log")
+os.makedirs(LOG_DIR, exist_ok=True)
 
-# Configure a module-level logger
-logger = configure_logging(level="INFO", log_dir="logs", name="io")
+timed_handler = TimedRotatingFileHandler(
+    LOG_FILE, when="midnight", interval=1, backupCount=7, encoding="utf-8"
+)
 
+logging.basicConfig(
+    level=logging.INFO,
+    format="%(asctime)s [%(levelname)s] %(message)s",
+    handlers=[logging.StreamHandler(), timed_handler],
+    datefmt="%Y-%m-%d %H:%M:%S",
+)
 
-def ensure_dir(path: Union[str, Path]) -> Path:
-    """Ensure directory exists, return Path object."""
-    p = Path(path)
-    p.mkdir(parents=True, exist_ok=True)
-    logger.debug(f"Ensured directory exists: {p}")
-    return p
+logger = logging.getLogger("io_utils")
 
 
-def save_dataframe(df: pd.DataFrame, path: Union[str, Path]) -> Path:
-    """Save DataFrame to Parquet, CSV, or Excel based on extension. Returns path."""
-    p = Path(path)
-    ensure_dir(p.parent)
-    try:
-        if p.suffix == ".parquet":
-            try:
-                df.to_parquet(p, index=False)
-            except ImportError as e:
-                raise RuntimeError("Parquet support requires pyarrow or fastparquet") from e
-        elif p.suffix == ".csv":
-            df.to_csv(p, index=False)
-        elif p.suffix in [".xlsx", ".xls"]:
-            try:
-                df.to_excel(p, index=False)
-            except ImportError as e:
-                raise RuntimeError("Excel support requires openpyxl or xlwt") from e
-        else:
-            raise ValueError(f"Unsupported file type for saving: {p.suffix}")
-        logger.info(f"Saved DataFrame to {p}")
-        return p
-    except Exception as e:
-        logger.error(f"Failed to save DataFrame to {p}: {e}")
-        raise RuntimeError(f"Failed to save DataFrame to {p}: {e}") from e
+def load_dataframe(path: str) -> pd.DataFrame:
+    """Load a DataFrame from a CSV or Parquet file."""
+    if not os.path.exists(path):
+        logger.error("File not found: %s", path)
+        raise FileNotFoundError(f"{path} does not exist")
 
-
-def load_dataframe(path: Union[str, Path]) -> pd.DataFrame:
-    """Load DataFrame from Parquet, CSV, or Excel."""
-    p = Path(path)
-    try:
-        if p.suffix == ".parquet":
-            df = pd.read_parquet(p)
-        elif p.suffix == ".csv":
-            df = pd.read_csv(p)
-        elif p.suffix in [".xlsx", ".xls"]:
-            df = pd.read_excel(p)
-        else:
-            raise ValueError(f"Unsupported file type for loading: {p.suffix}")
-        logger.info(f"Loaded DataFrame from {p} (rows={len(df)})")
-        return df
-    except Exception as e:
-        logger.error(f"Failed to load DataFrame from {p}: {e}")
-        raise RuntimeError(f"Failed to load DataFrame from {p}: {e}") from e
-
+    if path.endswith(".csv"):
+        df = pd.read_csv(path)
+    elif path.endswith(".parquet"):
+        df = pd.read_parquet(path, engine="pyarrow")
+    else:
+        logger.error("Unsupported file format for %s", path)
+        raise ValueError("Unsupported file format: must be CSV or Parquet")
+
+    logger.info("Loaded DataFrame from %s (shape: %s)", path, df.shape)
+    return df
 
-def save_json(obj: Any, path: Union[str, Path]) -> Path:
-    """Save JSON-serializable object to file. Returns path."""
-    p = Path(path)
-    ensure_dir(p.parent)
-    try:
-        with p.open("w", encoding="utf-8") as f:
-            json.dump(obj, f, indent=2)
-        logger.info(f"Saved JSON to {p}")
-        return p
-    except Exception as e:
-        logger.error(f"Failed to save JSON to {p}: {e}")
-        raise RuntimeError(f"Failed to save JSON to {p}: {e}") from e
 
+def save_dataframe(df: pd.DataFrame, path: str) -> str:
+    """Save a DataFrame to a CSV or Parquet file."""
+    if not isinstance(df, pd.DataFrame):
+        logger.error("save_dataframe expects a pandas DataFrame, got %s", type(df))
+        raise TypeError("save_dataframe expects a pandas DataFrame")
 
-def load_json(path: Union[str, Path]) -> Any:
-    """Load JSON object (dict, list, etc.) from file."""
-    p = Path(path)
-    try:
-        with p.open("r", encoding="utf-8") as f:
-            data = json.load(f)
-        logger.info(f"Loaded JSON from {p}")
-        return data
-    except Exception as e:
-        logger.error(f"Failed to load JSON from {p}: {e}")
-        raise RuntimeError(f"Failed to load JSON from {p}: {e}") from e
+    os.makedirs(os.path.dirname(path), exist_ok=True)
 
-
-def safe_load(path: Union[str, Path]) -> Any:
-    """Convenience loader: detects file type and loads appropriately."""
-    p = Path(path)
-    if p.suffix in [".parquet", ".csv", ".xlsx", ".xls"]:
-        return load_dataframe(p)
-    elif p.suffix == ".json":
-        return load_json(p)
+    if path.endswith(".csv"):
+        df.to_csv(path, index=False)
+    elif path.endswith(".parquet"):
+        df.to_parquet(path, index=False, engine="pyarrow")
     else:
-        raise ValueError(f"Unsupported file type for safe_load: {p.suffix}")
+        logger.error("Unsupported file format for %s", path)
+        raise ValueError("Unsupported file format: must be CSV or Parquet")
+
+    logger.info("Saved DataFrame to %s (shape: %s)", path, df.shape)
+    return path
 
 
-def safe_save(obj: Any, path: Union[str, Path]) -> Path:
-    """Convenience saver: detects object type and saves appropriately."""
-    p = Path(path)
-    if isinstance(obj, pd.DataFrame):
-        return save_dataframe(obj, p)
+def read_or_create(path: str, default_df: pd.DataFrame) -> pd.DataFrame:
+    """
+    Load a DataFrame if the file exists, otherwise save and return the default DataFrame.
+
+    Arguments:
+    path -- Path to the file
+    default_df -- DataFrame to save if file does not exist
+
+    Returns:
+    DataFrame -- Loaded or newly created DataFrame
+    """
+    if os.path.exists(path):
+        logger.info("File exists, loading: %s", path)
+        return load_dataframe(path)
     else:
-        return save_json(obj, p)
-
-
-def safe_copy(src: Union[str, Path], dest: Union[str, Path]) -> Path:
-    """Copy a file from src to dest with logging."""
-    src_path = Path(src)
-    dest_path = Path(dest)
-    ensure_dir(dest_path.parent)
-    try:
-        shutil.copy2(src_path, dest_path)
-        logger.info(f"Copied file from {src_path} to {dest_path}")
-        return dest_path
-    except Exception as e:
-        logger.error(f"Failed to copy file from {src_path} to {dest_path}: {e}")
-        raise RuntimeError(f"Failed to copy file from {src_path} to {dest_path}: {e}") from e
-
-
-def safe_move(src: Union[str, Path], dest: Union[str, Path]) -> Path:
-    """Move a file from src to dest with logging."""
-    src_path = Path(src)
-    dest_path = Path(dest)
-    ensure_dir(dest_path.parent)
-    try:
-        shutil.move(str(src_path), str(dest_path))
-        logger.info(f"Moved file from {src_path} to {dest_path}")
-        return dest_path
-    except Exception as e:
-        logger.error(f"Failed to move file from {src_path} to {dest_path}: {e}")
-        raise RuntimeError(f"Failed to move file from {src_path} to {dest_path}: {e}") from e
-
-
-def safe_delete(path: Union[str, Path]) -> None:
-    """Delete a file safely with logging."""
-    p = Path(path)
-    try:
-        if p.exists():
-            p.unlink()
-            logger.info(f"Deleted file: {p}")
-        else:
-            logger.warning(f"File not found for deletion: {p}")
-    except Exception as e:
-        logger.error(f"Failed to delete file {p}: {e}")
-        raise RuntimeError(f"Failed to delete file {p}: {e}") from e
-
-
-def safe_exists(path: Union[str, Path]) -> bool:
-    """Check if a file exists and log the result."""
-    p = Path(path)
-    exists = p.exists()
-    if exists:
-        logger.info(f"File exists: {p}")
-    else:
-        logger.warning(f"File does not exist: {p}")
-    return exists
+        logger.info("File not found, creating new: %s", path)
+        save_dataframe(default_df, path)
+        return default_df
Index: mlruns/0/36f6a6756d984d5f9adcc63774d26eed/tags/mlflow.source.git.commit
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/mlruns/0/36f6a6756d984d5f9adcc63774d26eed/tags/mlflow.source.git.commit b/mlruns/0/36f6a6756d984d5f9adcc63774d26eed/tags/mlflow.source.git.commit
new file mode 100644
--- /dev/null	(date 1765470426215)
+++ b/mlruns/0/36f6a6756d984d5f9adcc63774d26eed/tags/mlflow.source.git.commit	(date 1765470426215)
@@ -0,0 +1,1 @@
+ba58bcf9c077cd4a4662500f7d7cc2c58a4ec45c
\ No newline at end of file
Index: Makefile
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+># ============================================================\n# File: Makefile\n# Purpose: Unified task runner for NBA AI project\n# ============================================================\n\n.PHONY: install precommit check ci clean docs serve_docs train features mlflow\n\n# --- Bootstrap project ---\ninstall:\n\t./setup_project.sh\n\n# --- Pre-commit hooks ---\nprecommit:\n\tpre-commit run --all-files\n\n# --- Local validation pipeline ---\ncheck:\n\tinvoke check\n\n# --- Continuous Integration pipeline ---\nci:\n\tinvoke ci\n\n# --- Clean caches and temp files ---\nclean:\n\tinvoke clean\n\n# --- Build documentation ---\ndocs:\n\tinvoke docs\n\n# --- Serve documentation locally ---\nserve_docs:\n\tinvoke serve_docs\n\n# --- Model training ---\ntrain:\n\tinvoke train\n\n# --- Feature generation ---\nfeatures:\n\tinvoke features\n\n# --- Launch MLflow UI ---\nmlflow:\n\tinvoke mlflow\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/Makefile b/Makefile
--- a/Makefile	(revision 00055356329a845d3b2ca81b77c3ffb2e2b15969)
+++ b/Makefile	(date 1765470426210)
@@ -1,46 +1,43 @@
 # ============================================================
-# File: Makefile
-# Purpose: Unified task runner for NBA AI project
+# Project: nba_analysis
+# Purpose: Automate end-to-end pipeline (enrich → features → train → predict)
 # ============================================================
 
-.PHONY: install precommit check ci clean docs serve_docs train features mlflow
-
-# --- Bootstrap project ---
-install:
-	./setup_project.sh
+PYTHON := python
 
-# --- Pre-commit hooks ---
-precommit:
-	pre-commit run --all-files
+ENRICHED_FILE := data/cache/historical_schedule_with_results.parquet
+FEATURES_FILE := data/cache/features_full.parquet
+MODEL_FILE := models/nba_xgb.pkl
+PREDICTIONS_FILE := data/results/daily_predictions.csv
 
-# --- Local validation pipeline ---
-check:
-	invoke check
+# Default target
+run_all: enrich generate_features train predict
 
-# --- Continuous Integration pipeline ---
-ci:
-	invoke ci
+# Step 0: Enrich schedule with WL outcomes (skip future games)
+enrich:
+	$(PYTHON) -m src.scripts.enrich_schedule
 
-# --- Clean caches and temp files ---
-clean:
-	invoke clean
+# Step 1: Generate features from enriched schedule
+generate_features:
+	$(PYTHON) -m src.scripts.generate_features
 
-# --- Build documentation ---
-docs:
-	invoke docs
-
-# --- Serve documentation locally ---
-serve_docs:
-	invoke serve_docs
-
-# --- Model training ---
+# Step 2: Train model
 train:
-	invoke train
+	$(PYTHON) -m src.model_training.trainer_cli \
+		--model xgb \
+		--season 2025 \
+		--features $(FEATURES_FILE) \
+		--out $(MODEL_FILE)
 
-# --- Feature generation ---
-features:
-	invoke features
+# Step 3: Run daily predictions
+predict:
+	$(PYTHON) -m src.prediction_engine.daily_runner_cli \
+		--model $(MODEL_FILE) \
+		--season 2025 \
+		--limit 10 \
+		--out $(PREDICTIONS_FILE) \
+		--fmt csv
 
-# --- Launch MLflow UI ---
-mlflow:
-	invoke mlflow
+# Clean generated files
+clean:
+	rm -f $(ENRICHED_FILE) $(FEATURES_FILE) $(MODEL_FILE) $(PREDICTIONS_FILE)
Index: mlruns/0/36f6a6756d984d5f9adcc63774d26eed/tags/mlflow.runName
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/mlruns/0/36f6a6756d984d5f9adcc63774d26eed/tags/mlflow.runName b/mlruns/0/36f6a6756d984d5f9adcc63774d26eed/tags/mlflow.runName
new file mode 100644
--- /dev/null	(date 1765470426215)
+++ b/mlruns/0/36f6a6756d984d5f9adcc63774d26eed/tags/mlflow.runName	(date 1765470426215)
@@ -0,0 +1,1 @@
+xgb_pipeline_v2.0
\ No newline at end of file
Index: src/utils/__init__.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/src/utils/__init__.py b/src/utils/__init__.py
--- a/src/utils/__init__.py	(revision 00055356329a845d3b2ca81b77c3ffb2e2b15969)
+++ b/src/utils/__init__.py	(date 1765470426262)
@@ -1,0 +1,61 @@
+# ============================================================
+# File: src/utils/__init__.py
+# Purpose: Consolidated exports for all utility modules
+# Project: nba_analysis
+# Version: 1.1 (fixed stale NBA API exports)
+# ============================================================
+
+# Centralized imports from all utility modules
+
+# Logging configuration
+from .logging_config import configure_logging
+
+# I/O utilities
+from .io import load_dataframe, save_dataframe, read_or_create
+
+# Data cleaning
+from .data_cleaning import clean_data, rename_columns, prepare_game_data
+
+# Unique ID generation
+from .add_unique_id import add_unique_id
+
+# Mapping helpers
+from .mapping import map_team_ids, map_player_ids, map_ids
+
+# NBA API wrapper
+from src.api.nba_api_client import fetch_season_games, fetch_boxscores
+
+# Validation helpers
+from .validation import (
+    validate_game_ids,
+    validate_season,
+    validate_file_extension,
+    validate_inputs,
+)
+
+__all__ = [
+    # Logging
+    "configure_logging",
+    # I/O
+    "load_dataframe",
+    "save_dataframe",
+    "read_or_create",
+    # Cleaning
+    "clean_data",
+    "rename_columns",
+    "prepare_game_data",
+    # Unique IDs
+    "add_unique_id",
+    # Mapping
+    "map_team_ids",
+    "map_player_ids",
+    "map_ids",
+    # NBA API
+    "fetch_season_games",
+    "fetch_boxscores",
+    # Validation
+    "validate_game_ids",
+    "validate_season",
+    "validate_file_extension",
+    "validate_inputs",
+]
Index: mlruns/0/36f6a6756d984d5f9adcc63774d26eed/tags/mlflow.source.type
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/mlruns/0/36f6a6756d984d5f9adcc63774d26eed/tags/mlflow.source.type b/mlruns/0/36f6a6756d984d5f9adcc63774d26eed/tags/mlflow.source.type
new file mode 100644
--- /dev/null	(date 1765470426217)
+++ b/mlruns/0/36f6a6756d984d5f9adcc63774d26eed/tags/mlflow.source.type	(date 1765470426217)
@@ -0,0 +1,1 @@
+LOCAL
\ No newline at end of file
Index: mlruns/0/36f6a6756d984d5f9adcc63774d26eed/tags/mlflow.source.name
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/mlruns/0/36f6a6756d984d5f9adcc63774d26eed/tags/mlflow.source.name b/mlruns/0/36f6a6756d984d5f9adcc63774d26eed/tags/mlflow.source.name
new file mode 100644
--- /dev/null	(date 1765470426216)
+++ b/mlruns/0/36f6a6756d984d5f9adcc63774d26eed/tags/mlflow.source.name	(date 1765470426216)
@@ -0,0 +1,1 @@
+run_pipeline.py
\ No newline at end of file
Index: src/utils/logging_config.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+># ============================================================\n# Path: src/utils/logging_config.py\n# Filename: logging_config.py\n# Author: Your Team\n# Date: December 2025\n# Purpose: Configure logging for the NBA analytics project.\n# ============================================================\n\nimport logging\nimport os\n\ndef configure_logging(log_file: str = \"app.log\"):\n    \"\"\"\n    Configure logging to write both to a file and to the console.\n\n    Args:\n        log_file (str): Path to the log file.\n\n    Returns:\n        str: Path to the log file created.\n    \"\"\"\n    os.makedirs(os.path.dirname(log_file) or \".\", exist_ok=True)\n\n    # Reset any existing handlers\n    for handler in logging.root.handlers[:]:\n        logging.root.removeHandler(handler)\n\n    logging.basicConfig(\n        level=logging.INFO,\n        format=\"%(asctime)s %(levelname)s %(message)s\",\n        handlers=[\n            logging.FileHandler(log_file, mode=\"w\"),\n            logging.StreamHandler()\n        ]\n    )\n\n    logging.info(\"Logging configured. Writing to %s\", log_file)\n    return log_file\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/src/utils/logging_config.py b/src/utils/logging_config.py
--- a/src/utils/logging_config.py	(revision 00055356329a845d3b2ca81b77c3ffb2e2b15969)
+++ b/src/utils/logging_config.py	(date 1765470426265)
@@ -1,38 +1,60 @@
 # ============================================================
-# Path: src/utils/logging_config.py
-# Filename: logging_config.py
-# Author: Your Team
-# Date: December 2025
-# Purpose: Configure logging for the NBA analytics project.
+# File: src/utils/logging_config.py
+# Purpose: Configure logging with console + daily rotating file handler
+# Project: nba_analysis
+# Version: 1.4 (adds log level override, validation, returns named logger)
 # ============================================================
 
 import logging
 import os
+from logging.handlers import TimedRotatingFileHandler
+
 
-def configure_logging(log_file: str = "app.log"):
-    """
-    Configure logging to write both to a file and to the console.
+def configure_logging(
+    log_file: str = None, retention_days: int = None, name: str = "nba_analysis"
+) -> logging.Logger:
+    """Configure logging with console + daily rotating file handler."""
 
-    Args:
-        log_file (str): Path to the log file.
+    log_file = log_file or os.getenv("LOG_FILE", "logs/app.log")
 
-    Returns:
-        str: Path to the log file created.
-    """
+    try:
+        retention_days = int(retention_days or os.getenv("LOG_RETENTION_DAYS", "7"))
+        if retention_days < 1:
+            raise ValueError
+    except ValueError:
+        retention_days = 7
+        logging.warning("Invalid LOG_RETENTION_DAYS, defaulting to 7")
+
     os.makedirs(os.path.dirname(log_file) or ".", exist_ok=True)
 
-    # Reset any existing handlers
     for handler in logging.root.handlers[:]:
         logging.root.removeHandler(handler)
 
+    timed_handler = TimedRotatingFileHandler(
+        log_file,
+        when="midnight",
+        interval=1,
+        backupCount=retention_days,
+        encoding="utf-8",
+    )
+
+    log_level = os.getenv("LOG_LEVEL", "INFO").upper()
+    level = getattr(logging, log_level, logging.INFO)
+
     logging.basicConfig(
-        level=logging.INFO,
-        format="%(asctime)s %(levelname)s %(message)s",
-        handlers=[
-            logging.FileHandler(log_file, mode="w"),
-            logging.StreamHandler()
-        ]
+        level=level,
+        format="%(asctime)s [%(levelname)s] %(message)s",
+        handlers=[timed_handler, logging.StreamHandler()],
+        datefmt="%Y-%m-%d %H:%M:%S",
     )
 
-    logging.info("Logging configured. Writing to %s", log_file)
-    return log_file
+    logger = logging.getLogger(name)
+    logger.setLevel(level)
+
+    logger.info(
+        "Logging configured. Writing to %s (retention: %d days, level: %s)",
+        log_file,
+        retention_days,
+        log_level,
+    )
+    return logger
Index: mlruns/0/36f6a6756d984d5f9adcc63774d26eed/tags/mlflow.user
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/mlruns/0/36f6a6756d984d5f9adcc63774d26eed/tags/mlflow.user b/mlruns/0/36f6a6756d984d5f9adcc63774d26eed/tags/mlflow.user
new file mode 100644
--- /dev/null	(date 1765470426218)
+++ b/mlruns/0/36f6a6756d984d5f9adcc63774d26eed/tags/mlflow.user	(date 1765470426218)
@@ -0,0 +1,1 @@
+Mohamadou
\ No newline at end of file
Index: run_pipeline.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+># ============================================================\n# Path: run_pipeline.py\n# Purpose: NBA analytics pipeline with caching, incremental updates,\n#          progress logging, unique ID deduplication, and organized folders\n# ============================================================\n\nimport pandas as pd\nfrom datetime import date\nimport logging\nimport os\nimport json\nimport time\n\nfrom src.prediction_engine.game_features import fetch_season_games, generate_features_for_games\nfrom src.model_training.train_logreg import train_logreg\nfrom src.prediction_engine.predictor import NBAPredictor\n\n# -----------------------------\n# Logging configuration\n# -----------------------------\ndef configure_logging(log_file: str = \"pipeline.log\"):\n    os.makedirs(os.path.dirname(log_file) or \".\", exist_ok=True)\n    logger = logging.getLogger(\"pipeline\")\n    logger.setLevel(logging.INFO)\n\n    # Clear existing handlers\n    if logger.hasHandlers():\n        logger.handlers.clear()\n\n    fh = logging.FileHandler(log_file, mode=\"w\")\n    sh = logging.StreamHandler()\n    formatter = logging.Formatter(\"%(asctime)s %(levelname)s %(message)s\")\n    fh.setFormatter(formatter)\n    sh.setFormatter(formatter)\n\n    logger.addHandler(fh)\n    logger.addHandler(sh)\n\n    logger.info(\"Logging configured. Writing to %s\", log_file)\n    return logger\n\n# -----------------------------\n# Ensure data directories exist\n# -----------------------------\ndef ensure_dirs():\n    for folder in [\"data/csv\", \"data/parquet\", \"data/cache\", \"data/history\", \"models\"]:\n        os.makedirs(folder, exist_ok=True)\n\n# -----------------------------\n# Helper: Add unique identifier\n# -----------------------------\ndef add_unique_id(df):\n    if \"GAME_ID\" in df.columns and \"TEAM_ID\" in df.columns:\n        if \"prediction_date\" not in df.columns:\n            df[\"prediction_date\"] = pd.to_datetime(date.today()).strftime(\"%Y-%m-%d\")\n        df[\"unique_id\"] = (\n            df[\"GAME_ID\"].astype(str) + \"_\" +\n            df[\"TEAM_ID\"].astype(str) + \"_\" +\n            df[\"prediction_date\"].astype(str)\n        )\n    return df\n\n# -----------------------------\n# Helper: Load cached features\n# -----------------------------\ndef load_cached_features(cache_file=\"data/cache/features_full.parquet\", logger=None):\n    if os.path.exists(cache_file):\n        if logger: logger.info(\"Loading cached features from %s\", cache_file)\n        return pd.read_parquet(cache_file)\n    else:\n        if logger: logger.info(\"No cache found, starting fresh\")\n        return None\n\n# -----------------------------\n# Helper: Save features to cache\n# -----------------------------\ndef save_features_cache(df, cache_file=\"data/cache/features_full.parquet\", logger=None):\n    df = add_unique_id(df)\n    df = df.drop_duplicates(subset=[\"unique_id\"])\n    df.to_parquet(cache_file, index=False)\n    if logger: logger.info(\"Saved features cache to %s (rows=%d)\", cache_file, len(df))\n\n# -----------------------------\n# Main pipeline\n# -----------------------------\ndef main():\n    logger = configure_logging(\"pipeline.log\")\n    ensure_dirs()\n    logger.info(\"Pipeline started\")\n\n    cache_file = \"data/cache/features_full.parquet\"\n    features_full = load_cached_features(cache_file, logger)\n\n    # If no cache, fetch all seasons (batch mode)\n    if features_full is None:\n        logger.info(\"Fetching full historical dataset (2022–2024 + 2025 so far)\")\n        all_features = []\n        for season in [2022, 2023, 2024, 2025]:\n            try:\n                start_time = time.time()\n                game_ids = fetch_season_games(season, limit=None)\n                logger.info(\"Season %d: %d games to fetch\", season, len(game_ids))\n                season_df = generate_features_for_games(game_ids)\n                season_df = add_unique_id(season_df)\n                all_features.append(season_df)\n                logger.info(\"Season %d fetched in %.2f seconds\", season, time.time() - start_time)\n            except Exception as e:\n                logger.error(\"Failed to fetch season %d: %s\", season, e)\n        features_full = pd.concat(all_features, ignore_index=True)\n        save_features_cache(features_full, cache_file, logger)\n\n    else:\n        # Incremental update: only unseen 2025 games\n        logger.info(\"Checking for new 2025 games to append\")\n        try:\n            game_ids_2025 = fetch_season_games(2025, limit=None)\n            cached_ids = set(features_full[\"GAME_ID\"].unique())\n            new_ids = [gid for gid in game_ids_2025 if gid not in cached_ids]\n            logger.info(\"Found %d new games in 2025\", len(new_ids))\n            if new_ids:\n                features_2025 = generate_features_for_games(new_ids)\n                features_2025 = add_unique_id(features_2025)\n                features_full = pd.concat([features_full, features_2025], ignore_index=True)\n                features_full = features_full.drop_duplicates(subset=[\"unique_id\"])\n                save_features_cache(features_full, cache_file, logger)\n        except Exception as e:\n            logger.error(\"Incremental update failed: %s\", e)\n\n    logger.info(\"Training dataset size: %d rows\", len(features_full))\n\n    # Quick validation: check class balance\n    if \"win\" in features_full.columns:\n        win_rate = features_full[\"win\"].mean()\n        logger.info(\"Win rate in dataset: %.3f\", win_rate)\n\n    # Train model\n    try:\n        result = train_logreg(cache_file, out_dir=\"models\")\n        print(\"Training result:\", result)\n        logger.info(\"Training result: %s\", result)\n        with open(\"models/training_metrics.json\", \"w\") as f:\n            json.dump(result, f)\n    except Exception as e:\n        logger.error(\"Model training failed: %s\", e)\n        return\n\n    # Predict today’s games\n    today = date.today().strftime(\"%Y-%m-%d\")\n    try:\n        logger.info(f\"Fetching games for {today}\")\n        game_ids_today = fetch_season_games(2025, limit=5)\n        features_today = generate_features_for_games(game_ids_today)\n\n        predictor = NBAPredictor(model_path=\"models/nba_logreg.pkl\")\n        labels_today = predictor.predict_label(features_today)\n        probas_today = predictor.predict_proba(features_today)\n\n        features_today[\"pred_label\"] = labels_today\n        features_today[\"proba_win\"] = probas_today\n        features_today[\"proba_loss\"] = 1 - probas_today\n        features_today[\"prediction_date\"] = today\n        features_today = add_unique_id(features_today)\n        features_today = features_today.drop_duplicates(subset=[\"unique_id\"])\n\n        print(\"Today's games with predictions:\")\n        print(features_today)\n\n        # Save outputs safely\n        try:\n            features_today.to_csv(\"data/csv/predictions_today.csv\", index=False)\n            features_today.to_parquet(\"data/parquet/predictions_today.parquet\", index=False)\n            features_today.to_csv(f\"data/csv/predictions_{today}.csv\", index=False)\n            features_today.to_parquet(f\"data/parquet/predictions_{today}.parquet\", index=False)\n        except Exception as e:\n            logger.error(\"Failed to save prediction outputs: %s\", e)\n\n        # Append to history with deduplication\n        history_file = \"data/history/predictions_history.parquet\"\n        try:\n            if os.path.exists(history_file):\n                history_df = pd.read_parquet(history_file)\n                history_df = pd.concat([history_df, features_today], ignore_index=True)\n                history_df = history_df.drop_duplicates(subset=[\"unique_id\"])\n            else:\n                history_df = features_today\n            history_df.to_parquet(history_file, index=False)\n            logger.info(\"Appended today's predictions to %s (rows=%d)\", history_file, len(history_df))\n        except Exception as e:\n            logger.error(\"Failed to update history: %s\", e)\n\n    except Exception as e:\n        logger.error(\"Prediction step failed: %s\", e)\n\n    logger.info(\"Pipeline finished successfully\")\n\n\nif __name__ == \"__main__\":\n    main()\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/run_pipeline.py b/run_pipeline.py
--- a/run_pipeline.py	(revision 00055356329a845d3b2ca81b77c3ffb2e2b15969)
+++ b/run_pipeline.py	(date 1765470460000)
@@ -1,198 +1,84 @@
 # ============================================================
-# Path: run_pipeline.py
-# Purpose: NBA analytics pipeline with caching, incremental updates,
-#          progress logging, unique ID deduplication, and organized folders
+# File: run_pipeline.py
+# Purpose: Orchestrator for NBA prediction pipeline
+# Version: 1.1 (Refactored)
 # ============================================================
 
-import pandas as pd
-from datetime import date
+import argparse
+from pathlib import Path
+import yaml
 import logging
-import os
-import json
-import time
-
-from src.prediction_engine.game_features import fetch_season_games, generate_features_for_games
-from src.model_training.train_logreg import train_logreg
-from src.prediction_engine.predictor import NBAPredictor
+from datetime import date
 
-# -----------------------------
-# Logging configuration
-# -----------------------------
-def configure_logging(log_file: str = "pipeline.log"):
-    os.makedirs(os.path.dirname(log_file) or ".", exist_ok=True)
-    logger = logging.getLogger("pipeline")
-    logger.setLevel(logging.INFO)
-
-    # Clear existing handlers
-    if logger.hasHandlers():
-        logger.handlers.clear()
-
-    fh = logging.FileHandler(log_file, mode="w")
-    sh = logging.StreamHandler()
-    formatter = logging.Formatter("%(asctime)s %(levelname)s %(message)s")
-    fh.setFormatter(formatter)
-    sh.setFormatter(formatter)
-
-    logger.addHandler(fh)
-    logger.addHandler(sh)
-
-    logger.info("Logging configured. Writing to %s", log_file)
-    return logger
+from prediction_engine.daily_runner_mlflow import daily_runner_mlflow
+from src.model_training.train_combined import train_model
 
 # -----------------------------
-# Ensure data directories exist
+# Setup logging
 # -----------------------------
-def ensure_dirs():
-    for folder in ["data/csv", "data/parquet", "data/cache", "data/history", "models"]:
-        os.makedirs(folder, exist_ok=True)
+logging.basicConfig(
+    level=logging.INFO, format="%(asctime)s [%(levelname)s] %(message)s"
+)
 
 # -----------------------------
-# Helper: Add unique identifier
+# Load config safely
 # -----------------------------
-def add_unique_id(df):
-    if "GAME_ID" in df.columns and "TEAM_ID" in df.columns:
-        if "prediction_date" not in df.columns:
-            df["prediction_date"] = pd.to_datetime(date.today()).strftime("%Y-%m-%d")
-        df["unique_id"] = (
-            df["GAME_ID"].astype(str) + "_" +
-            df["TEAM_ID"].astype(str) + "_" +
-            df["prediction_date"].astype(str)
-        )
-    return df
+try:
+    with open("config.yaml") as f:
+        cfg = yaml.safe_load(f)
+except FileNotFoundError:
+    logging.error("Missing config.yaml file. Please create one before running.")
+    exit(1)
 
-# -----------------------------
-# Helper: Load cached features
-# -----------------------------
-def load_cached_features(cache_file="data/cache/features_full.parquet", logger=None):
-    if os.path.exists(cache_file):
-        if logger: logger.info("Loading cached features from %s", cache_file)
-        return pd.read_parquet(cache_file)
-    else:
-        if logger: logger.info("No cache found, starting fresh")
-        return None
+MODEL_PATH = cfg.get("model_path")
+MODEL_TYPE = cfg.get("model_type", "logreg")  # fallback to logistic regression
+CACHE_PATH = Path("data/cache/features_full.parquet")
 
 # -----------------------------
-# Helper: Save features to cache
+# Argument parser
 # -----------------------------
-def save_features_cache(df, cache_file="data/cache/features_full.parquet", logger=None):
-    df = add_unique_id(df)
-    df = df.drop_duplicates(subset=["unique_id"])
-    df.to_parquet(cache_file, index=False)
-    if logger: logger.info("Saved features cache to %s (rows=%d)", cache_file, len(df))
+parser = argparse.ArgumentParser(description="Run NBA prediction pipeline")
+parser.add_argument(
+    "--train",
+    action="store_true",
+    help="Train model instead of running daily predictions",
+)
+parser.add_argument("--date", type=str, help="YYYY-MM-DD for daily run (default today)")
+parser.add_argument(
+    "--model-type", type=str, help="Override model type (default from config)"
+)
+parser.add_argument(
+    "--save",
+    action="store_true",
+    help="Save predictions to CSV instead of just printing",
+)
+args = parser.parse_args()
 
 # -----------------------------
-# Main pipeline
+# Main logic
 # -----------------------------
-def main():
-    logger = configure_logging("pipeline.log")
-    ensure_dirs()
-    logger.info("Pipeline started")
-
-    cache_file = "data/cache/features_full.parquet"
-    features_full = load_cached_features(cache_file, logger)
-
-    # If no cache, fetch all seasons (batch mode)
-    if features_full is None:
-        logger.info("Fetching full historical dataset (2022–2024 + 2025 so far)")
-        all_features = []
-        for season in [2022, 2023, 2024, 2025]:
-            try:
-                start_time = time.time()
-                game_ids = fetch_season_games(season, limit=None)
-                logger.info("Season %d: %d games to fetch", season, len(game_ids))
-                season_df = generate_features_for_games(game_ids)
-                season_df = add_unique_id(season_df)
-                all_features.append(season_df)
-                logger.info("Season %d fetched in %.2f seconds", season, time.time() - start_time)
-            except Exception as e:
-                logger.error("Failed to fetch season %d: %s", season, e)
-        features_full = pd.concat(all_features, ignore_index=True)
-        save_features_cache(features_full, cache_file, logger)
-
-    else:
-        # Incremental update: only unseen 2025 games
-        logger.info("Checking for new 2025 games to append")
-        try:
-            game_ids_2025 = fetch_season_games(2025, limit=None)
-            cached_ids = set(features_full["GAME_ID"].unique())
-            new_ids = [gid for gid in game_ids_2025 if gid not in cached_ids]
-            logger.info("Found %d new games in 2025", len(new_ids))
-            if new_ids:
-                features_2025 = generate_features_for_games(new_ids)
-                features_2025 = add_unique_id(features_2025)
-                features_full = pd.concat([features_full, features_2025], ignore_index=True)
-                features_full = features_full.drop_duplicates(subset=["unique_id"])
-                save_features_cache(features_full, cache_file, logger)
-        except Exception as e:
-            logger.error("Incremental update failed: %s", e)
-
-    logger.info("Training dataset size: %d rows", len(features_full))
-
-    # Quick validation: check class balance
-    if "win" in features_full.columns:
-        win_rate = features_full["win"].mean()
-        logger.info("Win rate in dataset: %.3f", win_rate)
-
-    # Train model
-    try:
-        result = train_logreg(cache_file, out_dir="models")
-        print("Training result:", result)
-        logger.info("Training result: %s", result)
-        with open("models/training_metrics.json", "w") as f:
-            json.dump(result, f)
-    except Exception as e:
-        logger.error("Model training failed: %s", e)
-        return
-
-    # Predict today’s games
-    today = date.today().strftime("%Y-%m-%d")
-    try:
-        logger.info(f"Fetching games for {today}")
-        game_ids_today = fetch_season_games(2025, limit=5)
-        features_today = generate_features_for_games(game_ids_today)
+if args.train:
+    logging.info("Starting model training...")
+    train_model(
+        CACHE_PATH, out_dir=Path("models"), model_type=args.model_type or MODEL_TYPE
+    )
+    logging.info("Training complete. Model saved to models/")
+else:
+    # Resolve date
+    date_str = args.date or date.today().strftime("%Y-%m-%d")
 
-        predictor = NBAPredictor(model_path="models/nba_logreg.pkl")
-        labels_today = predictor.predict_label(features_today)
-        probas_today = predictor.predict_proba(features_today)
+    # Validate model path
+    if not MODEL_PATH or not Path(MODEL_PATH).exists():
+        logging.error(f"Model path {MODEL_PATH} not found. Please check config.yaml.")
+        exit(1)
 
-        features_today["pred_label"] = labels_today
-        features_today["proba_win"] = probas_today
-        features_today["proba_loss"] = 1 - probas_today
-        features_today["prediction_date"] = today
-        features_today = add_unique_id(features_today)
-        features_today = features_today.drop_duplicates(subset=["unique_id"])
+    logging.info(f"Running daily NBA predictions for {date_str}...")
+    df = daily_runner_mlflow(MODEL_PATH, game_date=date_str)
 
-        print("Today's games with predictions:")
-        print(features_today)
-
-        # Save outputs safely
-        try:
-            features_today.to_csv("data/csv/predictions_today.csv", index=False)
-            features_today.to_parquet("data/parquet/predictions_today.parquet", index=False)
-            features_today.to_csv(f"data/csv/predictions_{today}.csv", index=False)
-            features_today.to_parquet(f"data/parquet/predictions_{today}.parquet", index=False)
-        except Exception as e:
-            logger.error("Failed to save prediction outputs: %s", e)
-
-        # Append to history with deduplication
-        history_file = "data/history/predictions_history.parquet"
-        try:
-            if os.path.exists(history_file):
-                history_df = pd.read_parquet(history_file)
-                history_df = pd.concat([history_df, features_today], ignore_index=True)
-                history_df = history_df.drop_duplicates(subset=["unique_id"])
-            else:
-                history_df = features_today
-            history_df.to_parquet(history_file, index=False)
-            logger.info("Appended today's predictions to %s (rows=%d)", history_file, len(history_df))
-        except Exception as e:
-            logger.error("Failed to update history: %s", e)
-
-    except Exception as e:
-        logger.error("Prediction step failed: %s", e)
-
-    logger.info("Pipeline finished successfully")
-
-
-if __name__ == "__main__":
-    main()
+    if args.save:
+        out_file = Path(f"predictions_{date_str}.csv")
+        df.to_csv(out_file, index=False)
+        logging.info(f"Predictions saved to {out_file}")
+    else:
+        logging.info("Predictions completed. Sample output:")
+        print(df.head())
Index: mlruns/0/be74c0567e1442e19f7edd4fc8c9bb5a/metrics/accuracy
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/mlruns/0/be74c0567e1442e19f7edd4fc8c9bb5a/metrics/accuracy b/mlruns/0/be74c0567e1442e19f7edd4fc8c9bb5a/metrics/accuracy
new file mode 100644
--- /dev/null	(date 1765470426230)
+++ b/mlruns/0/be74c0567e1442e19f7edd4fc8c9bb5a/metrics/accuracy	(date 1765470426230)
@@ -0,0 +1,1 @@
+1765322531756 0.3 0
Index: mlruns/0/be74c0567e1442e19f7edd4fc8c9bb5a/meta.yaml
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/mlruns/0/be74c0567e1442e19f7edd4fc8c9bb5a/meta.yaml b/mlruns/0/be74c0567e1442e19f7edd4fc8c9bb5a/meta.yaml
new file mode 100644
--- /dev/null	(date 1765470426229)
+++ b/mlruns/0/be74c0567e1442e19f7edd4fc8c9bb5a/meta.yaml	(date 1765470426229)
@@ -0,0 +1,15 @@
+artifact_uri: file:///C:/Users/Mohamadou/PycharmProjects/nba_analytics/mlruns/0/be74c0567e1442e19f7edd4fc8c9bb5a/artifacts
+end_time: 1765322533030
+entry_point_name: ''
+experiment_id: '0'
+lifecycle_stage: active
+run_id: be74c0567e1442e19f7edd4fc8c9bb5a
+run_name: xgb_pipeline_v3.1
+run_uuid: be74c0567e1442e19f7edd4fc8c9bb5a
+source_name: ''
+source_type: 4
+source_version: ''
+start_time: 1765322531129
+status: 3
+tags: []
+user_id: Mohamadou
Index: src/utils/validation.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+># ============================================================\n# File: src/utils/validation.py\n# Purpose: Input validation helpers for NBA AI project\n# Project: nba_analysis\n# ============================================================\n\nfrom pathlib import Path\nfrom typing import Iterable, Optional\nimport pandas as pd\nfrom datetime import datetime\n\n\ndef validate_game_ids(game_ids: str | Iterable[str]) -> list[str]:\n    \"\"\"Validate and normalize a list of NBA game IDs.\"\"\"\n    if not game_ids:\n        return []\n    if isinstance(game_ids, str):\n        ids = [gid.strip() for gid in game_ids.split(\",\") if gid.strip()]\n    else:\n        ids = [str(gid).strip() for gid in game_ids if str(gid).strip()]\n    for gid in ids:\n        if not gid.isdigit() or len(gid) != 10:\n            raise ValueError(f\"Invalid game_id format: {gid}. Expected a 10-digit numeric string.\")\n    return ids\n\n\ndef validate_season(season: str) -> str:\n    \"\"\"Validate a season year and return NBA API season string.\"\"\"\n    if not season:\n        raise ValueError(\"Season year must be provided.\")\n    try:\n        year = int(season)\n    except ValueError:\n        raise ValueError(f\"Invalid season: {season}. Must be an integer year.\")\n    if year < 1996 or year > 2030:\n        raise ValueError(\"Season must be between 1996 and 2030.\")\n    return f\"{year-1}-{str(year)[-2:]}\"\n\n\ndef validate_output_dir(out_dir: str | Path) -> Path:\n    \"\"\"Ensure the output directory exists and is writable.\"\"\"\n    p = Path(out_dir)\n    try:\n        p.mkdir(parents=True, exist_ok=True)\n        test_file = p / \".write_test\"\n        test_file.touch()\n        test_file.unlink()\n    except Exception as e:\n        raise ValueError(f\"Output directory {p} is not writable: {e}\") from e\n    return p\n\n\ndef validate_file_extension(path: str | Path, allowed: list[str]) -> Path:\n    \"\"\"Validate that a file has one of the allowed extensions.\"\"\"\n    p = Path(path)\n    ext = p.suffix.lower()\n    if ext not in [a.lower() for a in allowed]:\n        raise ValueError(f\"Invalid file extension: {ext}. Allowed extensions are: {', '.join(allowed)}\")\n    return p\n\n\ndef validate_dataframe_columns(df: pd.DataFrame, required: list[str]) -> None:\n    \"\"\"Validate that a DataFrame contains all required columns.\"\"\"\n    missing = [col for col in required if col not in df.columns]\n    if missing:\n        raise ValueError(f\"Missing required columns: {', '.join(missing)}\")\n\n\ndef validate_numeric_range(value: float, min_val: float, max_val: float, name: str = \"value\") -> float:\n    \"\"\"Validate that a numeric value falls within a given range [min_val, max_val].\"\"\"\n    if not isinstance(value, (int, float)):\n        raise ValueError(f\"{name} must be numeric, got {type(value).__name__}\")\n    if value < min_val or value > max_val:\n        raise ValueError(f\"{name}={value} out of range [{min_val}, {max_val}]\")\n    return value\n\n\ndef validate_date(date_str: str, fmt: str = \"%Y-%m-%d\",\n                  min_date: Optional[str] = None,\n                  max_date: Optional[str] = None,\n                  name: str = \"date\") -> datetime:\n    \"\"\"Validate that a date string matches the given format and optionally falls within a range.\"\"\"\n    try:\n        dt = datetime.strptime(date_str, fmt)\n    except ValueError:\n        raise ValueError(f\"Invalid {name}: '{date_str}'. Expected format {fmt}.\")\n    if min_date:\n        min_dt = datetime.strptime(min_date, fmt)\n        if dt < min_dt:\n            raise ValueError(f\"{name}={date_str} is before minimum allowed {min_date}.\")\n    if max_date:\n        max_dt = datetime.strptime(max_date, fmt)\n        if dt > max_dt:\n            raise ValueError(f\"{name}={date_str} is after maximum allowed {max_date}.\")\n    return dt\n\n\ndef validate_team_code(team_code: str) -> str:\n    \"\"\"Validate that a team code is a valid NBA team abbreviation.\"\"\"\n    valid_codes = {\n        \"ATL\",\"BOS\",\"BKN\",\"CHA\",\"CHI\",\"CLE\",\"DAL\",\"DEN\",\"DET\",\n        \"GSW\",\"HOU\",\"IND\",\"LAC\",\"LAL\",\"MEM\",\"MIA\",\"MIL\",\"MIN\",\n        \"NOP\",\"NYK\",\"OKC\",\"ORL\",\"PHI\",\"PHX\",\"POR\",\"SAC\",\"SAS\",\n        \"TOR\",\"UTA\",\"WAS\"\n    }\n    code = team_code.strip().upper()\n    if code not in valid_codes:\n        raise ValueError(f\"Invalid team code: {team_code}. Must be one of {', '.join(sorted(valid_codes))}.\")\n    return code\n\n\ndef validate_player_id(player_id: str) -> str:\n    \"\"\"Validate that a player ID is numeric and has a valid length (7–10 digits).\"\"\"\n    pid = player_id.strip()\n    if not pid.isdigit() or not (7 <= len(pid) <= 10):\n        raise ValueError(f\"Invalid player_id format: {player_id}. Expected a 7–10 digit numeric string.\")\n    return pid\n\n\ndef validate_stat_category(stat: str) -> str:\n    \"\"\"Validate that a stat category is one of the recognized NBA stats.\"\"\"\n    valid_stats = {\n        \"points\",\"rebounds\",\"assists\",\"steals\",\"blocks\",\n        \"turnovers\",\"minutes\",\"fgm\",\"fga\",\"fg_pct\",\n        \"three_pm\",\"three_pa\",\"three_pct\",\"ftm\",\"fta\",\"ft_pct\",\n        \"plus_minus\",\"efficiency\"\n    }\n    stat_norm = stat.strip().lower()\n    if stat_norm not in valid_stats:\n        raise ValueError(f\"Invalid stat category: {stat}. Must be one of {', '.join(sorted(valid_stats))}.\")\n    return stat_norm\n\n\ndef validate_lineup(player_ids: Iterable[str]) -> list[str]:\n    \"\"\"Validate that a list of player IDs forms a valid lineup (exactly 5 unique valid IDs).\"\"\"\n    ids = [validate_player_id(pid) for pid in player_ids]\n    if len(ids) != 5:\n        raise ValueError(f\"Lineup must contain exactly 5 players, got {len(ids)}.\")\n    if len(set(ids)) != 5:\n        raise ValueError(\"Lineup contains duplicate player IDs.\")\n    return ids\n\n\ndef validate_schedule(schedule: Iterable[dict]) -> list[dict]:\n    \"\"\"Validate a schedule of games with date, home, and away team codes.\"\"\"\n    validated = []\n    for game in schedule:\n        if \"date\" not in game or \"home\" not in game or \"away\" not in game:\n            raise ValueError(f\"Game missing required fields: {game}\")\n        dt = validate_date(game[\"date\"])\n        home = validate_team_code(game[\"home\"])\n        away = validate_team_code(game[\"away\"])\n        if home == away:\n            raise ValueError(f\"Invalid game: home and away teams cannot be the same ({home}).\")\n        validated.append({\"date\": dt.strftime(\"%Y-%m-%d\"), \"home\": home, \"away\": away})\n    return validated\n\n\ndef validate_config(config: dict, required_keys: list[str]) -> dict:\n    \"\"\"\n    Validate a configuration dictionary.\n    - Ensures required keys exist.\n    - Validates values using appropriate helpers when possible.\n    Returns the normalized config if valid.\n    \"\"\"\n    for key in required_keys:\n        if key not in config:\n            raise ValueError(f\"Missing required config key: {key}\")\n\n    # Example validations\n    if \"season\" in config:\n        config[\"season\"] = validate_season(str(config[\"season\"]))\n    if \"output_dir\" in config:\n        config[\"output_dir\"] = str(validate_output_dir(config[\"output_dir\"]))\n    if \"stat\" in config:\n        config[\"stat\"] = validate_stat_category(config[\"stat\"])\n    if \"lineup\" in config:\n        config[\"lineup\"] = validate_lineup(config[\"lineup\"])\n\n    return config\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/src/utils/validation.py b/src/utils/validation.py
--- a/src/utils/validation.py	(revision 00055356329a845d3b2ca81b77c3ffb2e2b15969)
+++ b/src/utils/validation.py	(date 1765470426267)
@@ -1,180 +1,94 @@
 # ============================================================
 # File: src/utils/validation.py
-# Purpose: Input validation helpers for NBA AI project
+# Purpose: Validation helpers for game IDs, seasons, and file extensions
 # Project: nba_analysis
+# Version: 1.3 (adds combined helper validate_inputs)
 # ============================================================
 
+from datetime import datetime
 from pathlib import Path
-from typing import Iterable, Optional
+from typing import Iterable
 import pandas as pd
-from datetime import datetime
+import logging
 
+logger = logging.getLogger("validation_utils")
 
+
+# --- Existing helpers (unchanged) ---
 def validate_game_ids(game_ids: str | Iterable[str]) -> list[str]:
-    """Validate and normalize a list of NBA game IDs."""
     if not game_ids:
         return []
     if isinstance(game_ids, str):
         ids = [gid.strip() for gid in game_ids.split(",") if gid.strip()]
+    elif isinstance(game_ids, Iterable):
+        ids = [str(gid).strip() for gid in game_ids if str(gid).strip()]
     else:
-        ids = [str(gid).strip() for gid in game_ids if str(gid).strip()]
+        raise TypeError("game_ids must be a string or iterable of strings")
     for gid in ids:
         if not gid.isdigit() or len(gid) != 10:
-            raise ValueError(f"Invalid game_id format: {gid}. Expected a 10-digit numeric string.")
+            raise ValueError(
+                f"Invalid game_id format: {gid}. Expected a 10-digit numeric string."
+            )
+    logger.info("Validated %d game IDs", len(ids))
     return ids
 
 
 def validate_season(season: str) -> str:
-    """Validate a season year and return NBA API season string."""
     if not season:
         raise ValueError("Season year must be provided.")
+    if "-" in season:
+        return season
     try:
         year = int(season)
     except ValueError:
         raise ValueError(f"Invalid season: {season}. Must be an integer year.")
     if year < 1996 or year > 2030:
-        raise ValueError("Season must be between 1996 and 2030.")
-    return f"{year-1}-{str(year)[-2:]}"
-
-
-def validate_output_dir(out_dir: str | Path) -> Path:
-    """Ensure the output directory exists and is writable."""
-    p = Path(out_dir)
-    try:
-        p.mkdir(parents=True, exist_ok=True)
-        test_file = p / ".write_test"
-        test_file.touch()
-        test_file.unlink()
-    except Exception as e:
-        raise ValueError(f"Output directory {p} is not writable: {e}") from e
-    return p
+        raise ValueError(
+            f"Invalid season year: {season}. Must be between 1996 and 2030."
+        )
+    formatted = f"{year-1}-{str(year)[-2:]}"
+    logger.info("Validated season %s -> %s", season, formatted)
+    return formatted
 
 
 def validate_file_extension(path: str | Path, allowed: list[str]) -> Path:
-    """Validate that a file has one of the allowed extensions."""
+    if not isinstance(path, (str, Path)):
+        raise TypeError("path must be a string or Path object")
     p = Path(path)
     ext = p.suffix.lower()
     if ext not in [a.lower() for a in allowed]:
-        raise ValueError(f"Invalid file extension: {ext}. Allowed extensions are: {', '.join(allowed)}")
+        raise ValueError(
+            f"Invalid file extension: {ext}. Allowed extensions are: {', '.join(allowed)}"
+        )
+    logger.info("Validated file extension %s for path %s", ext, p)
     return p
 
 
-def validate_dataframe_columns(df: pd.DataFrame, required: list[str]) -> None:
-    """Validate that a DataFrame contains all required columns."""
-    missing = [col for col in required if col not in df.columns]
-    if missing:
-        raise ValueError(f"Missing required columns: {', '.join(missing)}")
-
-
-def validate_numeric_range(value: float, min_val: float, max_val: float, name: str = "value") -> float:
-    """Validate that a numeric value falls within a given range [min_val, max_val]."""
-    if not isinstance(value, (int, float)):
-        raise ValueError(f"{name} must be numeric, got {type(value).__name__}")
-    if value < min_val or value > max_val:
-        raise ValueError(f"{name}={value} out of range [{min_val}, {max_val}]")
-    return value
-
-
-def validate_date(date_str: str, fmt: str = "%Y-%m-%d",
-                  min_date: Optional[str] = None,
-                  max_date: Optional[str] = None,
-                  name: str = "date") -> datetime:
-    """Validate that a date string matches the given format and optionally falls within a range."""
-    try:
-        dt = datetime.strptime(date_str, fmt)
-    except ValueError:
-        raise ValueError(f"Invalid {name}: '{date_str}'. Expected format {fmt}.")
-    if min_date:
-        min_dt = datetime.strptime(min_date, fmt)
-        if dt < min_dt:
-            raise ValueError(f"{name}={date_str} is before minimum allowed {min_date}.")
-    if max_date:
-        max_dt = datetime.strptime(max_date, fmt)
-        if dt > max_dt:
-            raise ValueError(f"{name}={date_str} is after maximum allowed {max_date}.")
-    return dt
-
-
-def validate_team_code(team_code: str) -> str:
-    """Validate that a team code is a valid NBA team abbreviation."""
-    valid_codes = {
-        "ATL","BOS","BKN","CHA","CHI","CLE","DAL","DEN","DET",
-        "GSW","HOU","IND","LAC","LAL","MEM","MIA","MIL","MIN",
-        "NOP","NYK","OKC","ORL","PHI","PHX","POR","SAC","SAS",
-        "TOR","UTA","WAS"
-    }
-    code = team_code.strip().upper()
-    if code not in valid_codes:
-        raise ValueError(f"Invalid team code: {team_code}. Must be one of {', '.join(sorted(valid_codes))}.")
-    return code
-
-
-def validate_player_id(player_id: str) -> str:
-    """Validate that a player ID is numeric and has a valid length (7–10 digits)."""
-    pid = player_id.strip()
-    if not pid.isdigit() or not (7 <= len(pid) <= 10):
-        raise ValueError(f"Invalid player_id format: {player_id}. Expected a 7–10 digit numeric string.")
-    return pid
-
-
-def validate_stat_category(stat: str) -> str:
-    """Validate that a stat category is one of the recognized NBA stats."""
-    valid_stats = {
-        "points","rebounds","assists","steals","blocks",
-        "turnovers","minutes","fgm","fga","fg_pct",
-        "three_pm","three_pa","three_pct","ftm","fta","ft_pct",
-        "plus_minus","efficiency"
-    }
-    stat_norm = stat.strip().lower()
-    if stat_norm not in valid_stats:
-        raise ValueError(f"Invalid stat category: {stat}. Must be one of {', '.join(sorted(valid_stats))}.")
-    return stat_norm
+# --- New combined helper ---
+def validate_inputs(
+    game_ids: str | Iterable[str] = None,
+    season: str = None,
+    path: str | Path = None,
+    allowed_extensions: list[str] = None,
+) -> dict:
+    """
+    Combined helper: validate game IDs, season, and file extension in one call.
 
+    Args:
+        game_ids (str | Iterable[str], optional): Game IDs to validate.
+        season (str, optional): Season year or formatted string.
+        path (str | Path, optional): File path to validate.
+        allowed_extensions (list[str], optional): Allowed file extensions.
 
-def validate_lineup(player_ids: Iterable[str]) -> list[str]:
-    """Validate that a list of player IDs forms a valid lineup (exactly 5 unique valid IDs)."""
-    ids = [validate_player_id(pid) for pid in player_ids]
-    if len(ids) != 5:
-        raise ValueError(f"Lineup must contain exactly 5 players, got {len(ids)}.")
-    if len(set(ids)) != 5:
-        raise ValueError("Lineup contains duplicate player IDs.")
-    return ids
-
-
-def validate_schedule(schedule: Iterable[dict]) -> list[dict]:
-    """Validate a schedule of games with date, home, and away team codes."""
-    validated = []
-    for game in schedule:
-        if "date" not in game or "home" not in game or "away" not in game:
-            raise ValueError(f"Game missing required fields: {game}")
-        dt = validate_date(game["date"])
-        home = validate_team_code(game["home"])
-        away = validate_team_code(game["away"])
-        if home == away:
-            raise ValueError(f"Invalid game: home and away teams cannot be the same ({home}).")
-        validated.append({"date": dt.strftime("%Y-%m-%d"), "home": home, "away": away})
-    return validated
-
-
-def validate_config(config: dict, required_keys: list[str]) -> dict:
-    """
-    Validate a configuration dictionary.
-    - Ensures required keys exist.
-    - Validates values using appropriate helpers when possible.
-    Returns the normalized config if valid.
+    Returns:
+        dict: Dictionary with validated values.
     """
-    for key in required_keys:
-        if key not in config:
-            raise ValueError(f"Missing required config key: {key}")
-
-    # Example validations
-    if "season" in config:
-        config["season"] = validate_season(str(config["season"]))
-    if "output_dir" in config:
-        config["output_dir"] = str(validate_output_dir(config["output_dir"]))
-    if "stat" in config:
-        config["stat"] = validate_stat_category(config["stat"])
-    if "lineup" in config:
-        config["lineup"] = validate_lineup(config["lineup"])
-
-    return config
+    results = {}
+    if game_ids is not None:
+        results["game_ids"] = validate_game_ids(game_ids)
+    if season is not None:
+        results["season"] = validate_season(season)
+    if path is not None and allowed_extensions is not None:
+        results["path"] = validate_file_extension(path, allowed_extensions)
+    logger.info("Validated inputs: %s", results)
+    return results
Index: mlruns/0/be74c0567e1442e19f7edd4fc8c9bb5a/params/model_type
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/mlruns/0/be74c0567e1442e19f7edd4fc8c9bb5a/params/model_type b/mlruns/0/be74c0567e1442e19f7edd4fc8c9bb5a/params/model_type
new file mode 100644
--- /dev/null	(date 1765470426231)
+++ b/mlruns/0/be74c0567e1442e19f7edd4fc8c9bb5a/params/model_type	(date 1765470426231)
@@ -0,0 +1,1 @@
+xgb
\ No newline at end of file
Index: mlruns/0/be74c0567e1442e19f7edd4fc8c9bb5a/metrics/logloss
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/mlruns/0/be74c0567e1442e19f7edd4fc8c9bb5a/metrics/logloss b/mlruns/0/be74c0567e1442e19f7edd4fc8c9bb5a/metrics/logloss
new file mode 100644
--- /dev/null	(date 1765470426231)
+++ b/mlruns/0/be74c0567e1442e19f7edd4fc8c9bb5a/metrics/logloss	(date 1765470426231)
@@ -0,0 +1,1 @@
+1765322531750 1.2700801639565185 0
Index: mlruns/0/be74c0567e1442e19f7edd4fc8c9bb5a/tags/mlflow.source.git.commit
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/mlruns/0/be74c0567e1442e19f7edd4fc8c9bb5a/tags/mlflow.source.git.commit b/mlruns/0/be74c0567e1442e19f7edd4fc8c9bb5a/tags/mlflow.source.git.commit
new file mode 100644
--- /dev/null	(date 1765470426233)
+++ b/mlruns/0/be74c0567e1442e19f7edd4fc8c9bb5a/tags/mlflow.source.git.commit	(date 1765470426233)
@@ -0,0 +1,1 @@
+f1e4d918228b7b59e1737e41ef90bb3afd2436c0
\ No newline at end of file
Index: mlruns/0/be74c0567e1442e19f7edd4fc8c9bb5a/tags/mlflow.runName
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/mlruns/0/be74c0567e1442e19f7edd4fc8c9bb5a/tags/mlflow.runName b/mlruns/0/be74c0567e1442e19f7edd4fc8c9bb5a/tags/mlflow.runName
new file mode 100644
--- /dev/null	(date 1765470426232)
+++ b/mlruns/0/be74c0567e1442e19f7edd4fc8c9bb5a/tags/mlflow.runName	(date 1765470426232)
@@ -0,0 +1,1 @@
+xgb_pipeline_v3.1
\ No newline at end of file
Index: mlruns/0/be74c0567e1442e19f7edd4fc8c9bb5a/tags/mlflow.source.type
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/mlruns/0/be74c0567e1442e19f7edd4fc8c9bb5a/tags/mlflow.source.type b/mlruns/0/be74c0567e1442e19f7edd4fc8c9bb5a/tags/mlflow.source.type
new file mode 100644
--- /dev/null	(date 1765470426235)
+++ b/mlruns/0/be74c0567e1442e19f7edd4fc8c9bb5a/tags/mlflow.source.type	(date 1765470426235)
@@ -0,0 +1,1 @@
+LOCAL
\ No newline at end of file
Index: mlruns/0/be74c0567e1442e19f7edd4fc8c9bb5a/tags/mlflow.source.name
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/mlruns/0/be74c0567e1442e19f7edd4fc8c9bb5a/tags/mlflow.source.name b/mlruns/0/be74c0567e1442e19f7edd4fc8c9bb5a/tags/mlflow.source.name
new file mode 100644
--- /dev/null	(date 1765470426234)
+++ b/mlruns/0/be74c0567e1442e19f7edd4fc8c9bb5a/tags/mlflow.source.name	(date 1765470426234)
@@ -0,0 +1,1 @@
+run_pipeline.py
\ No newline at end of file
Index: mlruns/0/be74c0567e1442e19f7edd4fc8c9bb5a/tags/mlflow.user
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/mlruns/0/be74c0567e1442e19f7edd4fc8c9bb5a/tags/mlflow.user b/mlruns/0/be74c0567e1442e19f7edd4fc8c9bb5a/tags/mlflow.user
new file mode 100644
--- /dev/null	(date 1765470426235)
+++ b/mlruns/0/be74c0567e1442e19f7edd4fc8c9bb5a/tags/mlflow.user	(date 1765470426235)
@@ -0,0 +1,1 @@
+Mohamadou
\ No newline at end of file
Index: mlruns/0/be74c0567e1442e19f7edd4fc8c9bb5a/artifacts/tracker/games_tracker.csv
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/mlruns/0/be74c0567e1442e19f7edd4fc8c9bb5a/artifacts/tracker/games_tracker.csv b/mlruns/0/be74c0567e1442e19f7edd4fc8c9bb5a/artifacts/tracker/games_tracker.csv
new file mode 100644
--- /dev/null	(date 1765470426227)
+++ b/mlruns/0/be74c0567e1442e19f7edd4fc8c9bb5a/artifacts/tracker/games_tracker.csv	(date 1765470426227)
@@ -0,0 +1,51 @@
+Season,GAME_ID,unique_id,TEAM_ID,Date,HomeTeam,AwayTeam,PlayerNames,Recommendation,FeaturesUsed
+2025,2025000,2025000_0_2025-12-09,0,2025-11-01,Team0,Team1,LeBron James,Watch,"GAME_ID, TEAM_ID, HomeTeam, AwayTeam, points, Month, DayOfWeek, IsWeekend, prediction_date, unique_id, Season"
+2025,2025001,2025001_1_2025-12-09,1,2025-11-02,Team1,Team2,LeBron James,Watch,"GAME_ID, TEAM_ID, HomeTeam, AwayTeam, points, Month, DayOfWeek, IsWeekend, prediction_date, unique_id, Season"
+2025,2025002,2025002_2_2025-12-09,2,2025-11-03,Team2,Team3,LeBron James,Watch,"GAME_ID, TEAM_ID, HomeTeam, AwayTeam, points, Month, DayOfWeek, IsWeekend, prediction_date, unique_id, Season"
+2025,2025003,2025003_3_2025-12-09,3,2025-11-04,Team3,Team4,LeBron James,Watch,"GAME_ID, TEAM_ID, HomeTeam, AwayTeam, points, Month, DayOfWeek, IsWeekend, prediction_date, unique_id, Season"
+2025,2025004,2025004_4_2025-12-09,4,2025-11-05,Team4,Team5,LeBron James,Watch,"GAME_ID, TEAM_ID, HomeTeam, AwayTeam, points, Month, DayOfWeek, IsWeekend, prediction_date, unique_id, Season"
+2025,2025005,2025005_5_2025-12-09,5,2025-11-06,Team5,Team6,LeBron James,Watch,"GAME_ID, TEAM_ID, HomeTeam, AwayTeam, points, Month, DayOfWeek, IsWeekend, prediction_date, unique_id, Season"
+2025,2025006,2025006_6_2025-12-09,6,2025-11-07,Team6,Team7,LeBron James,Watch,"GAME_ID, TEAM_ID, HomeTeam, AwayTeam, points, Month, DayOfWeek, IsWeekend, prediction_date, unique_id, Season"
+2025,2025007,2025007_7_2025-12-09,7,2025-11-08,Team7,Team8,LeBron James,Watch,"GAME_ID, TEAM_ID, HomeTeam, AwayTeam, points, Month, DayOfWeek, IsWeekend, prediction_date, unique_id, Season"
+2025,2025008,2025008_8_2025-12-09,8,2025-11-09,Team8,Team9,LeBron James,Watch,"GAME_ID, TEAM_ID, HomeTeam, AwayTeam, points, Month, DayOfWeek, IsWeekend, prediction_date, unique_id, Season"
+2025,2025009,2025009_9_2025-12-09,9,2025-11-10,Team9,Team10,LeBron James,Watch,"GAME_ID, TEAM_ID, HomeTeam, AwayTeam, points, Month, DayOfWeek, IsWeekend, prediction_date, unique_id, Season"
+2025,2025010,2025010_10_2025-12-09,10,2025-11-11,Team10,Team11,LeBron James,Watch,"GAME_ID, TEAM_ID, HomeTeam, AwayTeam, points, Month, DayOfWeek, IsWeekend, prediction_date, unique_id, Season"
+2025,2025011,2025011_11_2025-12-09,11,2025-11-12,Team11,Team12,LeBron James,Watch,"GAME_ID, TEAM_ID, HomeTeam, AwayTeam, points, Month, DayOfWeek, IsWeekend, prediction_date, unique_id, Season"
+2025,2025012,2025012_12_2025-12-09,12,2025-11-13,Team12,Team13,LeBron James,Watch,"GAME_ID, TEAM_ID, HomeTeam, AwayTeam, points, Month, DayOfWeek, IsWeekend, prediction_date, unique_id, Season"
+2025,2025013,2025013_13_2025-12-09,13,2025-11-14,Team13,Team14,LeBron James,Watch,"GAME_ID, TEAM_ID, HomeTeam, AwayTeam, points, Month, DayOfWeek, IsWeekend, prediction_date, unique_id, Season"
+2025,2025014,2025014_14_2025-12-09,14,2025-11-15,Team14,Team15,LeBron James,Watch,"GAME_ID, TEAM_ID, HomeTeam, AwayTeam, points, Month, DayOfWeek, IsWeekend, prediction_date, unique_id, Season"
+2025,2025015,2025015_15_2025-12-09,15,2025-11-16,Team0,Team1,LeBron James,Watch,"GAME_ID, TEAM_ID, HomeTeam, AwayTeam, points, Month, DayOfWeek, IsWeekend, prediction_date, unique_id, Season"
+2025,2025016,2025016_16_2025-12-09,16,2025-11-17,Team1,Team2,LeBron James,Watch,"GAME_ID, TEAM_ID, HomeTeam, AwayTeam, points, Month, DayOfWeek, IsWeekend, prediction_date, unique_id, Season"
+2025,2025017,2025017_17_2025-12-09,17,2025-11-18,Team2,Team3,LeBron James,Watch,"GAME_ID, TEAM_ID, HomeTeam, AwayTeam, points, Month, DayOfWeek, IsWeekend, prediction_date, unique_id, Season"
+2025,2025018,2025018_18_2025-12-09,18,2025-11-19,Team3,Team4,LeBron James,Watch,"GAME_ID, TEAM_ID, HomeTeam, AwayTeam, points, Month, DayOfWeek, IsWeekend, prediction_date, unique_id, Season"
+2025,2025019,2025019_19_2025-12-09,19,2025-11-20,Team4,Team5,LeBron James,Watch,"GAME_ID, TEAM_ID, HomeTeam, AwayTeam, points, Month, DayOfWeek, IsWeekend, prediction_date, unique_id, Season"
+2025,2025020,2025020_20_2025-12-09,20,2025-11-21,Team5,Team6,LeBron James,Watch,"GAME_ID, TEAM_ID, HomeTeam, AwayTeam, points, Month, DayOfWeek, IsWeekend, prediction_date, unique_id, Season"
+2025,2025021,2025021_21_2025-12-09,21,2025-11-22,Team6,Team7,LeBron James,Watch,"GAME_ID, TEAM_ID, HomeTeam, AwayTeam, points, Month, DayOfWeek, IsWeekend, prediction_date, unique_id, Season"
+2025,2025022,2025022_22_2025-12-09,22,2025-11-23,Team7,Team8,LeBron James,Watch,"GAME_ID, TEAM_ID, HomeTeam, AwayTeam, points, Month, DayOfWeek, IsWeekend, prediction_date, unique_id, Season"
+2025,2025023,2025023_23_2025-12-09,23,2025-11-24,Team8,Team9,LeBron James,Watch,"GAME_ID, TEAM_ID, HomeTeam, AwayTeam, points, Month, DayOfWeek, IsWeekend, prediction_date, unique_id, Season"
+2025,2025024,2025024_24_2025-12-09,24,2025-11-25,Team9,Team10,LeBron James,Watch,"GAME_ID, TEAM_ID, HomeTeam, AwayTeam, points, Month, DayOfWeek, IsWeekend, prediction_date, unique_id, Season"
+2025,2025025,2025025_25_2025-12-09,25,2025-11-26,Team10,Team11,LeBron James,Watch,"GAME_ID, TEAM_ID, HomeTeam, AwayTeam, points, Month, DayOfWeek, IsWeekend, prediction_date, unique_id, Season"
+2025,2025026,2025026_26_2025-12-09,26,2025-11-27,Team11,Team12,LeBron James,Watch,"GAME_ID, TEAM_ID, HomeTeam, AwayTeam, points, Month, DayOfWeek, IsWeekend, prediction_date, unique_id, Season"
+2025,2025027,2025027_27_2025-12-09,27,2025-11-28,Team12,Team13,LeBron James,Watch,"GAME_ID, TEAM_ID, HomeTeam, AwayTeam, points, Month, DayOfWeek, IsWeekend, prediction_date, unique_id, Season"
+2025,2025028,2025028_28_2025-12-09,28,2025-11-01,Team13,Team14,LeBron James,Watch,"GAME_ID, TEAM_ID, HomeTeam, AwayTeam, points, Month, DayOfWeek, IsWeekend, prediction_date, unique_id, Season"
+2025,2025029,2025029_29_2025-12-09,29,2025-11-02,Team14,Team15,LeBron James,Watch,"GAME_ID, TEAM_ID, HomeTeam, AwayTeam, points, Month, DayOfWeek, IsWeekend, prediction_date, unique_id, Season"
+2025,2025030,2025030_0_2025-12-09,0,2025-11-03,Team0,Team1,LeBron James,Watch,"GAME_ID, TEAM_ID, HomeTeam, AwayTeam, points, Month, DayOfWeek, IsWeekend, prediction_date, unique_id, Season"
+2025,2025031,2025031_1_2025-12-09,1,2025-11-04,Team1,Team2,LeBron James,Watch,"GAME_ID, TEAM_ID, HomeTeam, AwayTeam, points, Month, DayOfWeek, IsWeekend, prediction_date, unique_id, Season"
+2025,2025032,2025032_2_2025-12-09,2,2025-11-05,Team2,Team3,LeBron James,Watch,"GAME_ID, TEAM_ID, HomeTeam, AwayTeam, points, Month, DayOfWeek, IsWeekend, prediction_date, unique_id, Season"
+2025,2025033,2025033_3_2025-12-09,3,2025-11-06,Team3,Team4,LeBron James,Watch,"GAME_ID, TEAM_ID, HomeTeam, AwayTeam, points, Month, DayOfWeek, IsWeekend, prediction_date, unique_id, Season"
+2025,2025034,2025034_4_2025-12-09,4,2025-11-07,Team4,Team5,LeBron James,Watch,"GAME_ID, TEAM_ID, HomeTeam, AwayTeam, points, Month, DayOfWeek, IsWeekend, prediction_date, unique_id, Season"
+2025,2025035,2025035_5_2025-12-09,5,2025-11-08,Team5,Team6,LeBron James,Watch,"GAME_ID, TEAM_ID, HomeTeam, AwayTeam, points, Month, DayOfWeek, IsWeekend, prediction_date, unique_id, Season"
+2025,2025036,2025036_6_2025-12-09,6,2025-11-09,Team6,Team7,LeBron James,Watch,"GAME_ID, TEAM_ID, HomeTeam, AwayTeam, points, Month, DayOfWeek, IsWeekend, prediction_date, unique_id, Season"
+2025,2025037,2025037_7_2025-12-09,7,2025-11-10,Team7,Team8,LeBron James,Watch,"GAME_ID, TEAM_ID, HomeTeam, AwayTeam, points, Month, DayOfWeek, IsWeekend, prediction_date, unique_id, Season"
+2025,2025038,2025038_8_2025-12-09,8,2025-11-11,Team8,Team9,LeBron James,Watch,"GAME_ID, TEAM_ID, HomeTeam, AwayTeam, points, Month, DayOfWeek, IsWeekend, prediction_date, unique_id, Season"
+2025,2025039,2025039_9_2025-12-09,9,2025-11-12,Team9,Team10,LeBron James,Watch,"GAME_ID, TEAM_ID, HomeTeam, AwayTeam, points, Month, DayOfWeek, IsWeekend, prediction_date, unique_id, Season"
+2025,2025040,2025040_10_2025-12-09,10,2025-11-13,Team10,Team11,LeBron James,Watch,"GAME_ID, TEAM_ID, HomeTeam, AwayTeam, points, Month, DayOfWeek, IsWeekend, prediction_date, unique_id, Season"
+2025,2025041,2025041_11_2025-12-09,11,2025-11-14,Team11,Team12,LeBron James,Watch,"GAME_ID, TEAM_ID, HomeTeam, AwayTeam, points, Month, DayOfWeek, IsWeekend, prediction_date, unique_id, Season"
+2025,2025042,2025042_12_2025-12-09,12,2025-11-15,Team12,Team13,LeBron James,Watch,"GAME_ID, TEAM_ID, HomeTeam, AwayTeam, points, Month, DayOfWeek, IsWeekend, prediction_date, unique_id, Season"
+2025,2025043,2025043_13_2025-12-09,13,2025-11-16,Team13,Team14,LeBron James,Watch,"GAME_ID, TEAM_ID, HomeTeam, AwayTeam, points, Month, DayOfWeek, IsWeekend, prediction_date, unique_id, Season"
+2025,2025044,2025044_14_2025-12-09,14,2025-11-17,Team14,Team15,LeBron James,Watch,"GAME_ID, TEAM_ID, HomeTeam, AwayTeam, points, Month, DayOfWeek, IsWeekend, prediction_date, unique_id, Season"
+2025,2025045,2025045_15_2025-12-09,15,2025-11-18,Team0,Team1,LeBron James,Watch,"GAME_ID, TEAM_ID, HomeTeam, AwayTeam, points, Month, DayOfWeek, IsWeekend, prediction_date, unique_id, Season"
+2025,2025046,2025046_16_2025-12-09,16,2025-11-19,Team1,Team2,LeBron James,Watch,"GAME_ID, TEAM_ID, HomeTeam, AwayTeam, points, Month, DayOfWeek, IsWeekend, prediction_date, unique_id, Season"
+2025,2025047,2025047_17_2025-12-09,17,2025-11-20,Team2,Team3,LeBron James,Watch,"GAME_ID, TEAM_ID, HomeTeam, AwayTeam, points, Month, DayOfWeek, IsWeekend, prediction_date, unique_id, Season"
+2025,2025048,2025048_18_2025-12-09,18,2025-11-21,Team3,Team4,LeBron James,Watch,"GAME_ID, TEAM_ID, HomeTeam, AwayTeam, points, Month, DayOfWeek, IsWeekend, prediction_date, unique_id, Season"
+2025,2025049,2025049_19_2025-12-09,19,2025-11-22,Team4,Team5,LeBron James,Watch,"GAME_ID, TEAM_ID, HomeTeam, AwayTeam, points, Month, DayOfWeek, IsWeekend, prediction_date, unique_id, Season"
Index: tests/test_predictor.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+># ============================================================\n# Path: tests/test_predictor.py\n# Purpose: Unit tests for src/prediction_engine/predictor.py\n# Project: nba_analysis\n# ============================================================\n\nimport pytest\nimport pandas as pd\nimport numpy as np\nfrom sklearn.linear_model import LogisticRegression\nfrom src.model_inference.predictor import Predictor\n\n\ndef test_predict_proba_and_label_binary():\n    X = pd.DataFrame({\"feat1\": [0, 1, 0, 1], \"feat2\": [1, 0, 1, 0]})\n    y = np.array([0, 1, 0, 1])\n    model = LogisticRegression().fit(X, y)\n    predictor = Predictor(model)\n\n    proba = predictor.predict_proba(X)\n    assert proba.shape[1] == 2\n    labels = predictor.predict_label(X, threshold=0.5)\n    assert set(labels).issubset({0, 1})\n\n\ndef test_invalid_input():\n    model = LogisticRegression()\n    predictor = Predictor(model)\n    with pytest.raises(TypeError):\n        predictor.predict_proba(\"not a dataframe\")\n    with pytest.raises(ValueError):\n        predictor.predict_proba(pd.DataFrame())\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/tests/test_predictor.py b/tests/test_predictor.py
--- a/tests/test_predictor.py	(revision 00055356329a845d3b2ca81b77c3ffb2e2b15969)
+++ b/tests/test_predictor.py	(date 1765470426276)
@@ -1,32 +1,157 @@
 # ============================================================
-# Path: tests/test_predictor.py
+# File: tests/test_predictor.py
 # Purpose: Unit tests for src/prediction_engine/predictor.py
 # Project: nba_analysis
 # ============================================================
 
-import pytest
-import pandas as pd
+import os
+import sys
+from pathlib import Path
+
+import joblib
 import numpy as np
+import pandas as pd
+import pytest
 from sklearn.linear_model import LogisticRegression
-from src.model_inference.predictor import Predictor
 
+from src.prediction_engine.predictor import NBAPredictor, main as predictor_cli
 
-def test_predict_proba_and_label_binary():
-    X = pd.DataFrame({"feat1": [0, 1, 0, 1], "feat2": [1, 0, 1, 0]})
-    y = np.array([0, 1, 0, 1])
-    model = LogisticRegression().fit(X, y)
-    predictor = Predictor(model)
 
-    proba = predictor.predict_proba(X)
-    assert proba.shape[1] == 2
-    labels = predictor.predict_label(X, threshold=0.5)
-    assert set(labels).issubset({0, 1})
+@pytest.fixture
+def trained_model(tmp_path):
+    """
+    Train a tiny LogisticRegression on synthetic data and save it.
+    Ensures feature_names_in_ is available after fit.
+    """
+    X = pd.DataFrame(
+        {
+            "feat_a": np.random.randn(50),
+            "feat_b": np.random.randn(50),
+            "feat_c": np.random.randn(50),
+        }
+    )
+    y = (X["feat_a"] * 0.5 + X["feat_b"] * -0.3 + np.random.randn(50) * 0.1 > 0).astype(
+        int
+    )
 
-
-def test_invalid_input():
     model = LogisticRegression()
-    predictor = Predictor(model)
-    with pytest.raises(TypeError):
-        predictor.predict_proba("not a dataframe")
-    with pytest.raises(ValueError):
-        predictor.predict_proba(pd.DataFrame())
+    model.fit(X, y)
+
+    model_path = tmp_path / "model.pkl"
+    joblib.dump(model, model_path)
+    return model_path, X.columns.tolist()
+
+
+@pytest.fixture
+def features_df():
+    """
+    Build a features DataFrame with extra non-numeric and identifier columns.
+    Ensures the predictor's numeric coercion and dropping logic works.
+    """
+    df = pd.DataFrame(
+        {
+            "feat_a": [0.1, 0.2, -0.3],
+            "feat_b": [1.0, -0.5, 0.0],
+            "feat_c": [2.0, 2.5, -1.5],
+            "TEAM_NAME": ["LAL", "BOS", "NYK"],
+            "OPPONENT_TEAM_NAME": ["BOS", "LAL", "PHI"],
+            "GAME_ID": ["g1", "g2", "g3"],
+            "unique_id": ["u1", "u2", "u3"],
+            "prediction_date": ["2025-12-10"] * 3,
+        }
+    )
+    return df
+
+
+def test_predict_proba_numeric_coercion_and_alignment(trained_model, features_df):
+    model_path, expected_cols = trained_model
+    predictor = NBAPredictor(model_path=str(model_path), log_dir=str(model_path.parent))
+
+    # Ensure predictor aligns to the trained model's expected features
+    proba = predictor.predict_proba(features_df)
+    assert isinstance(proba, pd.Series)
+    assert proba.name == "win_proba"
+    assert len(proba) == len(features_df)
+
+    # Verify no non-numeric columns are passed to the model (implicitly via no error)
+    # And mean is a finite number
+    assert np.isfinite(proba.mean())
+
+
+def test_predict_label_threshold_behavior(trained_model, features_df):
+    model_path, _ = trained_model
+    predictor = NBAPredictor(model_path=str(model_path), log_dir=str(model_path.parent))
+
+    labels_low = predictor.predict_label(features_df, threshold=0.1)
+    labels_high = predictor.predict_label(features_df, threshold=0.9)
+
+    assert isinstance(labels_low, pd.Series)
+    assert labels_low.name == "win_pred"
+    assert labels_low.dtype in (np.int64, np.int32, "int64", "int32")
+    # Lower threshold should yield more 1s than high threshold
+    assert labels_low.sum() >= labels_high.sum()
+
+
+def test_missing_expected_features_are_filled(trained_model):
+    model_path, expected_cols = trained_model
+    predictor = NBAPredictor(model_path=str(model_path), log_dir=str(model_path.parent))
+
+    # Provide only a subset; predictor should reindex and fill missing with 0
+    partial = pd.DataFrame({"feat_a": [0.1, -0.2, 0.3]})
+    proba = predictor.predict_proba(partial)
+    assert len(proba) == 3
+    # Sanity: predictions succeed even with missing features
+    assert np.isfinite(proba).all()
+
+
+def test_cli_smoke_proba(tmp_path, trained_model, features_df, monkeypatch):
+    # Save features to CSV
+    feats_path = tmp_path / "feats.csv"
+    features_df.to_csv(feats_path, index=False)
+
+    model_path, _ = trained_model
+    out_path = tmp_path / "preds.csv"
+
+    # Mock MLflow to avoid external interactions
+    class DummyRun:
+        def __enter__(self):
+            return self
+
+        def __exit__(self, *args):
+            return False
+
+    import src.prediction_engine.predictor as predictor_mod
+
+    monkeypatch.setattr(predictor_mod.mlflow, "start_run", lambda **kwargs: DummyRun())
+    monkeypatch.setattr(predictor_mod.mlflow, "log_param", lambda *args, **kwargs: None)
+    monkeypatch.setattr(
+        predictor_mod.mlflow, "log_metric", lambda *args, **kwargs: None
+    )
+    monkeypatch.setattr(
+        predictor_mod.mlflow, "log_artifact", lambda *args, **kwargs: None
+    )
+
+    # Prepare argv
+    argv = [
+        "predictor",
+        "--model",
+        str(model_path),
+        "--features",
+        str(feats_path),
+        "--mode",
+        "proba",
+        "--output",
+        str(out_path),
+        "--mlflow",
+    ]
+    monkeypatch.setattr(sys, "argv", argv)
+
+    # Run CLI
+    predictor_cli()
+
+    # Output file should exist and contain predictions
+    assert out_path.exists()
+    df_out = pd.read_csv(out_path)
+    # csv contains one column of predictions; name may be 'win_proba'
+    assert len(df_out) == len(features_df)
+    assert df_out.shape[1] >= 1
Index: src/utils/add_unique_id.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/src/utils/add_unique_id.py b/src/utils/add_unique_id.py
new file mode 100644
--- /dev/null	(date 1765470426263)
+++ b/src/utils/add_unique_id.py	(date 1765470426263)
@@ -0,0 +1,52 @@
+# ============================================================
+# File: src/utils/add_unique_id.py
+# Purpose: Add unique identifier column to features DataFrame
+# Project: nba_analysis
+# Version: 1.4 (deduplication + type enforcement)
+# ============================================================
+
+import logging
+import datetime
+import pandas as pd
+
+
+def add_unique_id(df: pd.DataFrame) -> pd.DataFrame:
+    """
+    Add a unique_id column combining GAME_ID, TEAM_ID, and prediction_date.
+    Guarantees unique_id even if GAME_ID/TEAM_ID are missing.
+    Deduplicates rows with identical unique_id values.
+    """
+    if not isinstance(df, pd.DataFrame):
+        raise TypeError("add_unique_id expects a pandas DataFrame")
+
+    df = df.copy()
+
+    if "GAME_ID" not in df.columns:
+        logging.warning("GAME_ID missing, assigning placeholder")
+        df["GAME_ID"] = [f"unknown_game_{i}" for i in range(len(df))]
+    df["GAME_ID"] = df["GAME_ID"].astype(str)
+
+    if "TEAM_ID" not in df.columns:
+        logging.warning("TEAM_ID missing, assigning placeholder")
+        df["TEAM_ID"] = -1
+    df["TEAM_ID"] = pd.to_numeric(df["TEAM_ID"], errors="coerce").fillna(-1).astype(int)
+
+    if "prediction_date" not in df.columns:
+        logging.warning("prediction_date missing, assigning today's date")
+        df["prediction_date"] = datetime.date.today().isoformat()
+    df["prediction_date"] = df["prediction_date"].astype(str)
+
+    df["unique_id"] = (
+        df["GAME_ID"].astype(str)
+        .str.cat(df["TEAM_ID"].astype(str), sep="_")
+        .str.cat(df["prediction_date"].astype(str), sep="_")
+    )
+
+    # Deduplicate on unique_id
+    before = len(df)
+    df = df.drop_duplicates(subset=["unique_id"]).reset_index(drop=True)
+    after = len(df)
+
+    logging.info("Generated %d unique IDs (deduplicated from %d rows). Example: %s",
+                 after, before, df["unique_id"].iloc[0] if not df.empty else "N/A")
+    return df
Index: tests/conftest.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+># ============================================================\n# Path: tests/conftest.py\n# Filename: conftest.py\n# Author: Your Team\n# Date: December 2025\n# Purpose: Shared pytest fixtures for NBA analytics tests\n# ============================================================\n\nimport pytest\nimport pandas as pd\nfrom src.prediction_engine.game_features import fetch_season_games, fetch_game_features\n\n@pytest.fixture(scope=\"function\")\ndef real_features(tmp_path):\n    \"\"\"\n    Fetch a small batch of real NBA games from the 2023 season\n    and save them as a parquet file for testing.\n    \"\"\"\n    game_ids = fetch_season_games(2023, limit=3)\n    features = pd.concat([fetch_game_features(gid) for gid in game_ids], ignore_index=True)\n\n    features_path = tmp_path / \"features.parquet\"\n    features.to_parquet(features_path, index=False)\n    return str(features_path)\n\n@pytest.fixture\ndef model_dir(tmp_path):\n    \"\"\"Provide a temporary directory for saving models.\"\"\"\n    out_dir = tmp_path / \"models\"\n    out_dir.mkdir()\n    return str(out_dir)\n\n@pytest.fixture\ndef set_env(monkeypatch, request):\n    \"\"\"\n    Fixture to set environment variables.\n    By default sets NBA_API_KEY only.\n    If test is marked with 'with_db', also sets dummy database values\n    using both prefixed (DATABASE_HOST) and short (HOST) names.\n    \"\"\"\n    monkeypatch.setenv(\"NBA_API_KEY\", \"dummy_key\")\n\n    if request.node.get_closest_marker(\"with_db\"):\n        # Prefixed env vars\n        monkeypatch.setenv(\"DATABASE_HOST\", \"localhost\")\n        monkeypatch.setenv(\"DATABASE_PORT\", \"5432\")\n        monkeypatch.setenv(\"DATABASE_USER\", \"test_user\")\n        monkeypatch.setenv(\"DATABASE_PASSWORD\", \"test_pass\")\n\n        # Short env vars (optional fallback)\n        monkeypatch.setenv(\"HOST\", \"localhost\")\n        monkeypatch.setenv(\"PORT\", \"5432\")\n        monkeypatch.setenv(\"USER\", \"test_user\")\n        monkeypatch.setenv(\"PASSWORD\", \"test_pass\")\n\n    yield\n\n    # Clean up\n    for var in [\n        \"NBA_API_KEY\",\n        \"DATABASE_HOST\", \"DATABASE_PORT\", \"DATABASE_USER\", \"DATABASE_PASSWORD\",\n        \"HOST\", \"PORT\", \"USER\", \"PASSWORD\"\n    ]:\n        monkeypatch.delenv(var, raising=False)\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/tests/conftest.py b/tests/conftest.py
--- a/tests/conftest.py	(revision 00055356329a845d3b2ca81b77c3ffb2e2b15969)
+++ b/tests/conftest.py	(date 1765470426269)
@@ -1,64 +1,76 @@
-# ============================================================
-# Path: tests/conftest.py
-# Filename: conftest.py
-# Author: Your Team
-# Date: December 2025
-# Purpose: Shared pytest fixtures for NBA analytics tests
-# ============================================================
-
-import pytest
 import pandas as pd
-from src.prediction_engine.game_features import fetch_season_games, fetch_game_features
+import pytest
 
-@pytest.fixture(scope="function")
-def real_features(tmp_path):
-    """
-    Fetch a small batch of real NBA games from the 2023 season
-    and save them as a parquet file for testing.
-    """
-    game_ids = fetch_season_games(2023, limit=3)
-    features = pd.concat([fetch_game_features(gid) for gid in game_ids], ignore_index=True)
-
-    features_path = tmp_path / "features.parquet"
-    features.to_parquet(features_path, index=False)
-    return str(features_path)
 
 @pytest.fixture
-def model_dir(tmp_path):
-    """Provide a temporary directory for saving models."""
-    out_dir = tmp_path / "models"
-    out_dir.mkdir()
-    return str(out_dir)
+def sample_game_data():
+    # Fixture to provide sample game data
+    return pd.DataFrame(
+        {
+            "GAME_DATE": ["2025-12-10", "2025-12-09"],
+            "TEAM_NAME": ["Lakers", "Celtics"],
+            "MATCHUP": ["Lakers vs. Celtics", "Celtics vs. Lakers"],
+            "POINTS": [102, 99],
+            "TARGET": [1, 0],
+            "TEAM_ID": [1, 2],
+            "GAME_ID": ["game_001", "game_002"],
+            "OPPONENT_TEAM_ID": [2, 1],
+        }
+    )
+
+
+def test_sample_game_data_columns(sample_game_data):
+    # Test if the correct columns are present
+    assert "GAME_DATE" in sample_game_data.columns
+    assert "TEAM_NAME" in sample_game_data.columns
+    assert "MATCHUP" in sample_game_data.columns
+    assert "POINTS" in sample_game_data.columns
+    assert "TARGET" in sample_game_data.columns
+    assert "TEAM_ID" in sample_game_data.columns
+    assert "GAME_ID" in sample_game_data.columns
+    assert "OPPONENT_TEAM_ID" in sample_game_data.columns
+
+
+def test_sample_game_data_row_count(sample_game_data):
+    # Test if the data contains the correct number of rows
+    assert len(sample_game_data) == 2  # There should be 2 rows in the sample data
 
-@pytest.fixture
-def set_env(monkeypatch, request):
-    """
-    Fixture to set environment variables.
-    By default sets NBA_API_KEY only.
-    If test is marked with 'with_db', also sets dummy database values
-    using both prefixed (DATABASE_HOST) and short (HOST) names.
-    """
-    monkeypatch.setenv("NBA_API_KEY", "dummy_key")
 
-    if request.node.get_closest_marker("with_db"):
-        # Prefixed env vars
-        monkeypatch.setenv("DATABASE_HOST", "localhost")
-        monkeypatch.setenv("DATABASE_PORT", "5432")
-        monkeypatch.setenv("DATABASE_USER", "test_user")
-        monkeypatch.setenv("DATABASE_PASSWORD", "test_pass")
+def test_target_column_values(sample_game_data):
+    # Test if the 'TARGET' column contains expected values (e.g., binary outcome)
+    assert set(sample_game_data["TARGET"]) == {0, 1}  # Ensure TARGET is binary
 
-        # Short env vars (optional fallback)
-        monkeypatch.setenv("HOST", "localhost")
-        monkeypatch.setenv("PORT", "5432")
-        monkeypatch.setenv("USER", "test_user")
-        monkeypatch.setenv("PASSWORD", "test_pass")
 
-    yield
+def test_calculate_team_win_percentage(sample_game_data):
+    # Example of a calculation you might perform: calculate win percentage
+    sample_game_data["TEAM_WIN_PERCENTAGE"] = (
+        sample_game_data.groupby("TEAM_NAME")["POINTS"]
+        .expanding()
+        .mean()
+        .reset_index(level=0, drop=True)
+    )
 
-    # Clean up
-    for var in [
-        "NBA_API_KEY",
-        "DATABASE_HOST", "DATABASE_PORT", "DATABASE_USER", "DATABASE_PASSWORD",
-        "HOST", "PORT", "USER", "PASSWORD"
-    ]:
-        monkeypatch.delenv(var, raising=False)
+    # Test if the calculated win percentage column exists
+    assert "TEAM_WIN_PERCENTAGE" in sample_game_data.columns
+
+    # Test if the win percentages are correct (e.g., it should be the same as POINTS for this test)
+    assert sample_game_data.loc[0, "TEAM_WIN_PERCENTAGE"] == 102
+    assert sample_game_data.loc[1, "TEAM_WIN_PERCENTAGE"] == 99
+
+
+def test_matchup_format(sample_game_data):
+    # Test if the 'MATCHUP' column is in the expected format
+    assert sample_game_data["MATCHUP"].iloc[0] == "Lakers vs. Celtics"
+    assert sample_game_data["MATCHUP"].iloc[1] == "Celtics vs. Lakers"
+
+
+def test_game_date_format(sample_game_data):
+    # Test if the 'GAME_DATE' column is in the correct datetime format
+    sample_game_data["GAME_DATE"] = pd.to_datetime(sample_game_data["GAME_DATE"])
+    assert (
+        sample_game_data["GAME_DATE"].dtype == "datetime64[ns]"
+    )  # Ensure it's datetime type
+
+
+if __name__ == "__main__":
+    pytest.main()
Index: src/utils/mapping.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/src/utils/mapping.py b/src/utils/mapping.py
new file mode 100644
--- /dev/null	(date 1765470426266)
+++ b/src/utils/mapping.py	(date 1765470426266)
@@ -0,0 +1,76 @@
+# ============================================================
+# File: src/utils/mapping.py
+# Purpose: Map TEAM_ID, PLAYER_ID to human-readable names
+# Project: nba_analysis
+# Version: 1.3 (adds combined helper map_ids)
+# ============================================================
+
+import pandas as pd
+import logging
+
+# Default synthetic mapping dictionaries
+TEAM_MAP = {i: f"Team_{i}" for i in range(30)}
+PLAYER_MAP = {i: f"Player_{i}" for i in range(1000)}
+
+
+def map_team_ids(
+    df: pd.DataFrame, team_col: str = "TEAM_ID", team_map: dict = None
+) -> pd.DataFrame:
+    """Map TEAM_ID values in a DataFrame to human-readable team names."""
+    if not isinstance(df, pd.DataFrame):
+        raise TypeError("map_team_ids expects a pandas DataFrame")
+
+    df = df.copy()
+    team_map = team_map or TEAM_MAP
+
+    if team_col in df.columns:
+        df["TEAM_NAME"] = df[team_col].map(team_map).fillna("UnknownTeam")
+    else:
+        df["TEAM_NAME"] = "UnknownTeam"
+
+    mapped_count = (df["TEAM_NAME"] != "UnknownTeam").sum()
+    logging.info("Mapped %d team IDs, %d unknown", mapped_count, len(df) - mapped_count)
+
+    return df
+
+
+def map_player_ids(
+    df: pd.DataFrame, player_col: str = "PLAYER_ID", player_map: dict = None
+) -> pd.DataFrame:
+    """Map PLAYER_ID values in a DataFrame to human-readable player names."""
+    if not isinstance(df, pd.DataFrame):
+        raise TypeError("map_player_ids expects a pandas DataFrame")
+
+    df = df.copy()
+    player_map = player_map or PLAYER_MAP
+
+    if player_col in df.columns:
+        df["PLAYER_NAME"] = df[player_col].map(player_map).fillna("UnknownPlayer")
+    else:
+        df["PLAYER_NAME"] = "UnknownPlayer"
+
+    mapped_count = (df["PLAYER_NAME"] != "UnknownPlayer").sum()
+    logging.info(
+        "Mapped %d player IDs, %d unknown", mapped_count, len(df) - mapped_count
+    )
+
+    return df
+
+
+def map_ids(
+    df: pd.DataFrame,
+    team_col: str = "TEAM_ID",
+    player_col: str = "PLAYER_ID",
+    team_map: dict = None,
+    player_map: dict = None,
+) -> pd.DataFrame:
+    """
+    Combined helper: map both TEAM_ID and PLAYER_ID to human-readable names in one call.
+    """
+    if not isinstance(df, pd.DataFrame):
+        raise TypeError("map_ids expects a pandas DataFrame")
+
+    df = map_team_ids(df, team_col=team_col, team_map=team_map)
+    df = map_player_ids(df, player_col=player_col, player_map=player_map)
+    logging.info("Mapped both team and player IDs for %d rows", len(df))
+    return df
Index: src/utils/data_cleaning.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/src/utils/data_cleaning.py b/src/utils/data_cleaning.py
new file mode 100644
--- /dev/null	(date 1765470426263)
+++ b/src/utils/data_cleaning.py	(date 1765470426263)
@@ -0,0 +1,71 @@
+# ============================================================
+# File: src/utils/data_cleaning.py
+# Purpose: Utility functions for cleaning and renaming NBA game data
+# Project: nba_analysis
+# Version: 1.3 (adds combined helper + consistent headers)
+#
+# Dependencies:
+# - pandas
+# - logging (for info/debug messages)
+# ============================================================
+
+import pandas as pd
+import logging
+
+
+def clean_data(df: pd.DataFrame) -> pd.DataFrame:
+    """
+    Clean NBA game data by converting GAME_DATE to datetime and dropping rows
+    missing critical identifiers (GAME_DATE, TEAM_ID, GAME_ID).
+    """
+    if not isinstance(df, pd.DataFrame):
+        raise TypeError("clean_data expects a pandas DataFrame")
+
+    df = df.copy()
+
+    if "GAME_DATE" in df.columns:
+        df["GAME_DATE"] = pd.to_datetime(df["GAME_DATE"], errors="coerce")
+
+    critical_cols = [c for c in ["GAME_DATE", "TEAM_ID", "GAME_ID"] if c in df.columns]
+    before = len(df)
+    if critical_cols:
+        df = df.dropna(subset=critical_cols)
+    after = len(df)
+    logging.info("Dropped %d rows with missing critical values", before - after)
+
+    return df
+
+
+def rename_columns(df: pd.DataFrame) -> pd.DataFrame:
+    """
+    Rename columns to standardized names for consistency.
+    """
+    if not isinstance(df, pd.DataFrame):
+        raise TypeError("rename_columns expects a pandas DataFrame")
+
+    df = df.copy()
+    df.columns = [c.upper() for c in df.columns]
+
+    rename_map = {
+        "PTS": "POINTS",
+        "WL": "TARGET",
+        "REB": "REBOUNDS",
+        "AST": "ASSISTS",
+    }
+
+    existing_map = {k: v for k, v in rename_map.items() if k in df.columns}
+    if existing_map:
+        df.rename(columns=existing_map, inplace=True)
+        logging.info("Renamed columns: %s", existing_map)
+
+    return df
+
+
+def prepare_game_data(df: pd.DataFrame) -> pd.DataFrame:
+    """
+    Combined helper: always clean and rename NBA game data in one call.
+    """
+    df = clean_data(df)
+    df = rename_columns(df)
+    logging.info("Game data prepared with %d rows and standardized columns", len(df))
+    return df
Index: src/utils/nba_wrapper.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/src/utils/nba_wrapper.py b/src/utils/nba_wrapper.py
new file mode 100644
--- /dev/null	(date 1765470426266)
+++ b/src/utils/nba_wrapper.py	(date 1765470426266)
@@ -0,0 +1,95 @@
+# ============================================================
+# File: src/utils/nba_api_wrapper.py
+# Purpose: Wrapper for nba_api endpoints to fetch games & stats
+# Project: nba_analysis
+# Version: 1.3 (adds combined helper fetch_games)
+# ============================================================
+
+import logging
+from datetime import datetime
+import pandas as pd
+from nba_api.stats.endpoints import leaguegamefinder, scoreboard
+
+logger = logging.getLogger("nba_api_wrapper")
+
+EXPECTED_COLS = [
+    "GAME_DATE",
+    "TEAM_NAME",
+    "MATCHUP",
+    "GAME_ID",
+    "TEAM_ID",
+    "OPPONENT_TEAM_ID",
+    "POINTS",
+    "TARGET",
+]
+EXPECTED_TODAY_COLS = ["GAME_ID", "HOME_TEAM_ID", "AWAY_TEAM_ID"]
+
+
+def fetch_season_games(season: str) -> pd.DataFrame:
+    """Fetch all games for a given season using nba_api."""
+    if not isinstance(season, str):
+        raise TypeError("season must be a string like '2022-23'")
+
+    logger.info("Fetching games for season %s...", season)
+    try:
+        gamefinder = leaguegamefinder.LeagueGameFinder(season_nullable=season)
+        df = gamefinder.get_data_frames()[0]
+
+        df = df[
+            [
+                "GAME_DATE",
+                "TEAM_NAME",
+                "MATCHUP",
+                "GAME_ID",
+                "TEAM_ID",
+                "OPPONENT_TEAM_ID",
+                "PTS",
+                "WL",
+            ]
+        ]
+        df.rename(columns={"PTS": "POINTS", "WL": "TARGET"}, inplace=True)
+        df["GAME_DATE"] = pd.to_datetime(df["GAME_DATE"], errors="coerce")
+        df["TARGET"] = df["TARGET"].map({"W": 1, "L": 0})
+
+        logger.info("Fetched %d games for season %s.", len(df), season)
+        return df
+    except Exception as e:
+        logger.error("Error fetching season %s: %s", season, e)
+        return pd.DataFrame(columns=EXPECTED_COLS)
+
+
+def fetch_today_games() -> pd.DataFrame:
+    """Fetch today's NBA games using the scoreboard endpoint."""
+    today_str = datetime.now().strftime("%Y-%m-%d")
+    logger.info("Fetching today's games for %s...", today_str)
+    try:
+        sb = scoreboard.Scoreboard(game_date=today_str)
+        games = sb.get_data_frames()[0]
+    except Exception as e:
+        logger.error("Error fetching today's scoreboard: %s", e)
+        return pd.DataFrame(columns=EXPECTED_TODAY_COLS)
+
+    if games.empty:
+        logger.info("No NBA games today.")
+        return pd.DataFrame(columns=EXPECTED_TODAY_COLS)
+
+    games = games.rename(columns={"VISITOR_TEAM_ID": "AWAY_TEAM_ID"})
+    games = games[["GAME_ID", "HOME_TEAM_ID", "AWAY_TEAM_ID"]]
+
+    logger.info("Fetched %d games for %s.", len(games), today_str)
+    return games
+
+
+def fetch_games(season: str = None) -> pd.DataFrame:
+    """
+    Combined helper: fetch either today's games (if season is None) or all games for a season.
+
+    Args:
+        season (str, optional): NBA season string (e.g., '2022-23'). If None, fetch today's games.
+
+    Returns:
+        pd.DataFrame: Games data with consistent schema.
+    """
+    if season is None:
+        return fetch_today_games()
+    return fetch_season_games(season)
Index: src/api/nba_api_client.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/src/api/nba_api_client.py b/src/api/nba_api_client.py
new file mode 100644
--- /dev/null	(date 1765470426242)
+++ b/src/api/nba_api_client.py	(date 1765470426242)
@@ -0,0 +1,161 @@
+# ============================================================
+# File: src/api/nba_api_client.py
+# Purpose: Fetch NBA data (season schedule, boxscores, today’s games, next game day)
+# ============================================================
+
+import logging
+import datetime
+import pandas as pd
+from nba_api.stats.endpoints import (
+    leaguegamefinder,
+    boxscoretraditionalv2,
+    teamdetails,
+    scoreboardv2,
+)
+
+logger = logging.getLogger("api.nba_api_client")
+
+
+# --- Helper: TEAM_ID → Abbreviation ---
+def get_team_abbreviation(team_id: int) -> str:
+    """Map TEAM_ID to team abbreviation (e.g., BOS, LAL)."""
+    try:
+        details = teamdetails.TeamDetails(team_id=team_id)
+        df = details.get_data_frames()[0]
+        return df.loc[0, "ABBREVIATION"]
+    except Exception as e:
+        logger.warning("Failed to fetch team abbreviation for %s: %s", team_id, e)
+        return str(team_id)
+
+
+# --- Season Games ---
+def fetch_season_games(season: int) -> pd.DataFrame:
+    """Fetch all games for a season with WL outcomes and team abbreviations."""
+    gamefinder = leaguegamefinder.LeagueGameFinder(season_nullable=season)
+    df = gamefinder.get_data_frames()[0]
+
+    if "TEAM_ID" in df.columns:
+        df["TEAM_ABBREVIATION"] = df["TEAM_ID"].apply(get_team_abbreviation)
+
+    return df
+
+
+# --- Boxscores ---
+def fetch_boxscores(game_ids: list[str]) -> pd.DataFrame:
+    """Fetch boxscores for a list of game IDs, including points scored and allowed."""
+    all_boxscores = []
+    for gid in game_ids:
+        try:
+            box = boxscoretraditionalv2.BoxScoreTraditionalV2(game_id=gid)
+            df_box = box.get_data_frames()[1]  # team stats table
+            df_box = df_box[["GAME_ID", "TEAM_ID", "PTS"]]
+            all_boxscores.append(df_box)
+        except Exception as e:
+            logger.warning("Failed to fetch boxscore for %s: %s", gid, e)
+
+    if not all_boxscores:
+        return pd.DataFrame()
+
+    boxscores = pd.concat(all_boxscores, ignore_index=True)
+    boxscores["PTS_OPP"] = boxscores.groupby("GAME_ID")["PTS"].transform(
+        lambda x: x[::-1].values
+    )
+    boxscores["TEAM_ABBREVIATION"] = boxscores["TEAM_ID"].apply(get_team_abbreviation)
+    return boxscores
+
+
+# --- Today’s Games with fallback ---
+def fetch_today_games(date: str | None = None) -> pd.DataFrame:
+    """
+    Fetch today's NBA games with team abbreviations.
+    If no games today, check NBA Cup schedule or next game day.
+    Adds a column GAME_TYPE = 'Regular Season' or 'NBA Cup'.
+    """
+    if date is None:
+        date = datetime.date.today().strftime("%Y-%m-%d")
+
+    try:
+        scoreboard = scoreboardv2.ScoreboardV2(game_date=date)
+        games = scoreboard.get_data_frames()[0]
+
+        required_cols = ["GAME_ID", "GAME_DATE_EST", "HOME_TEAM_ID", "VISITOR_TEAM_ID"]
+        games = games[[c for c in required_cols if c in games.columns]]
+
+        if not games.empty:
+            team_map = {}
+            for tid in pd.concat(
+                [games["HOME_TEAM_ID"], games["VISITOR_TEAM_ID"]]
+            ).unique():
+                team_map[tid] = get_team_abbreviation(int(tid))
+            games["HOME_TEAM_ABBREVIATION"] = games["HOME_TEAM_ID"].map(team_map)
+            games["AWAY_TEAM_ABBREVIATION"] = games["VISITOR_TEAM_ID"].map(team_map)
+
+            # Label as Regular Season by default
+            games["GAME_TYPE"] = "Regular Season"
+            return games[
+                [
+                    "GAME_ID",
+                    "GAME_DATE_EST",
+                    "HOME_TEAM_ABBREVIATION",
+                    "AWAY_TEAM_ABBREVIATION",
+                    "GAME_TYPE",
+                ]
+            ]
+
+        # --- Fallback: look ahead to next game day ---
+        logger.info("No games today. Searching for next scheduled game day...")
+        next_date = datetime.date.today()
+        for _ in range(7):  # look ahead up to a week
+            next_date += datetime.timedelta(days=1)
+            try:
+                sb_next = scoreboardv2.ScoreboardV2(
+                    game_date=next_date.strftime("%Y-%m-%d")
+                )
+                games_next = sb_next.get_data_frames()[0]
+                games_next = games_next[
+                    [c for c in required_cols if c in games_next.columns]
+                ]
+                if not games_next.empty:
+                    team_map = {}
+                    for tid in pd.concat(
+                        [games_next["HOME_TEAM_ID"], games_next["VISITOR_TEAM_ID"]]
+                    ).unique():
+                        team_map[tid] = get_team_abbreviation(int(tid))
+                    games_next["HOME_TEAM_ABBREVIATION"] = games_next[
+                        "HOME_TEAM_ID"
+                    ].map(team_map)
+                    games_next["AWAY_TEAM_ABBREVIATION"] = games_next[
+                        "VISITOR_TEAM_ID"
+                    ].map(team_map)
+
+                    # Detect NBA Cup by date range (Dec 9–16, 2025 for this season)
+                    cup_start = datetime.date(2025, 12, 9)
+                    cup_end = datetime.date(2025, 12, 16)
+                    if cup_start <= next_date <= cup_end:
+                        games_next["GAME_TYPE"] = "NBA Cup"
+                    else:
+                        games_next["GAME_TYPE"] = "Regular Season"
+
+                    logger.info(
+                        "Next NBA game day is %s (%s)",
+                        next_date.strftime("%Y-%m-%d"),
+                        games_next["GAME_TYPE"].iloc[0],
+                    )
+                    return games_next[
+                        [
+                            "GAME_ID",
+                            "GAME_DATE_EST",
+                            "HOME_TEAM_ABBREVIATION",
+                            "AWAY_TEAM_ABBREVIATION",
+                            "GAME_TYPE",
+                        ]
+                    ]
+            except Exception:
+                continue
+
+        logger.warning("No NBA games found in the next 7 days.")
+        return pd.DataFrame()
+
+    except Exception as e:
+        logger.error("Failed to fetch today’s games: %s", e)
+        return pd.DataFrame()
Index: mlruns/0/808f8a1b6d9a4823a195c994a6062b1f/tags/mlflow.runName
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/mlruns/0/808f8a1b6d9a4823a195c994a6062b1f/tags/mlflow.runName b/mlruns/0/808f8a1b6d9a4823a195c994a6062b1f/tags/mlflow.runName
new file mode 100644
--- /dev/null	(date 1765470426224)
+++ b/mlruns/0/808f8a1b6d9a4823a195c994a6062b1f/tags/mlflow.runName	(date 1765470426224)
@@ -0,0 +1,1 @@
+xgb_pipeline_v1.3
\ No newline at end of file
Index: mlruns/0/808f8a1b6d9a4823a195c994a6062b1f/meta.yaml
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/mlruns/0/808f8a1b6d9a4823a195c994a6062b1f/meta.yaml b/mlruns/0/808f8a1b6d9a4823a195c994a6062b1f/meta.yaml
new file mode 100644
--- /dev/null	(date 1765470426223)
+++ b/mlruns/0/808f8a1b6d9a4823a195c994a6062b1f/meta.yaml	(date 1765470426223)
@@ -0,0 +1,15 @@
+artifact_uri: file:///C:/Users/Mohamadou/PycharmProjects/nba_analytics/mlruns/0/808f8a1b6d9a4823a195c994a6062b1f/artifacts
+end_time: 1765311967972
+entry_point_name: ''
+experiment_id: '0'
+lifecycle_stage: active
+run_id: 808f8a1b6d9a4823a195c994a6062b1f
+run_name: xgb_pipeline_v1.3
+run_uuid: 808f8a1b6d9a4823a195c994a6062b1f
+source_name: ''
+source_type: 4
+source_version: ''
+start_time: 1765311967934
+status: 4
+tags: []
+user_id: Mohamadou
Index: mlruns/0/808f8a1b6d9a4823a195c994a6062b1f/tags/mlflow.source.name
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/mlruns/0/808f8a1b6d9a4823a195c994a6062b1f/tags/mlflow.source.name b/mlruns/0/808f8a1b6d9a4823a195c994a6062b1f/tags/mlflow.source.name
new file mode 100644
--- /dev/null	(date 1765470426225)
+++ b/mlruns/0/808f8a1b6d9a4823a195c994a6062b1f/tags/mlflow.source.name	(date 1765470426225)
@@ -0,0 +1,1 @@
+run_pipeline.py
\ No newline at end of file
Index: mlruns/0/808f8a1b6d9a4823a195c994a6062b1f/tags/mlflow.source.git.commit
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/mlruns/0/808f8a1b6d9a4823a195c994a6062b1f/tags/mlflow.source.git.commit b/mlruns/0/808f8a1b6d9a4823a195c994a6062b1f/tags/mlflow.source.git.commit
new file mode 100644
--- /dev/null	(date 1765470426224)
+++ b/mlruns/0/808f8a1b6d9a4823a195c994a6062b1f/tags/mlflow.source.git.commit	(date 1765470426224)
@@ -0,0 +1,1 @@
+ba58bcf9c077cd4a4662500f7d7cc2c58a4ec45c
\ No newline at end of file
Index: mlruns/0/808f8a1b6d9a4823a195c994a6062b1f/tags/mlflow.user
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/mlruns/0/808f8a1b6d9a4823a195c994a6062b1f/tags/mlflow.user b/mlruns/0/808f8a1b6d9a4823a195c994a6062b1f/tags/mlflow.user
new file mode 100644
--- /dev/null	(date 1765470426226)
+++ b/mlruns/0/808f8a1b6d9a4823a195c994a6062b1f/tags/mlflow.user	(date 1765470426226)
@@ -0,0 +1,1 @@
+Mohamadou
\ No newline at end of file
Index: mlruns/0/808f8a1b6d9a4823a195c994a6062b1f/tags/mlflow.source.type
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/mlruns/0/808f8a1b6d9a4823a195c994a6062b1f/tags/mlflow.source.type b/mlruns/0/808f8a1b6d9a4823a195c994a6062b1f/tags/mlflow.source.type
new file mode 100644
--- /dev/null	(date 1765470426226)
+++ b/mlruns/0/808f8a1b6d9a4823a195c994a6062b1f/tags/mlflow.source.type	(date 1765470426226)
@@ -0,0 +1,1 @@
+LOCAL
\ No newline at end of file
Index: src/prediction_engine/daily_runner_with_tracker.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/src/prediction_engine/daily_runner_with_tracker.py b/src/prediction_engine/daily_runner_with_tracker.py
new file mode 100644
--- /dev/null	(date 1765470426254)
+++ b/src/prediction_engine/daily_runner_with_tracker.py	(date 1765470426254)
@@ -0,0 +1,49 @@
+# ============================================================
+# File: src/prediction_engine/daily_runner.py
+# Purpose: Canonical daily runner for NBA predictions
+# Project: nba_analysis
+# Version: 2.0 (returns features, predictions, player info)
+# ============================================================
+
+import logging
+import pandas as pd
+
+from src.api.nba_api_client import fetch_season_games
+from src.features.feature_engineering import generate_features_for_games
+from src.prediction_engine.predictor import Predictor
+
+logger = logging.getLogger("prediction_engine.daily_runner")
+if not logger.handlers:
+    handler = logging.StreamHandler()
+    handler.setFormatter(logging.Formatter("%(asctime)s [%(levelname)s] %(message)s"))
+    logger.addHandler(handler)
+    logger.setLevel(logging.INFO)
+
+
+def run_daily_predictions(model_path, season=2025, limit=10):
+    """Fetch games, generate features, run predictions, return DataFrames."""
+    games = fetch_season_games(season, limit=limit)
+    if games is None or games.empty:
+        logger.warning("No games found for season %s", season)
+        return None, None, None
+
+    features = generate_features_for_games(games.to_dict(orient="records"))
+    if features is None or features.empty:
+        logger.warning("No features generated.")
+        return None, None, None
+
+    predictor = Predictor(model_path=model_path)
+    X = features.drop(columns=["win"], errors="ignore")
+
+    predictions_df = pd.DataFrame(
+        {
+            "GAME_ID": features["GAME_ID"],
+            "win_proba": predictor.predict_proba(X),
+            "win_pred": predictor.predict_label(X),
+        }
+    )
+
+    # Placeholder: player info could come from another API or join
+    player_info_df = pd.DataFrame(columns=["GAME_ID", "TEAM_ID", "PlayerNames"])
+
+    return features, predictions_df, player_info_df
Index: src/prediction_engine/daily_runner.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/src/prediction_engine/daily_runner.py b/src/prediction_engine/daily_runner.py
new file mode 100644
--- /dev/null	(date 1765470426252)
+++ b/src/prediction_engine/daily_runner.py	(date 1765470426252)
@@ -0,0 +1,89 @@
+# ============================================================
+# File: src/prediction_engine/daily_runner.py
+# Purpose: Core logic for daily predictions
+# ============================================================
+
+import logging
+import pandas as pd
+from nba_api.stats.endpoints import commonteamroster
+from src.api.nba_api_client import fetch_today_games
+from src.model_training.utils import load_model, build_features
+
+logger = logging.getLogger("prediction_engine.daily_runner")
+
+
+def fetch_team_roster(team_id: int, season: int) -> pd.DataFrame:
+    """
+    Fetch roster info for a given team and season.
+    Returns DataFrame with PLAYER_ID, PLAYER, POSITION, HEIGHT, WEIGHT, BIRTH_DATE, EXP.
+    """
+    try:
+        roster = commonteamroster.CommonTeamRoster(team_id=team_id, season=season)
+        df = roster.get_data_frames()[0]
+        return df[
+            ["PLAYER_ID", "PLAYER", "POSITION", "HEIGHT", "WEIGHT", "BIRTH_DATE", "EXP"]
+        ]
+    except Exception as e:
+        logger.warning("Failed to fetch roster for team %s: %s", team_id, e)
+        return pd.DataFrame()
+
+
+def run_daily_predictions(model: str, season: int, limit: int = 10):
+    """
+    Run daily predictions.
+    Always returns 3 DataFrames: (features_df, predictions_df, player_info_df).
+    If no games are found, returns empty DataFrames.
+    """
+    try:
+        # --- Fetch today's games (or next available) ---
+        games_df = fetch_today_games()
+        if games_df.empty:
+            logger.warning("No games found today. Returning empty DataFrames.")
+            return pd.DataFrame(), pd.DataFrame(), pd.DataFrame()
+
+        # --- Build features ---
+        features_df = build_features(games_df, season=season)
+
+        # --- Load model ---
+        model_obj = load_model(model)
+
+        # --- Generate predictions ---
+        y_pred = model_obj.predict(features_df)
+        y_prob = None
+        try:
+            y_prob = model_obj.predict_proba(features_df)[:, 1]
+        except Exception:
+            pass
+
+        predictions_df = games_df.copy()
+        predictions_df["PREDICTED_WIN"] = y_pred
+        if y_prob is not None:
+            predictions_df["WIN_PROBABILITY"] = y_prob
+
+        # --- Player info (rosters for all teams in games) ---
+        player_info_frames = []
+        if "HOME_TEAM_ID" in games_df.columns and "VISITOR_TEAM_ID" in games_df.columns:
+            team_ids = pd.concat(
+                [games_df["HOME_TEAM_ID"], games_df["VISITOR_TEAM_ID"]]
+            ).unique()
+            for team_id in team_ids:
+                roster_df = fetch_team_roster(int(team_id), season)
+                if not roster_df.empty:
+                    roster_df["TEAM_ID"] = team_id
+                    player_info_frames.append(roster_df)
+
+        player_info_df = (
+            pd.concat(player_info_frames, ignore_index=True)
+            if player_info_frames
+            else pd.DataFrame()
+        )
+
+        # --- Limit results ---
+        if limit:
+            predictions_df = predictions_df.head(limit)
+
+        return features_df, predictions_df, player_info_df
+
+    except Exception as e:
+        logger.error("Daily predictions failed: %s", e)
+        return pd.DataFrame(), pd.DataFrame(), pd.DataFrame()
Index: src/prediction_engine/daily_runner_cli.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/src/prediction_engine/daily_runner_cli.py b/src/prediction_engine/daily_runner_cli.py
new file mode 100644
--- /dev/null	(date 1765470426253)
+++ b/src/prediction_engine/daily_runner_cli.py	(date 1765470426253)
@@ -0,0 +1,100 @@
+# ============================================================
+# File: src/prediction_engine/daily_runner_cli.py
+# Purpose: CLI for running daily predictions
+# ============================================================
+
+import logging
+import click
+import os
+import pandas as pd
+from src.prediction_engine.daily_runner import run_daily_predictions
+
+logger = logging.getLogger("prediction_engine.daily_runner_cli")
+logging.basicConfig(level=logging.INFO)
+
+
+@click.command()
+@click.option(
+    "--model",
+    required=True,
+    help="Path to trained model file (e.g., models/nba_xgb.pkl)",
+)
+@click.option("--season", required=True, type=int, help="Season year (e.g., 2025)")
+@click.option("--limit", default=10, help="Limit number of games to predict")
+@click.option(
+    "--out", default="data/results/daily_predictions.csv", help="Output CSV path"
+)
+@click.option(
+    "--fmt", default="csv", type=click.Choice(["csv", "parquet"]), help="Output format"
+)
+def cli(model, season, limit, out, fmt):
+    logger.info("Starting daily runner...")
+
+    try:
+        features_df, predictions_df, player_info_df = run_daily_predictions(
+            model=model, season=season, limit=limit
+        )
+
+        # Ensure results folder exists
+        os.makedirs(os.path.dirname(out), exist_ok=True)
+
+        # --- Save predictions ---
+        if predictions_df.empty:
+            logger.warning(
+                "No predictions generated (no games today or API timeout). Writing empty file."
+            )
+            empty_cols = [
+                "GAME_ID",
+                "GAME_DATE_EST",
+                "HOME_TEAM_ABBREVIATION",
+                "AWAY_TEAM_ABBREVIATION",
+                "PREDICTED_WIN",
+                "WIN_PROBABILITY",
+                "GAME_TYPE",
+            ]
+            pd.DataFrame(columns=empty_cols).to_csv(out, index=False)
+        else:
+            if fmt == "csv":
+                predictions_df.to_csv(out, index=False)
+            else:
+                predictions_df.to_parquet(out, index=False)
+            logger.info("Predictions saved to %s", out)
+
+            # Print next available game day info
+            next_date = predictions_df["GAME_DATE_EST"].iloc[0]
+            game_type = (
+                predictions_df["GAME_TYPE"].iloc[0]
+                if "GAME_TYPE" in predictions_df.columns
+                else "Unknown"
+            )
+            logger.info("Next NBA game day is %s (%s)", next_date, game_type)
+
+        # --- Save player info ---
+        player_info_out = "data/results/player_info.csv"
+        os.makedirs(os.path.dirname(player_info_out), exist_ok=True)
+        if not player_info_df.empty:
+            player_info_df.to_csv(player_info_out, index=False)
+            logger.info("Player info saved to %s", player_info_out)
+        else:
+            pd.DataFrame(
+                columns=[
+                    "PLAYER_ID",
+                    "PLAYER",
+                    "POSITION",
+                    "HEIGHT",
+                    "WEIGHT",
+                    "BIRTH_DATE",
+                    "EXP",
+                    "TEAM_ID",
+                ]
+            ).to_csv(player_info_out, index=False)
+            logger.warning(
+                "No player info available. Empty file written to %s", player_info_out
+            )
+
+    except Exception as e:
+        logger.error("Daily runner failed: %s", e)
+
+
+if __name__ == "__main__":
+    cli()
Index: src/interpretability/shap_analysis.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/src/interpretability/shap_analysis.py b/src/interpretability/shap_analysis.py
new file mode 100644
--- /dev/null	(date 1765470426247)
+++ b/src/interpretability/shap_analysis.py	(date 1765470426247)
@@ -0,0 +1,84 @@
+# ============================================================
+# File: src/interpretability/shap_analysis.py
+# Purpose: SHAP interpretability for XGBoost model
+# Version: 1.3 (numeric coercion, safer SHAP calls, metrics logging)
+# ============================================================
+
+import os
+import logging
+import joblib
+import matplotlib.pyplot as plt
+import mlflow
+import pandas as pd
+import shap
+import numpy as np
+
+logger = logging.getLogger("interpretability.shap")
+
+
+def run_shap(
+    model_path, cache_file, out_dir="results/interpretability", top_n=3, clf_step="clf"
+):
+    """
+    Run SHAP analysis for an XGBoost model inside a sklearn pipeline.
+    Args:
+        model_path (str): Path to saved pipeline (joblib).
+        cache_file (str): Path to cached dataset (parquet or csv).
+        out_dir (str): Directory to save SHAP plots.
+        top_n (int): Number of top features for dependence plots.
+        clf_step (str): Name of classifier step in pipeline.
+    """
+    os.makedirs(out_dir, exist_ok=True)
+
+    pipeline = joblib.load(model_path)
+    if clf_step not in pipeline.named_steps:
+        raise KeyError(
+            f"Classifier step '{clf_step}' not found in pipeline steps: {list(pipeline.named_steps.keys())}"
+        )
+    xgb_model = pipeline.named_steps[clf_step]
+
+    # Load dataset
+    if cache_file.endswith(".parquet"):
+        df = pd.read_parquet(cache_file)
+    elif cache_file.endswith(".csv"):
+        df = pd.read_csv(cache_file)
+    else:
+        raise ValueError(f"Unsupported cache file format: {cache_file}")
+
+    # Drop target column(s) and coerce all features to numeric
+    X = df.drop(columns=["target", "TARGET"], errors="ignore")
+    X = X.apply(pd.to_numeric, errors="coerce")
+
+    explainer = shap.TreeExplainer(xgb_model)
+    shap_values = explainer(X)
+
+    # Summary plot
+    summary_path = os.path.join(out_dir, "shap_summary.png")
+    shap.summary_plot(shap_values, X, show=False)
+    plt.savefig(summary_path, bbox_inches="tight")
+    plt.close()
+
+    # Top features by mean absolute SHAP
+    mean_abs = pd.Series(
+        np.abs(np.asarray(shap_values.values)).mean(axis=0), index=X.columns
+    ).sort_values(ascending=False)
+    top_features = mean_abs.index[:top_n].tolist()
+    dep_paths = []
+
+    for feat in top_features:
+        dep_path = os.path.join(out_dir, f"shap_dependence_{feat}.png")
+        shap.dependence_plot(feat, shap_values, X, show=False)
+        plt.savefig(dep_path, bbox_inches="tight")
+        plt.close()
+        dep_paths.append(dep_path)
+
+    # Log to MLflow (nested run)
+    with mlflow.start_run(run_name="shap_analysis", nested=True):
+        mlflow.log_artifact(summary_path, artifact_path="interpretability")
+        for p in dep_paths:
+            mlflow.log_artifact(p, artifact_path="interpretability")
+        # Log mean SHAP values for top features
+        for feat in top_features:
+            mlflow.log_metric(f"shap_mean_abs_{feat}", float(mean_abs[feat]))
+
+    logger.info("✅ SHAP report logged. Top features: %s", top_features)
Index: .github/workflows/daily_ci.yml
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/.github/workflows/daily_ci.yml b/.github/workflows/daily_ci.yml
new file mode 100644
--- /dev/null	(date 1765470426208)
+++ b/.github/workflows/daily_ci.yml	(date 1765470426208)
@@ -0,0 +1,62 @@
+name: Nightly Retrain, Predictions, and Comparison
+
+on:
+  schedule:
+    - cron: "0 6 * * *"   # every day at 6:00 UTC
+  workflow_dispatch:
+
+jobs:
+  retrain_and_predict:
+    runs-on: ubuntu-latest
+    steps:
+      - name: Checkout repo
+        uses: actions/checkout@v3
+
+      - name: Set up Python
+        uses: actions/setup-python@v4
+        with:
+          python-version: '3.11'
+
+      - name: Install dependencies
+        run: pip install -r requirements.txt
+
+      - name: Enrich schedule
+        run: python -m src.scripts.enrich_schedule --season 2025
+
+      - name: Generate features
+        run: python -m src.scripts.generate_features
+
+      - name: Retrain model and log metrics
+        run: |
+          python -m src.model_training.trainer_cli \
+            --model xgb \
+            --season 2025 \
+            --features data/cache/features_full.parquet \
+            --out models/nba_xgb.pkl \
+            --metrics_out data/results/model_metrics.csv \
+            --importance_out data/results/feature_importance.csv
+
+      - name: Generate predictions and player info
+        run: |
+          python -m src.prediction_engine.daily_runner_cli \
+            --model models/nba_xgb.pkl \
+            --season 2025 \
+            --limit 10 \
+            --out data/results/daily_predictions.csv \
+            --fmt csv
+
+      - name: Compare algorithms
+        run: python -m src.model_training.compare_algorithms
+
+      - name: Commit updated results
+        run: |
+          git config --global user.name "github-actions"
+          git config --global user.email "actions@github.com"
+          git add models/nba_xgb.pkl \
+                 data/results/daily_predictions.csv \
+                 data/results/player_info.csv \
+                 data/results/model_metrics.csv \
+                 data/results/feature_importance.csv \
+                 data/results/model_comparison.csv
+          git commit -m "chore: nightly retrain, predictions, player info, and algorithm comparison" || echo "No changes to commit"
+          git push
Index: tests/test_io.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+># ============================================================\n# Path: tests/test_io.py\n# Purpose: Unit tests for src/utils/io.py\n# Project: nba_analysis\n# ============================================================\n\nimport pandas as pd\nfrom src.utils.io import safe_save, safe_load, safe_delete, safe_exists\n\n\ndef test_safe_exists_and_delete(tmp_path):\n    df = pd.DataFrame({\"a\": [1]})\n    fpath = tmp_path / \"exists.csv\"\n    safe_save(df, fpath)\n\n    # File should exist\n    assert safe_exists(fpath)\n\n    # Delete file\n    safe_delete(fpath)\n\n    # File should not exist\n    assert not safe_exists(fpath)\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/tests/test_io.py b/tests/test_io.py
--- a/tests/test_io.py	(revision 00055356329a845d3b2ca81b77c3ffb2e2b15969)
+++ b/tests/test_io.py	(date 1765470426272)
@@ -1,23 +1,63 @@
 # ============================================================
-# Path: tests/test_io.py
-# Purpose: Unit tests for src/utils/io.py
+# File: tests/test_io.py
+# Purpose: Validate DataFrame I/O utilities (load, save, read_or_create)
 # Project: nba_analysis
 # ============================================================
 
+import os
 import pandas as pd
-from src.utils.io import safe_save, safe_load, safe_delete, safe_exists
+import pytest
+
+from src.utils.io import load_dataframe, save_dataframe, read_or_create
+
+
+def test_save_and_load_csv(tmp_path):
+    df = pd.DataFrame({"A": [1, 2], "B": ["x", "y"]})
+    path = tmp_path / "test.csv"
+
+    # Save and load
+    save_dataframe(df, str(path))
+    loaded = load_dataframe(str(path))
+
+    pd.testing.assert_frame_equal(df, loaded)
+
+
+def test_save_and_load_parquet(tmp_path):
+    df = pd.DataFrame({"A": [1, 2], "B": ["x", "y"]})
+    path = tmp_path / "test.parquet"
 
+    save_dataframe(df, str(path))
+    loaded = load_dataframe(str(path))
 
-def test_safe_exists_and_delete(tmp_path):
-    df = pd.DataFrame({"a": [1]})
-    fpath = tmp_path / "exists.csv"
-    safe_save(df, fpath)
+    pd.testing.assert_frame_equal(df, loaded)
 
-    # File should exist
-    assert safe_exists(fpath)
 
-    # Delete file
-    safe_delete(fpath)
+def test_load_dataframe_file_not_found():
+    with pytest.raises(FileNotFoundError):
+        load_dataframe("nonexistent.csv")
 
-    # File should not exist
-    assert not safe_exists(fpath)
+
+def test_save_dataframe_invalid_type(tmp_path):
+    path = tmp_path / "bad.csv"
+    with pytest.raises(TypeError):
+        save_dataframe([1, 2, 3], str(path))
+
+
+def test_read_or_create_creates_file(tmp_path):
+    default_df = pd.DataFrame({"GAME_ID": [], "TEAM_ID": [], "PTS": []})
+    path = tmp_path / "games.csv"
+
+    # File does not exist, should create
+    df = read_or_create(str(path), default_df)
+    assert df.equals(default_df)
+    assert os.path.exists(path)
+
+
+def test_read_or_create_loads_existing(tmp_path):
+    df = pd.DataFrame({"A": [1], "B": [2]})
+    path = tmp_path / "existing.csv"
+
+    df.to_csv(path, index=False)
+
+    loaded = read_or_create(str(path), pd.DataFrame())
+    pd.testing.assert_frame_equal(df, loaded)
Index: src/analytics/rankings.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+># ============================================================\n# Path: src/analytics/rankings.py\n# Purpose: Track top teams/players, betting recommendations,\n#          and winning streaks from prediction history + box scores\n# ============================================================\n\nimport pandas as pd\nimport os\n\ndef compute_team_rankings(history_file, out_dir=\"data/analytics\"):\n    \"\"\"\n    Compute top 6 performing teams per conference.\n    Metrics: win %, prediction accuracy, avg predicted probability.\n    \"\"\"\n    df = pd.read_parquet(history_file)\n\n    # Actual win % per team\n    team_stats = df.groupby(\"TEAM_ID\").agg(\n        games_played=(\"GAME_ID\", \"count\"),\n        wins=(\"win\", \"sum\"),\n        avg_pred_proba=(\"pred_proba\", \"mean\"),\n        pred_accuracy=(\"pred_label\", lambda x: (x == df.loc[x.index, \"win\"]).mean())\n    ).reset_index()\n\n    team_stats[\"win_pct\"] = team_stats[\"wins\"] / team_stats[\"games_played\"]\n\n    # Add conference info (requires mapping TEAM_ID → conference)\n    # Example: load mapping file\n    conf_map = pd.read_csv(\"data/raw/team_conference_map.csv\")  # TEAM_ID, Conference\n    team_stats = team_stats.merge(conf_map, on=\"TEAM_ID\", how=\"left\")\n\n    # Top 6 per conference\n    top_teams = team_stats.sort_values([\"Conference\", \"win_pct\"], ascending=[True, False])\n    top_teams = top_teams.groupby(\"Conference\").head(6)\n\n    os.makedirs(out_dir, exist_ok=True)\n    top_teams.to_parquet(os.path.join(out_dir, \"team_rankings.parquet\"), index=False)\n    top_teams.to_csv(os.path.join(out_dir, \"team_rankings.csv\"), index=False)\n\n    return top_teams\n\n\ndef compute_player_rankings(player_box_file, out_dir=\"data/analytics\"):\n    \"\"\"\n    Compute top 6 performing players per conference.\n    Metrics: avg points, consistency, prop hit rate (20+ pts).\n    \"\"\"\n    df = pd.read_parquet(player_box_file)\n\n    player_stats = df.groupby(\"PLAYER_ID\").agg(\n        avg_points=(\"PTS\", \"mean\"),\n        games_played=(\"GAME_ID\", \"count\"),\n        prop_hit_rate=(\"PTS\", lambda x: (x >= 20).mean()),\n        consistency=(\"PTS\", \"std\")\n    ).reset_index()\n\n    # Add conference info (requires mapping PLAYER_ID → TEAM_ID → Conference)\n    team_map = pd.read_csv(\"data/raw/team_conference_map.csv\")\n    player_team_map = df[[\"PLAYER_ID\", \"TEAM_ID\"]].drop_duplicates()\n    player_stats = player_stats.merge(player_team_map, on=\"PLAYER_ID\", how=\"left\")\n    player_stats = player_stats.merge(team_map, on=\"TEAM_ID\", how=\"left\")\n\n    # Top 6 per conference\n    top_players = player_stats.sort_values([\"Conference\", \"avg_points\"], ascending=[True, False])\n    top_players = top_players.groupby(\"Conference\").head(6)\n\n    os.makedirs(out_dir, exist_ok=True)\n    top_players.to_parquet(os.path.join(out_dir, \"player_rankings.parquet\"), index=False)\n    top_players.to_csv(os.path.join(out_dir, \"player_rankings.csv\"), index=False)\n\n    return top_players\n\n\ndef betting_recommendations(team_rankings, threshold=0.55):\n    \"\"\"\n    Generate teams to bet on vs. avoid.\n    Bet On: win_pct > threshold and pred_accuracy > threshold\n    Avoid: win_pct < threshold or pred_accuracy < threshold\n    \"\"\"\n    bet_on = team_rankings[\n        (team_rankings[\"win_pct\"] > threshold) &\n        (team_rankings[\"pred_accuracy\"] > threshold)\n    ][\"TEAM_ID\"].tolist()\n\n    avoid = team_rankings[\n        (team_rankings[\"win_pct\"] < threshold) |\n        (team_rankings[\"pred_accuracy\"] < threshold)\n    ][\"TEAM_ID\"].tolist()\n\n    return {\"bet_on\": bet_on, \"avoid\": avoid}\n\n\ndef track_winning_streaks(history_file, out_dir=\"data/analytics\"):\n    \"\"\"\n    Track team winning streaks from history.\n    \"\"\"\n    df = pd.read_parquet(history_file)\n    streaks = []\n\n    for team_id, group in df.groupby(\"TEAM_ID\"):\n        group = group.sort_values(\"prediction_date\")\n        current_streak = 0\n        max_streak = 0\n        for win in group[\"win\"]:\n            if win == 1:\n                current_streak += 1\n                max_streak = max(max_streak, current_streak)\n            else:\n                current_streak = 0\n        streaks.append({\"TEAM_ID\": team_id, \"max_streak\": max_streak})\n\n    streaks_df = pd.DataFrame(streaks)\n    os.makedirs(out_dir, exist_ok=True)\n    streaks_df.to_parquet(os.path.join(out_dir, \"winning_streaks.parquet\"), index=False)\n    streaks_df.to_csv(os.path.join(out_dir, \"winning_streaks.csv\"), index=False)\n\n    return streaks_df\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/src/analytics/rankings.py b/src/analytics/rankings.py
--- a/src/analytics/rankings.py	(revision 00055356329a845d3b2ca81b77c3ffb2e2b15969)
+++ b/src/analytics/rankings.py	(date 1765470426241)
@@ -1,117 +1,155 @@
 # ============================================================
-# Path: src/analytics/rankings.py
+# File: src/analytics/rankings.py
 # Purpose: Track top teams/players, betting recommendations,
 #          and winning streaks from prediction history + box scores
+# Version: 4.2 (fixes MLflow logging, centralized logger, validation)
 # ============================================================
 
-import pandas as pd
 import os
+import logging
+from typing import Dict, Optional
 
-def compute_team_rankings(history_file, out_dir="data/analytics"):
-    """
-    Compute top 6 performing teams per conference.
-    Metrics: win %, prediction accuracy, avg predicted probability.
-    """
-    df = pd.read_parquet(history_file)
+import pandas as pd
+import mlflow
+
+from mlflow_setup import configure_mlflow
+from src.config_schema import load_config
 
-    # Actual win % per team
-    team_stats = df.groupby("TEAM_ID").agg(
-        games_played=("GAME_ID", "count"),
-        wins=("win", "sum"),
-        avg_pred_proba=("pred_proba", "mean"),
-        pred_accuracy=("pred_label", lambda x: (x == df.loc[x.index, "win"]).mean())
-    ).reset_index()
 
-    team_stats["win_pct"] = team_stats["wins"] / team_stats["games_played"]
+class RankingsManager:
+    def __init__(self, config_file: str = "config.yaml") -> None:
+        # Load validated config
+        self.cfg = load_config(config_file)
 
-    # Add conference info (requires mapping TEAM_ID → conference)
-    # Example: load mapping file
-    conf_map = pd.read_csv("data/raw/team_conference_map.csv")  # TEAM_ID, Conference
-    team_stats = team_stats.merge(conf_map, on="TEAM_ID", how="left")
+        # Unpack config sections
+        self.paths = self.cfg.paths
+        self.nba_cfg = self.cfg.nba
+        self.output_cfg = self.cfg.output
+        self.logging_cfg = self.cfg.logging
+        self.mlflow_cfg = self.cfg.mlflow
+        self.betting_cfg = getattr(self.cfg, "betting", None)
 
-    # Top 6 per conference
-    top_teams = team_stats.sort_values(["Conference", "win_pct"], ascending=[True, False])
-    top_teams = top_teams.groupby("Conference").head(6)
+        # Ensure directories exist
+        os.makedirs(self.paths.history, exist_ok=True)
+        os.makedirs(self.paths.raw, exist_ok=True)
+        os.makedirs(self.paths.analytics, exist_ok=True)
 
-    os.makedirs(out_dir, exist_ok=True)
-    top_teams.to_parquet(os.path.join(out_dir, "team_rankings.parquet"), index=False)
-    top_teams.to_csv(os.path.join(out_dir, "team_rankings.csv"), index=False)
+        # Use a named logger; avoid basicConfig clashes
+        self.logger = logging.getLogger("analytics.rankings")
+        if not self.logger.handlers:
+            handler = logging.FileHandler(self.logging_cfg.file)
+            handler.setFormatter(
+                logging.Formatter("%(asctime)s [%(levelname)s] %(message)s")
+            )
+            self.logger.addHandler(handler)
+            self.logger.setLevel(getattr(logging, self.logging_cfg.level, logging.INFO))
 
-    return top_teams
+        # Paths
+        self.conf_map_file = os.path.join(self.paths.raw, "team_conference_map.csv")
+        self.analytics_dir = self.paths.analytics
 
+        # Configure MLflow (if enabled in config)
+        self.mlflow_enabled = getattr(self.mlflow_cfg, "enabled", True)
+        if self.mlflow_enabled:
+            try:
+                configure_mlflow(self.mlflow_cfg)
+                self.logger.info(
+                    "MLflow configured: tracking_uri=%s", self.mlflow_cfg.tracking_uri
+                )
+            except Exception as e:
+                self.logger.error("Failed to configure MLflow: %s", e)
+                self.mlflow_enabled = False
 
-def compute_player_rankings(player_box_file, out_dir="data/analytics"):
-    """
-    Compute top 6 performing players per conference.
-    Metrics: avg points, consistency, prop hit rate (20+ pts).
-    """
-    df = pd.read_parquet(player_box_file)
+    # -----------------------------
+    # BETTING RECOMMENDATIONS
+    # -----------------------------
+    def betting_recommendations(
+        self, team_rankings: pd.DataFrame, threshold: Optional[float] = None
+    ) -> Dict[str, pd.DataFrame]:
+        """
+        Produce bet/avoid recommendations based on win_pct and pred_accuracy columns.
 
-    player_stats = df.groupby("PLAYER_ID").agg(
-        avg_points=("PTS", "mean"),
-        games_played=("GAME_ID", "count"),
-        prop_hit_rate=("PTS", lambda x: (x >= 20).mean()),
-        consistency=("PTS", "std")
-    ).reset_index()
+        Args:
+            team_rankings: DataFrame with at least 'win_pct' and 'pred_accuracy' columns.
+            threshold: Optional override for the decision threshold in [0, 1].
+                       If None, uses config.betting.threshold.
 
-    # Add conference info (requires mapping PLAYER_ID → TEAM_ID → Conference)
-    team_map = pd.read_csv("data/raw/team_conference_map.csv")
-    player_team_map = df[["PLAYER_ID", "TEAM_ID"]].drop_duplicates()
-    player_stats = player_stats.merge(player_team_map, on="PLAYER_ID", how="left")
-    player_stats = player_stats.merge(team_map, on="TEAM_ID", how="left")
+        Returns:
+            dict: {'bet_on': DataFrame, 'avoid': DataFrame}
+        """
+        # Validate columns
+        required_cols = {"win_pct", "pred_accuracy"}
+        missing = required_cols - set(team_rankings.columns)
+        if missing:
+            msg = f"team_rankings missing required columns: {missing}"
+            self.logger.error(msg)
+            raise ValueError(msg)
 
-    # Top 6 per conference
-    top_players = player_stats.sort_values(["Conference", "avg_points"], ascending=[True, False])
-    top_players = top_players.groupby("Conference").head(6)
+        # Resolve threshold
+        if threshold is None:
+            if (
+                not self.betting_cfg
+                or getattr(self.betting_cfg, "threshold", None) is None
+            ):
+                msg = "Betting threshold not provided and not found in config."
+                self.logger.error(msg)
+                raise ValueError(msg)
+            threshold = float(self.betting_cfg.threshold)
 
-    os.makedirs(out_dir, exist_ok=True)
-    top_players.to_parquet(os.path.join(out_dir, "player_rankings.parquet"), index=False)
-    top_players.to_csv(os.path.join(out_dir, "player_rankings.csv"), index=False)
+        # Validate threshold bounds
+        if not (0.0 <= threshold <= 1.0):
+            msg = f"Invalid threshold {threshold}. Must be within [0, 1]."
+            self.logger.error(msg)
+            raise ValueError(msg)
 
-    return top_players
-
-
-def betting_recommendations(team_rankings, threshold=0.55):
-    """
-    Generate teams to bet on vs. avoid.
-    Bet On: win_pct > threshold and pred_accuracy > threshold
-    Avoid: win_pct < threshold or pred_accuracy < threshold
-    """
-    bet_on = team_rankings[
-        (team_rankings["win_pct"] > threshold) &
-        (team_rankings["pred_accuracy"] > threshold)
-    ]["TEAM_ID"].tolist()
+        # Create views
+        bet_on = team_rankings[
+            (team_rankings["win_pct"] > threshold)
+            & (team_rankings["pred_accuracy"] > threshold)
+        ].copy()
 
-    avoid = team_rankings[
-        (team_rankings["win_pct"] < threshold) |
-        (team_rankings["pred_accuracy"] < threshold)
-    ]["TEAM_ID"].tolist()
+        avoid = team_rankings[
+            (team_rankings["win_pct"] < threshold)
+            | (team_rankings["pred_accuracy"] < threshold)
+        ].copy()
+
+        self.logger.info(
+            "Betting recommendations: bet_on=%d, avoid=%d", len(bet_on), len(avoid)
+        )
+
+        # Log to MLflow (optional)
+        self._log_to_mlflow_artifacts(
+            bet_on, "bet_on", {"count": len(bet_on), "threshold": threshold}
+        )
+        self._log_to_mlflow_artifacts(
+            avoid, "avoid", {"count": len(avoid), "threshold": threshold}
+        )
 
-    return {"bet_on": bet_on, "avoid": avoid}
+        return {"bet_on": bet_on, "avoid": avoid}
 
-
-def track_winning_streaks(history_file, out_dir="data/analytics"):
-    """
-    Track team winning streaks from history.
-    """
-    df = pd.read_parquet(history_file)
-    streaks = []
+    # -----------------------------
+    # INTERNAL: MLflow logging helper
+    # -----------------------------
+    def _log_to_mlflow_artifacts(
+        self, df: pd.DataFrame, name: str, metrics: Dict[str, float]
+    ) -> None:
+        """Log metrics and a CSV artifact to MLflow if enabled."""
+        if not self.mlflow_enabled:
+            return
 
-    for team_id, group in df.groupby("TEAM_ID"):
-        group = group.sort_values("prediction_date")
-        current_streak = 0
-        max_streak = 0
-        for win in group["win"]:
-            if win == 1:
-                current_streak += 1
-                max_streak = max(max_streak, current_streak)
-            else:
-                current_streak = 0
-        streaks.append({"TEAM_ID": team_id, "max_streak": max_streak})
+        try:
+            with mlflow.start_run(nested=True):
+                # Metrics
+                for k, v in metrics.items():
+                    try:
+                        mlflow.log_metric(f"{name}_{k}", float(v))
+                    except Exception:
+                        # Skip non-numeric
+                        continue
 
-    streaks_df = pd.DataFrame(streaks)
-    os.makedirs(out_dir, exist_ok=True)
-    streaks_df.to_parquet(os.path.join(out_dir, "winning_streaks.parquet"), index=False)
-    streaks_df.to_csv(os.path.join(out_dir, "winning_streaks.csv"), index=False)
-
-    return streaks_df
+                # Artifact: save dataframe to a temp CSV and log
+                artifact_path = os.path.join(self.analytics_dir, f"{name}.csv")
+                df.to_csv(artifact_path, index=False)
+                mlflow.log_artifact(artifact_path, artifact_path=name)
+        except Exception as e:
+            self.logger.error("Failed to log to MLflow for %s: %s", name, e)
Index: tests/test_predictor_cli.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+># ============================================================\n# Path: tests/test_predictor_cli.py\n# Filename: test_predictor_cli.py\n# Author: Your Team\n# Date: December 2025\n# Purpose: Tests for NBAPredictor CLI interface\n# ============================================================\n\nimport pytest\nimport pandas as pd\nfrom click.testing import CliRunner\nfrom src.prediction_engine.predictor_cli import cli\n\n@pytest.fixture\ndef runner():\n    return CliRunner()\n\ndef test_cli_proba_output(runner, tmp_path):\n    # Run CLI with --proba option\n    result = runner.invoke(cli, [\"--proba\", \"--limit\", \"1\"])\n    assert result.exit_code == 0\n\n    # Parse output into DataFrame\n    df = pd.read_json(result.output)\n\n    # Ensure probability column exists\n    assert \"proba\" in df.columns\n\n    # Access probability safely\n    proba = df[\"proba\"].iloc[0]\n    assert 0 <= proba <= 1\n\ndef test_cli_label_output(runner, tmp_path):\n    # Run CLI with --label option\n    result = runner.invoke(cli, [\"--label\", \"--limit\", \"1\"])\n    assert result.exit_code == 0\n\n    df = pd.read_json(result.output)\n\n    # Ensure label column exists\n    assert \"label\" in df.columns\n\n    label = df[\"label\"].iloc[0]\n    assert label in [0, 1]\n\ndef test_cli_with_tags(runner, tmp_path):\n    # Run CLI with --tags option\n    result = runner.invoke(cli, [\"--tags\", \"--limit\", \"1\"])\n    assert result.exit_code == 0\n\n    df = pd.read_json(result.output)\n\n    # Ensure tags column exists\n    assert \"tags\" in df.columns\n\n    tags = df[\"tags\"].iloc[0]\n    assert isinstance(tags, list)\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/tests/test_predictor_cli.py b/tests/test_predictor_cli.py
--- a/tests/test_predictor_cli.py	(revision 00055356329a845d3b2ca81b77c3ffb2e2b15969)
+++ b/tests/test_predictor_cli.py	(date 1765470426277)
@@ -1,57 +1,92 @@
 # ============================================================
-# Path: tests/test_predictor_cli.py
-# Filename: test_predictor_cli.py
-# Author: Your Team
-# Date: December 2025
-# Purpose: Tests for NBAPredictor CLI interface
+# File: tests/test_predictor_cli.py
+# Purpose: Unit tests for predictor_cli.py
 # ============================================================
 
-import pytest
+import sys
 import pandas as pd
+import pytest
 from click.testing import CliRunner
-from src.prediction_engine.predictor_cli import cli
+
+import src.prediction_engine.predictor_cli as predictor_cli
+
 
 @pytest.fixture
 def runner():
     return CliRunner()
 
-def test_cli_proba_output(runner, tmp_path):
-    # Run CLI with --proba option
-    result = runner.invoke(cli, ["--proba", "--limit", "1"])
-    assert result.exit_code == 0
+
+def test_no_games(monkeypatch, runner):
+    monkeypatch.setattr(
+        predictor_cli, "fetch_season_games", lambda season, limit: pd.DataFrame()
+    )
+    result = runner.invoke(
+        predictor_cli.cli, ["--model", "fake.pkl", "--season", "2025"]
+    )
+    assert "❌ No games found." in result.output
+
+
+def test_proba_default(monkeypatch, runner):
+    games_df = pd.DataFrame([{"game_id": 1}, {"game_id": 2}])
+    features_df = pd.DataFrame({"feat_a": [0.1, 0.2], "win": [1, 0]})
+
+    monkeypatch.setattr(
+        predictor_cli, "fetch_season_games", lambda season, limit: games_df
+    )
+    monkeypatch.setattr(
+        predictor_cli, "generate_features_for_games", lambda games: features_df
+    )
+
+    class DummyPredictor:
+        def __init__(self, model_path):
+            pass
+
+        def predict_proba(self, X):
+            return pd.Series([0.7, 0.3])
+
+    monkeypatch.setattr(predictor_cli, "NBAPredictor", DummyPredictor)
 
-    # Parse output into DataFrame
-    df = pd.read_json(result.output)
+    result = runner.invoke(
+        predictor_cli.cli, ["--model", "fake.pkl", "--season", "2025"]
+    )
+    assert "win_proba" in result.output
+    assert "0.7" in result.output or "0.3" in result.output
 
-    # Ensure probability column exists
-    assert "proba" in df.columns
 
-    # Access probability safely
-    proba = df["proba"].iloc[0]
-    assert 0 <= proba <= 1
+def test_label(monkeypatch, runner):
+    games_df = pd.DataFrame([{"game_id": 1}, {"game_id": 2}])
+    features_df = pd.DataFrame({"feat_a": [0.1, 0.2], "win": [1, 0]})
 
-def test_cli_label_output(runner, tmp_path):
-    # Run CLI with --label option
-    result = runner.invoke(cli, ["--label", "--limit", "1"])
-    assert result.exit_code == 0
+    monkeypatch.setattr(
+        predictor_cli, "fetch_season_games", lambda season, limit: games_df
+    )
+    monkeypatch.setattr(
+        predictor_cli, "generate_features_for_games", lambda games: features_df
+    )
 
-    df = pd.read_json(result.output)
+    class DummyPredictor:
+        def __init__(self, model_path):
+            pass
 
-    # Ensure label column exists
-    assert "label" in df.columns
+        def predict_label(self, X):
+            return pd.Series([1, 0])
 
-    label = df["label"].iloc[0]
-    assert label in [0, 1]
+    monkeypatch.setattr(predictor_cli, "NBAPredictor", DummyPredictor)
 
-def test_cli_with_tags(runner, tmp_path):
-    # Run CLI with --tags option
-    result = runner.invoke(cli, ["--tags", "--limit", "1"])
-    assert result.exit_code == 0
+    result = runner.invoke(
+        predictor_cli.cli, ["--model", "fake.pkl", "--season", "2025", "--label"]
+    )
+    assert "win_pred" in result.output
+    assert "1" in result.output or "0" in result.output
 
-    df = pd.read_json(result.output)
 
-    # Ensure tags column exists
-    assert "tags" in df.columns
+def test_error(monkeypatch, runner):
+    def fail_fetch(season, limit):
+        raise RuntimeError("API failure")
 
-    tags = df["tags"].iloc[0]
-    assert isinstance(tags, list)
+    monkeypatch.setattr(predictor_cli, "fetch_season_games", fail_fetch)
+
+    result = runner.invoke(
+        predictor_cli.cli, ["--model", "fake.pkl", "--season", "2025"]
+    )
+    assert "❌ Prediction run failed" in result.output
Index: cleanup.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/cleanup.py b/cleanup.py
new file mode 100644
--- /dev/null	(date 1765470426211)
+++ b/cleanup.py	(date 1765470426211)
@@ -0,0 +1,58 @@
+import os
+import shutil
+import logging
+
+# Initialize logger
+logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
+
+# List of directories to clean up (You can add/remove based on your needs)
+directories_to_clean = [
+    'docs/',
+    'mlruns/',
+    'models/',
+    'results/',
+    'cache/',
+    'notebooks/'
+]
+
+def check_empty(directory: str) -> bool:
+    """Check if a directory is empty."""
+    return len(os.listdir(directory)) == 0
+
+def delete_directory(directory: str):
+    """Delete a directory after checking its contents."""
+    try:
+        # Ensure that the directory exists
+        if os.path.exists(directory):
+            if check_empty(directory):
+                logging.info(f"{directory} is empty. Deleting...")
+                shutil.rmtree(directory)
+                logging.info(f"Deleted {directory}")
+            else:
+                logging.info(f"{directory} is not empty. Skipping deletion.")
+        else:
+            logging.warning(f"{directory} does not exist.")
+    except Exception as e:
+        logging.error(f"Error deleting {directory}: {e}")
+
+def prompt_user_for_cleanup():
+    """Prompt the user for confirmation before performing cleanup."""
+    print("\n--- Cleanup Warning ---")
+    print("This script will delete certain directories if they are not needed anymore.")
+    print("Please ensure you have backed up any important data before proceeding.")
+    confirmation = input("Do you want to proceed with cleanup? (yes/no): ").lower()
+    if confirmation != "yes":
+        print("Cleanup aborted.")
+        exit()
+
+def clean_up():
+    """Main function to clean up the directories."""
+    prompt_user_for_cleanup()
+
+    for directory in directories_to_clean:
+        delete_directory(directory)
+
+    print("\n--- Cleanup Complete ---")
+
+if __name__ == "__main__":
+    clean_up()
diff --git a/src/predictor/__init__.py b/src/predictor/__init__.py
new file mode 100644
diff --git a/src/daily_runner/__init__.py b/src/daily_runner/__init__.py
new file mode 100644
diff --git a/src/scripts/__init__.py b/src/scripts/__init__.py
new file mode 100644
diff --git a/tests/test_predictor_mlflow.py b/src/api/__init__.py
rename from tests/test_predictor_mlflow.py
rename to src/api/__init__.py
diff --git a/src/model_inference/__init__.py b/src/analytics/__init__.py
rename from src/model_inference/__init__.py
rename to src/analytics/__init__.py
diff --git a/src/interpretability/__init__.py b/src/interpretability/__init__.py
new file mode 100644
